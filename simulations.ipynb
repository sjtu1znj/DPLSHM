{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import scipy.stats as st\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter  \n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "import cmprsk.cmprsk as cmprsk\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "device=\"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "def g1(x):\n",
    "    g = x[:,0] + 2*x[:,1] + 3*x[:,2] + 4*x[:,3] + 5*x[:,4] -15.5\n",
    "    return g\n",
    "def g2(x):\n",
    "    g = x[:,0]**2 + 2*x[:,1]**2 + x[:,2]**3 + torch.sqrt(x[:,3]+1) + torch.log(x[:,4]+1) -8.6\n",
    "    return g\n",
    "def g3(x):\n",
    "    g = x[:,0]**2*x[:,1]**3 + torch.log(x[:,2]+1) + torch.sqrt(x[:,3]*x[:,4]+1) + torch.exp(x[:,4]/2) -8.2\n",
    "    return g\n",
    "def g4(x):\n",
    "    g = (x[:,0]**2*x[:,1]**3 + torch.log(x[:,2]+1) + torch.sqrt(x[:,3]*x[:,4]+1) + torch.exp(x[:,4]/2))**2/20 -6.0\n",
    "    return g\n",
    "#定义非参数部分函数\n",
    "\n",
    "def inv_func1(beta11,beta12 , g, u, z, x,p):\n",
    "    '''\n",
    "    beta: 线性协变量系数\n",
    "    g:非线性协变量函数\n",
    "    u:均匀分布随机变量\n",
    "    z:线性部分协变量\n",
    "    x:非线性部分协变量\n",
    "    p:主要事件生成概率\n",
    "    '''\n",
    "    hazard = torch.exp(beta11*z[:,0]+beta12*z[:,1]  + g(x))\n",
    "    y = -torch.log(1-(1-(1-u)**(1/(hazard)))/p)\n",
    "    return y\n",
    "#定义主要事件逆概率函数\n",
    "\n",
    "def inv_func2(beta21,beta22,z,u):\n",
    "    '''\n",
    "    beta: 线性协变量系数\n",
    "    u:均匀分布随机变量\n",
    "    z:线性部分协变量\n",
    "    '''\n",
    "    hazard = torch.exp(beta21*z[:,0]+beta22*z[:,1])\n",
    "    rate = torch.exp(hazard)\n",
    "    y = -torch.log(1-u)/rate\n",
    "    return y\n",
    "#定义竞争风险事件的逆概率函数\n",
    "def inv_funcc(a,b,u):\n",
    "    c=u*(b-a)+a\n",
    "    return c\n",
    "#定义删失事件的逆概率函数\n",
    "def dataproduce(n, beta11, beta12, beta21, beta22, g_index, z_index, seed, p, a, b):\n",
    "    '''\n",
    "    n:样本量\n",
    "    beta:线性协变量系数\n",
    "    g_index:非线性协变量函数的选择指标\n",
    "    z_index:线性协变量分布的选择指标\n",
    "    seed:随机数种子\n",
    "    mu:删失随机变量参数\n",
    "    p:生成主要事件概率\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)#设置随机数种子\n",
    "    sigma = 0.5*torch.ones((5,5)) + 0.5*torch.eye(5)\n",
    "    mvnorm = st.multivariate_normal(mean=[0,0,0,0,0], cov=sigma)#定义Gaussian copula\n",
    "    X = torch.from_numpy(2*st.norm.cdf(mvnorm.rvs(n)))\n",
    "    if z_index==1:\n",
    "        Z = torch.randint(2,size = [2*n])\n",
    "    else:\n",
    "        Z = torch.randn(2*n)/torch.sqrt(torch.tensor([2.0]))+0.5#生成线性协变量\n",
    "    Z = Z.reshape(n,2)\n",
    "    if g_index==1:\n",
    "        g = g1\n",
    "    if g_index==2:\n",
    "        g = g2\n",
    "    if g_index==3:\n",
    "        g = g3\n",
    "    if g_index==4:\n",
    "        g = g4\n",
    "    #选择函数g\n",
    "    u =  torch.rand(n)\n",
    "    t = inv_func1(beta11, beta12, g, u, Z, X,p)#生成事件发生时间\n",
    "    index2=~(t>0)\n",
    "    n2=int(sum(index2))\n",
    "    #确定竞争风险事件的指标集和样本量\n",
    "    u=torch.rand(n2)\n",
    "    t2=inv_func2(beta21,beta22,Z[index2],u)\n",
    "    #生成竞争风险事件的数据\n",
    "    t[index2]=t2\n",
    "    epsilon = torch.zeros(n)\n",
    "    epsilon[index2] = 1\n",
    "    epsilon = epsilon+1\n",
    "    #得出事件类型指标\n",
    "    u = torch.rand(n)\n",
    "    C = inv_funcc(a,b,u)\n",
    "    delta = (C >= t)+0.0\n",
    "    t = torch.minimum(t,C)\n",
    "    return t.reshape(n,1),epsilon.reshape(n,1),Z.reshape(n,2),X,delta.reshape(n,1)\n",
    "#定义数据生成函数\n",
    "\n",
    "def likelihood(t, epsilon, Z, beta, g, delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    l = torch.sum(((2-epsilon_re[delta ==1])*((Z@beta)[delta ==1]+(g)[delta ==1]-torch.log((R@torch.exp(Z@beta+g))[delta ==1]))))\n",
    "    return l\n",
    "#定义似然函数\n",
    "def dedata(data):\n",
    "    T = data[:,0:1]\n",
    "    epsilon = data[:,1:2]\n",
    "    Z = data[:,2:4]\n",
    "    X = data[:,4:9]\n",
    "    delta = data[:,9:]\n",
    "    return T,epsilon,Z,X,delta\n",
    "#将整个数据集分成list\n",
    "def cv(data,seed):\n",
    "    train,test = train_test_split(data, train_size=0.80, test_size=0.20,random_state=seed)\n",
    "    train,valid = train_test_split(train, train_size=0.80, test_size=0.20,random_state=seed)\n",
    "    return train,valid,test\n",
    "#划分训练集，验证集，测试集\n",
    "def G_hat(T,delta):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=1-delta)\n",
    "    km = kmf.survival_function_[\"KM_estimate\"]\n",
    "    return torch.tensor(km.loc[T[:,0]].values.reshape(len(T[:,0]),1)).to(device)\n",
    "#定义删失变量的估计值\n",
    "\n",
    "def likelihood_linear(t, epsilon, Z, X, beta, gamma,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    gamma = gamma.reshape((len(gamma),1))\n",
    "    l = torch.sum(((2-epsilon_re[delta ==1])*((Z@beta)[delta ==1]+(X@gamma)[delta ==1]-torch.log((R@torch.exp(Z@beta+X@gamma))[delta ==1]))))\n",
    "    return l\n",
    "#定义fine and gray model对应的似然函数\n",
    "\n",
    "class Net_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_linear,self).__init__()\n",
    "        self.beta=nn.Parameter(torch.zeros(2).to(device),requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros(5).to(device),requires_grad=True)\n",
    "#定义fine and gray model对应的优化网络\n",
    "        \n",
    "class Myloss_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, t, epsilon, Z, X, beta, gamma,delta,G_hat):\n",
    "        return -likelihood_linear(t, epsilon, Z, X, beta,gamma,delta,G_hat)\n",
    "#定义fine and gray model对应的损失函数\n",
    "\n",
    "       \n",
    "def beta0(data,G_hat_train,G_hat_valid):\n",
    "    train, valid, test = cv(data,1)\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    model  = Net_linear().to(device)\n",
    "    criterion = Myloss_linear()\n",
    "    optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
    "    model.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(5e2)):\n",
    "        optimizer.zero_grad()\n",
    "        loss=criterion(T_train,epsilon_train,Z_train,X_train, model.beta,model.gamma,delta_train,G_hat_train)\n",
    "        loss2 = criterion(T_valid,epsilon_valid,Z_valid,X_valid, model.beta,model.gamma,delta_valid,G_hat_valid)\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > 1e-5:\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model.beta\n",
    "#定义参数部分初始化函数\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net1,self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net2,self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,40),nn.ReLU(),\n",
    "            nn.Linear(40,40),nn.ReLU(),\n",
    "            nn.Linear(40,20),nn.ReLU(),\n",
    "            nn.Linear(20,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net3,self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Sequential(nn.Linear(5,10), nn.ReLU()),\n",
    "            nn.Sequential(nn.Linear(10,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,20), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(20,10), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(10,1))\n",
    "        )\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net4,self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(5, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "#定义DNN结构\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, T,epsilon,Z ,beta ,g,delta,G_hat):\n",
    "        return -likelihood(T,epsilon,Z,beta ,g,delta,G_hat)\n",
    "#定义损失函数\n",
    "\n",
    "def chooseNet(index,beta_0):\n",
    "    if index ==1:\n",
    "        model=Net1(beta_0 = beta_0)\n",
    "        lr = 5e-2\n",
    "    if index ==2:\n",
    "        model=Net2(beta_0 = beta_0)\n",
    "        lr = 1e-3\n",
    "    if index ==3:\n",
    "        model=Net3(beta_0 = beta_0)\n",
    "        lr = 1e-3\n",
    "    if index ==4:\n",
    "        model=Net4(beta_0 = beta_0)\n",
    "        lr = 5e-4\n",
    "    return model.to(device),lr\n",
    "#定义网络选择函数\n",
    "def REg(g_0, g_hat):\n",
    "    g_hat=g_hat.reshape([len(g_0),])\n",
    "    y = torch.sqrt(torch.mean(((g_hat-torch.mean(g_hat))-g_0)**2)/(torch.mean((g_0)**2)))\n",
    "    return y\n",
    "class h_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(h_net, self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(1,5),nn.ReLU(),\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,10),nn.ReLU(),\n",
    "            nn.Linear(10,5),nn.ReLU(),\n",
    "            nn.Linear(5,1),)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x \n",
    "\n",
    "class g_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(g_net, self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,40),nn.ReLU(),\n",
    "            nn.Linear(40,40),nn.ReLU(),\n",
    "            nn.Linear(40,20),nn.ReLU(),\n",
    "            nn.Linear(20,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "def VE_loss_func(epsilon,delta,Z,h,g):\n",
    "    l = torch.mean((2-epsilon)*delta*(Z-h-g)**2)\n",
    "    return l\n",
    "class VE_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, epsilon,delta,Z,h,g):\n",
    "        return VE_loss_func(epsilon,delta,Z,h,g)\n",
    "    \n",
    "def estimation(Net_index, train, valid, test,G_hat_train,G_hat_valid,G_hat_train0,G_hat_valid0,g):\n",
    "    window = []\n",
    "    positive = 0\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    beta_0 = beta0(train,G_hat_train0,G_hat_valid0)\n",
    "    '''\n",
    "    model = Net(beta_0=beta_0)\n",
    "    criterion = MyLoss()\n",
    "    lr = 1e-3\n",
    "    optimizer = torch.optim.Adam([{'params':model.module.parameters(),'lr':lr},\n",
    "                                {'params':model.beta,'lr':50*lr}])\n",
    "    '''\n",
    "    model,lr = chooseNet(Net_index,beta_0)\n",
    "    criterion = MyLoss()\n",
    "    optimizer = torch.optim.Adam([{'params':model.module.parameters(),'lr':lr},\n",
    "                                {'params':model.beta,'lr':50*lr}])\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    loss3 = (torch.tensor(1e5))\n",
    "    loss4 = (torch.tensor(1e6))\n",
    "    for j in range(int(1000)):\n",
    "        optimizer.zero_grad()\n",
    "        g_train = model(X_train)\n",
    "        loss=criterion(T_train,epsilon_train,Z_train,model.beta ,g_train,delta_train,G_hat_train)\n",
    "        loss1 = loss\n",
    "        if torch.isnan(loss2):\n",
    "            break\n",
    "        g_valid = model(X_valid)\n",
    "        loss2 = criterion(T_valid,epsilon_valid,Z_valid,model.beta ,g_valid,delta_valid,G_hat_valid)\n",
    "        g_test = model(X_test)\n",
    "        loss3 = criterion(T_valid,epsilon_valid,Z_valid,model.beta ,g_valid,delta_valid,G_hat_valid)\n",
    "        loss4 = torch.min(loss3,loss4)\n",
    "        window.append(loss2)\n",
    "        d = 10\n",
    "        if j >100:\n",
    "            criterion_set = torch.mean(torch.tensor(window)[j-d:j]-torch.tensor(window)[j-d-1:j-1])\n",
    "            if criterion_set>0:\n",
    "                positive = positive+1\n",
    "                if loss3 - loss4 > 0.015*torch.abs(loss4):\n",
    "                    break\n",
    "                if positive > 50:\n",
    "                    break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss2, model.beta[0], model.beta[1], REg(g(X_test),g_test),g_test,model\n",
    "\n",
    "\n",
    "def variance_estimation(train, valid, test):\n",
    "    criterion_ve = VE_loss()\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    h_net_1=h_net().to(device)\n",
    "    h_net_2=h_net().to(device)\n",
    "    g_net_1=g_net().to(device)\n",
    "    g_net_2=g_net().to(device)\n",
    "    optimizer_1 = torch.optim.Adam(list(h_net_1.parameters())+list(g_net_1.parameters()), lr=0.001)\n",
    "    h_net_1.train()\n",
    "    g_net_1.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(1e4)):\n",
    "        optimizer_1.zero_grad()\n",
    "        loss = criterion_ve(epsilon_train, delta_train, Z_train[:,[0]], h_net_1(T_train), g_net_1(X_train))\n",
    "        loss2 = criterion_ve(epsilon_valid, delta_valid, Z_valid[:,[0]], h_net_1(T_valid), g_net_1(X_valid))\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > 1e-2:\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "    optimizer_2 = torch.optim.Adam(list(h_net_2.parameters())+list(g_net_2.parameters()), lr=0.001)\n",
    "    h_net_2.train()\n",
    "    g_net_2.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(1e4)):\n",
    "        optimizer_2.zero_grad()\n",
    "        loss = criterion_ve(epsilon_train, delta_train, Z_train[:,[1]], h_net_2(T_train), g_net_2(X_train))\n",
    "        loss2 = criterion_ve(epsilon_valid, delta_valid, Z_valid[:,[1]], h_net_2(T_valid), g_net_2(X_valid))\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > 2e-2:\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer_2.step()\n",
    "    a = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[0]]-h_net_1(T_train)-g_net_1(X_train))**2)\n",
    "    b = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[0]]-h_net_1(T_train)-g_net_1(X_train))*(Z_train[:,[1]]-h_net_2(T_train)-g_net_2(X_train)))\n",
    "    c = b \n",
    "    d = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[1]]-h_net_2(T_train)-g_net_2(X_train))**2)\n",
    "    std1 = torch.sqrt(d/(a*d-b*c))\n",
    "    std2 = torch.sqrt(a/(a*d-b*c))\n",
    "    return std1,std2\n",
    "def CI_cover(n,estimation, std, parameter):\n",
    "    n = torch.tensor(n)\n",
    "    cover = 0\n",
    "    if (estimation-1.96*std/torch.sqrt(n)<= parameter)&(estimation+1.96*std/torch.sqrt(n)>= parameter):\n",
    "        cover = cover+1\n",
    "    return cover\n",
    "def riskmatrix(epsilon,T,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = T.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    return R\n",
    "def riskmatrix_add(epsilon,T,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = T.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) < (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    return R\n",
    "def TP(R,M):\n",
    "    n = len(M)\n",
    "    expM_vector = torch.exp(M).reshape(1,n)\n",
    "    Y_expM_matrix = R*expM_vector#列时间变化，行样本变化\n",
    "    M_order_matrix = (M.reshape(n,1)>M.reshape(1,n)).double()\n",
    "    Numerator = Y_expM_matrix@M_order_matrix#T,M\n",
    "    Denominator = Y_expM_matrix@torch.ones((n,n)).to(device)\n",
    "    TP  = Numerator/Denominator\n",
    "    return TP\n",
    "def FP(R_add,M):\n",
    "    n = len(M)\n",
    "    M_order_matrix = (M.reshape(n,1)>M.reshape(1,n)).double()\n",
    "    Numerator = R_add@M_order_matrix#T,M\n",
    "    Denominator = R_add@torch.ones((n,n)).to(device)\n",
    "    Denominator[Denominator==0]=1\n",
    "    FP = Numerator/Denominator\n",
    "    return FP\n",
    "def auc_t(M,TPR,FPR):\n",
    "    n = len(M)\n",
    "    auc_array = torch.zeros(n)\n",
    "    for i in range(n):\n",
    "        sorted_index = torch.argsort(FPR[i])\n",
    "        fpr_list_sorted =  torch.tensor(FPR[i])[sorted_index]\n",
    "        tpr_list_sorted = torch.tensor(TPR[i])[sorted_index]\n",
    "        auc_array[i] = integrate.trapz(y=tpr_list_sorted, x=fpr_list_sorted)\n",
    "    return auc_array\n",
    "def Stof(S,t):\n",
    "    index = torch.argsort(t[:,0])\n",
    "    t_ordered = t[index]\n",
    "    S_ordered = S[index]\n",
    "    f = (torch.concatenate((torch.ones((1,len(S_ordered))).to(device), S_ordered[:-1])) -S_ordered )/(t_ordered- torch.concatenate((torch.tensor([[0]]).to(device), t_ordered[:-1])))\n",
    "    return f\n",
    "def Gamma_hat(t, epsilon, Z,beta, g, delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1).to(device)@(delta).T))*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S=(R@torch.exp(Z@beta+g))/n\n",
    "    Gamma = ((t>=(t.T)).double()@((2-epsilon)*delta/S))/n\n",
    "    return Gamma\n",
    "def f_hat(t, epsilon, Z,beta, g, delta):\n",
    "    Gamma = Gamma_hat(t, epsilon, Z,beta, g, delta)\n",
    "    n = len(epsilon)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    f = Stof(S,t)\n",
    "    return torch.mean(f,axis = 1)\n",
    "def S_hat(t, epsilon, Z,beta, g, delta):\n",
    "    Gamma = Gamma_hat(t, epsilon, Z,beta, g, delta)\n",
    "    n = len(epsilon)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    index = torch.argsort(t[:,0])\n",
    "    S_ordered = S[index]\n",
    "    return torch.mean(S_ordered,axis = 1)\n",
    "def Gamma_real(t,p):\n",
    "    Gamma = -torch.log(1-p*(1-torch.exp(-t)))\n",
    "    return Gamma\n",
    "def f_real(t,  Z,beta, g, p):\n",
    "    n = len(t)\n",
    "    Gamma = Gamma_real(t, p)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((len(g),1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    f = Stof(S,t)\n",
    "    return torch.mean(f,axis = 1)\n",
    "def S_real(t,  Z,beta, g, p):\n",
    "    n = len(t)\n",
    "    Gamma = Gamma_real(t, p)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((len(g),1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    index = torch.argsort(t[:,0])\n",
    "    S_ordered = S[index]\n",
    "    return torch.mean(S_ordered,axis = 1)\n",
    "\n",
    "def C_tau(auc_array,U,U_ordered, X, Z, g_, p):  \n",
    "    beta = torch.Tensor([1.0,1.0]).to(device)\n",
    "    g = g_(X)\n",
    "    f = f_real(U.reshape((len(U),1)),  Z,beta, g, p)\n",
    "    S = S_real(U.reshape((len(U),1)),  Z,beta, g, p)\n",
    "    t_index = torch.sort(U).indices\n",
    "    t_ordered = U[t_index]\n",
    "    sub_index = torch.where(torch.isin(t_ordered, U_ordered))\n",
    "    w = (2*f*S)[sub_index]\n",
    "    W = torch.trapz(y=w, x=t_ordered[sub_index])\n",
    "    w_tau = w/W\n",
    "    C = torch.trapz(y=auc_array*w_tau, x=t_ordered[sub_index])\n",
    "    return C\n",
    "def cubic_spline(q,u,m):#q+4:样条基个数，u输入向量\n",
    "    T = np.around(np.linspace(0,0.8,q),2)\n",
    "    n = len(u)\n",
    "    B = np.zeros((n,q))\n",
    "    for i in range(n):\n",
    "        B[i,0] = 1\n",
    "        B[i,1] = u[i]\n",
    "        B[i,2] = u[i]**2\n",
    "        B[i,3] = u[i]**3\n",
    "        for j in range(q):\n",
    "            if u[i] > T[j]:\n",
    "                B[i,j] = (u[i] - T[j])**3\n",
    "    return B\n",
    "def outcome_to_df(outcomes,i):\n",
    "    outcomes_colname = ['beta1', 'beta2', 'REg', 'std1', 'std2', 'cover_rate1', 'cover_rate2', 'AUC(25%)','AUC(50%)','AUC(75%)','C^tau']\n",
    "    df_outcomes_DP = (pd.DataFrame(np.array(outcomes)[:,i:])).rename(columns=dict(zip((pd.DataFrame(np.array(outcomes)[:,i:])), outcomes_colname)))\n",
    "    return df_outcomes_DP\n",
    "def TPR_FPR(U_test,M,M0):\n",
    "    U_ordered = U_test[(U_test>0)&(U_test<U_test.max())].sort().values\n",
    "    Y_t = (~(U_test.reshape((len(U_test),1)) < U_ordered.reshape((1,len(U_ordered))))).int().to(device)\n",
    "    '''\n",
    "    Y_i(t_j)对应的矩阵(i,j)\n",
    "    '''\n",
    "    Yt_eM = Y_t*(torch.exp(M0).reshape((len(M0),1)))\n",
    "    W_t = Yt_eM.sum(axis =0).reshape((1,len(U_ordered)))\n",
    "    TPR = ((M.reshape(len(M),1)>M.sort().values.reshape(1,len(M))).double().T)@(Yt_eM/W_t)\n",
    "    '''\n",
    "    mi,tj对应(i,j)\n",
    "    '''\n",
    "    Y_t_add = (~(U_test.reshape((len(U_test),1)) <= U_ordered.reshape((1,len(U_ordered))))).int().to(device)\n",
    "    W_t_add = Y_t_add.sum(axis =0).reshape((1,len(U_ordered)))\n",
    "    FPR = ((M.reshape(len(M),1)>M.sort().values.reshape(1,len(M))).double().T)@(Y_t_add/W_t_add)\n",
    "    return torch.flip(TPR,[0]),torch.flip(FPR,[0])\n",
    "def Uproduce(n, beta11, beta12, beta21, beta22, g_index, z_index, seed, p, a, b):\n",
    "    '''\n",
    "    n:样本量\n",
    "    beta:线性协变量系数\n",
    "    g_index:非线性协变量函数的选择指标\n",
    "    z_index:线性协变量分布的选择指标\n",
    "    seed:随机数种子\n",
    "    mu:删失随机变量参数\n",
    "    p:生成主要事件概率\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)#设置随机数种子\n",
    "    sigma = 0.5*torch.ones((5,5)) + 0.5*torch.eye(5)\n",
    "    mvnorm = st.multivariate_normal(mean=[0,0,0,0,0], cov=sigma)#定义Gaussian copula\n",
    "    X = torch.from_numpy(2*st.norm.cdf(mvnorm.rvs(n)))\n",
    "    if z_index==1:\n",
    "        Z = torch.randint(2,size = [2*n])\n",
    "    else:\n",
    "        Z = torch.randn(2*n)/torch.sqrt(torch.tensor([2.0]))+0.5#生成线性协变量\n",
    "    Z = Z.reshape(n,2)\n",
    "    if g_index==1:\n",
    "        g = g1\n",
    "    if g_index==2:\n",
    "        g = g2\n",
    "    if g_index==3:\n",
    "        g = g3\n",
    "    if g_index==4:\n",
    "        g = g4\n",
    "    #选择函数g\n",
    "    u =  torch.rand(n)\n",
    "    t = inv_func1(beta11, beta12, g, u, Z, X,p)#生成事件发生时间\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DP 0.9600492459483265 1.1546755132020308 0.09318914396982556 0.8834723390965932 0.46001276978521627 1 1 0.9542306838082555 0.9554703591490092 0.9402954030860531 0.9484899001687999\n",
      "Linear 0.9601050314967353 1.1547799190182866 0.0932463296557707 0.2773823777109898 0.298843314441489 1 1 0.9542306836664338 0.9554703559479912 0.940295313088309 0.9484896574926766\n",
      "spline 1.0170259785953137 1.2265884568439402 0.1775185095291591 0.3035121433614764 0.31819273073151944 1 1 0.9522455929602589 0.954620495897248 0.9406155497174213 0.9465654631476077\n",
      "1\n",
      "DP 1.027040756816662 0.9501453334162604 0.04560066542464417 0.3382056817698626 2.5152987474711925 2 2 0.9724133527376935 0.9728507436255385 0.9700687439564015 0.9708919894213365\n",
      "Linear 1.0270541816543242 0.9501457994561113 0.04559376357802273 0.3220838984343338 0.2951311301032851 2 2 0.9724133527376935 0.9728507436255385 0.9700687439564015 0.9708919894213365\n",
      "spline 1.103842768817601 0.966807199144466 0.17850294227471292 0.34378694062075277 0.31848978909778186 2 2 0.972058243321823 0.9793620947712851 0.9779468085965711 0.9765628610434004\n",
      "2\n",
      "DP 0.9259213795987434 0.7591236172599748 0.22138736049748192 0.37002538559160086 0.43694823163737057 3 3 0.9764458071271199 0.9740860106680385 0.9723233258704458 0.976374875512335\n",
      "Linear 0.9101303662201046 0.7655593661149928 0.06865359100189278 0.2802464257302229 0.27017107863676804 3 3 0.9766472838604694 0.9810421777172214 0.9808877983154254 0.9793230109948574\n",
      "spline 0.9666324040659529 0.721815315765725 0.5087010563784503 0.29409270194685455 0.28456916523083375 3 3 0.9760773231648214 0.9804183892575264 0.9803432782231782 0.9786321589246894\n",
      "3\n",
      "DP 1.2669107625094327 0.9029800665413116 0.25828044472077916 0.3140624463238128 0.3318830393469669 4 4 0.9726321889661923 0.9728290746880945 0.9675162895103948 0.9722539229464097\n",
      "Linear 1.2737607486245561 0.9215100731465821 0.02664874359667193 0.2740164429874344 0.2755334986641609 4 4 0.972630941712972 0.9728271180510215 0.9675292279867335 0.9722575361751133\n",
      "spline 1.3240977513307524 0.9602009053178456 0.1516678150708371 0.28334990501605317 0.29939208252731697 3 4 0.9727957476667823 0.9728199118423337 0.9683855280326641 0.9727226172688865\n",
      "4\n",
      "DP 0.7992067386229355 1.0593571803144353 0.3302246590345164 0.41587147001560565 0.36342830298666323 5 5 0.9890710035854713 0.9881560307463089 0.9881013238035297 0.9875294528541299\n",
      "Linear 0.7565187199860836 1.0636075688150461 0.15243126904498616 0.2849845431637581 0.2977053084726333 5 5 0.9890746689759868 0.988160639879347 0.9881061433132855 0.987532430658804\n",
      "spline 0.7671761992020724 1.0696191029241395 0.2668134336007263 0.29741104200407087 0.30746362505361013 4 5 0.9890308759988489 0.9881427197823142 0.9880882278488108 0.9880459944060032\n",
      "5\n",
      "DP 1.0316061471153686 1.09578981448331 0.21382316455105352 0.800163195094066 0.37453008815360117 6 6 0.9651856749532193 0.9660279709608198 0.9669663144693654 0.9663292936310989\n",
      "Linear 1.0395829084107306 1.099856835996187 0.06165038628650731 0.2693700806230729 0.2838327756443217 6 6 0.965688113170695 0.9666868675946136 0.9670244872327747 0.9683886815528457\n",
      "spline 1.0007097298986296 1.170898643256833 0.22369014562808148 0.27758614997413106 0.29149187699515167 5 6 0.9649149911071085 0.9658581789670059 0.9663048199126056 0.9675078820237695\n",
      "6\n",
      "DP 1.1051572388101651 0.7451615936563878 0.1149135468496661 0.4590852976136321 0.5001363790139897 7 7 0.969546229186575 0.9614707618620422 0.9607093176616326 0.964384210663389\n",
      "Linear 1.0666666606837054 0.7255056084830223 0.10598089134703052 0.27141859788811323 0.2713690492216929 7 6 0.9697595476976224 0.9617739094873998 0.9607964583869352 0.9645516154450823\n",
      "spline 1.090631214513684 0.8269663061736486 0.20699147972504886 0.2914405844508183 0.305332125020559 6 7 0.9696728575383766 0.9617023378611398 0.9607083718726775 0.964473106171217\n",
      "7\n",
      "DP 0.7945553067415335 0.8901864867894093 0.05593569667768277 0.41094778153198497 1.297474131195718 8 8 0.9737078273698032 0.9744686785173146 0.9756642374685531 0.9734457069726892\n",
      "Linear 0.7611723312499558 0.8798199664388819 0.04916809186456141 0.268778005349437 0.2771266787595567 8 7 0.9737125706885702 0.9744752243054552 0.9756639441814907 0.9734497080158209\n",
      "spline 0.8228965140832639 0.9804216812352287 0.1649767350568522 0.2750478944133773 0.2884785401808943 7 8 0.9732113792894137 0.9741147515968984 0.9750121983588937 0.974441136036784\n",
      "8\n",
      "DP 1.0001015496664694 1.5317735649845852 0.262266794563999 0.27226678240949265 0.5110429160117388 9 8 0.9822825128908401 0.982366963777868 0.9892540121514615 0.9849061674575668\n",
      "Linear 1.0307627459646524 1.546644008486848 0.17189751661728941 0.3110114271867693 0.32225278962187603 9 7 0.9880973773575944 0.9893117306003281 0.9892545345864157 0.9888116405140096\n",
      "spline 1.0581729674508957 1.5545270664601323 0.2693860119838859 0.31931009457638276 0.32357275506986105 8 8 0.9822528642567083 0.9892569698538931 0.9891752334684547 0.987351637809168\n",
      "9\n",
      "DP 0.8723439041521577 1.0007645693676293 0.3776662666791039 0.4398182712740315 0.35544157935304305 10 9 0.9769835175008748 0.9754136167135166 0.974088613985644 0.9774190128373158\n",
      "Linear 0.9592473113742032 1.204186926185934 0.09351479918899312 0.30139643046702236 0.29535232363018593 10 8 0.9770534710314009 0.9754900305046492 0.9741466813103716 0.9758404710285842\n",
      "spline 1.0063446914731247 1.2941713257470993 0.27606259190052734 0.3428286858212817 0.31329329598139455 9 9 0.9766398911421716 0.9749178114991 0.9734488797161702 0.9753553855653838\n",
      "10\n",
      "DP 1.0955571273727742 1.1673587621167965 0.15120120403932522 0.29124110538545045 0.8164560928603083 11 10 0.9810379485858562 0.9797245001665402 0.9760325492114197 0.977610518725384\n",
      "Linear 1.0941649466402306 1.1516493345540724 0.1254945703156005 0.2759711309427745 0.27138560439282416 11 9 0.9752436424397464 0.9728041497415664 0.9675152777086973 0.9715595521247826\n",
      "spline 0.9894809982086961 1.2948439500132798 0.2397492150317337 0.2764738759899656 0.2732944148640488 10 9 0.9751895497060356 0.9727652325201976 0.9674859278180067 0.9714934087751881\n",
      "11\n",
      "DP 1.1693274050565994 0.946576476160323 0.13595468782950398 0.2955552475594969 0.6256799820285087 12 11 0.9712782658027125 0.9685030450875552 0.9648819296998995 0.9689090148464534\n",
      "Linear 1.1791710798588835 0.9511448115105915 0.11922294696070002 0.2926169296382493 0.276177455677356 12 10 0.9712756913339494 0.968499901485623 0.9648774648703121 0.9688870396948828\n",
      "spline 1.2097195935850544 0.9265978295112772 0.2517354695916552 0.3176522197644552 0.2985394331853797 11 10 0.9709571856268555 0.968086431840405 0.9644909478802133 0.9685020504406217\n",
      "12\n",
      "DP 1.072329929149411 1.2163696755418538 0.30112286500539825 0.4245066087421183 1.2208988050643894 13 12 0.9721375166204385 0.9670198865151018 0.9669418301509749 0.9680623097429029\n",
      "Linear 1.0085261845031905 1.197887426348766 0.08499459443535558 0.2819306369170664 0.27390266899042903 13 11 0.9665203146889096 0.9603084352129716 0.9586452240662438 0.9621076495882592\n",
      "spline 1.0955676739419866 1.3355223769638263 0.19827017520188148 0.2929965250433543 0.3077428656681637 12 10 0.9658272063758713 0.959482824841007 0.9668935916053416 0.9649399553038518\n",
      "13\n",
      "DP 0.9708244156640689 1.0328672369224243 0.23177800348974553 0.4583939709602349 0.40148302608665065 14 13 0.9894769930436431 0.991789220937515 0.9905996137834956 0.9892940084904591\n",
      "Linear 1.028952628234134 1.2187756898528102 0.15521923076814043 0.3009831251378234 0.2874597186121579 14 12 0.9836823284763445 0.991826896787372 0.9908434717226027 0.9883624886074677\n",
      "spline 1.0631476633066932 1.304067684483345 0.2550180929599632 0.3341317262845293 0.3143395140348146 13 11 0.9836798130823607 0.9918198134065624 0.9908373411079051 0.9883573886527459\n",
      "14\n",
      "DP 0.8006451232868168 0.9626100532516862 0.1095755603976874 0.40207039255563914 0.6285782926847469 15 14 0.9727829982455015 0.9704131285401348 0.9722146621780838 0.973417602343504\n",
      "Linear 0.8053439054425305 0.9671219103820168 0.10714410836328926 0.27038922836346074 0.2612145717030377 15 13 0.9727830126251938 0.9704131409926966 0.9722146648966746 0.9734176107928072\n",
      "spline 0.832067344256323 1.0921273940048002 0.21094490971253368 0.2833233881037827 0.28644251364087064 14 12 0.9713693152167459 0.9692543684123127 0.9712634310469822 0.9723924259793663\n",
      "15\n",
      "DP 0.9631325273271608 0.7980325828412659 0.1343995578662963 0.31619793488594594 0.5106783585950756 16 15 0.9655954911498623 0.9635275966661485 0.9600922550832227 0.9617849932521416\n",
      "Linear 0.9630965252240047 0.798015302361644 0.1357655829044458 0.2644446355521966 0.268818965148453 16 14 0.9655954911498703 0.9635275966661621 0.9600922550832246 0.9617849932521492\n",
      "spline 0.9395667099722876 0.7215673580983138 0.17771383461327114 0.2642999019875701 0.2840507099759617 15 13 0.9654981171764103 0.9563974671723949 0.9600654845225596 0.9605744630697203\n",
      "16\n",
      "DP 0.8624390651546867 1.0089108784126903 0.23158686488980473 0.376803776069477 0.7471274997971903 17 16 0.9709825682867915 0.9655509673545662 0.961383801916379 0.9646615923099974\n",
      "Linear 0.8415411284803496 1.01450218505663 0.15703444543702566 0.2795447124896565 0.2745466147529825 17 15 0.9711596408832698 0.9726998469071371 0.9701879410972155 0.9709490483090162\n",
      "spline 0.8455016376908876 1.0645385597322348 0.27430217361785225 0.28448846755631 0.2935781691363836 16 14 0.9694855038258832 0.9637540759805583 0.9682356473425431 0.9669734169940696\n",
      "17\n",
      "DP 0.9641728969617873 0.6462527298118165 0.14405941189367782 0.2605368328820789 0.27586058587455703 18 16 0.9812850859207951 0.9820627639467161 0.9889179341375688 0.9836091829744866\n",
      "Linear 0.9787623541362778 0.6453816009136873 0.11272192632139443 0.3006526229970081 0.275987512751069 18 15 0.9812721686821186 0.9820428502805006 0.9803443134684954 0.9823160494879744\n",
      "spline 1.048020079768419 0.687463000684993 0.19976232695388962 0.2936447453736769 0.2681011072978993 17 14 0.9812278817575171 0.9819858568610643 0.9802626622492618 0.9809686606136871\n",
      "18\n",
      "DP 1.0967152278107615 0.9118175322114485 0.16429835656549371 0.38256820591929463 0.31873137753612435 19 17 0.975084975622816 0.9714624011544865 0.9685870032114369 0.9709709022440266\n",
      "Linear 1.0950431953743007 0.9063088905481357 0.16474069361105842 0.2911050442328287 0.2793707944469159 19 16 0.9750849749806134 0.9714624003089032 0.9685870031969203 0.9709709016528099\n",
      "spline 1.1074143431249412 0.8505104120797217 0.2543601795288573 0.3181804988828591 0.27714736877735424 18 15 0.9740201864018685 0.9701790989007477 0.9682124392174436 0.9701339820955007\n",
      "19\n",
      "DP 1.1619609709987864 1.1738106663746737 0.23494737550515912 0.29717183908354267 0.4059958742260932 20 18 0.9739821477244306 0.9710017429478985 0.9660765527011217 0.9692120475352163\n",
      "Linear 1.1241062944269786 1.1387721783112417 0.05325850741257319 0.2725841032978051 0.27658376956275094 20 17 0.9739401767305641 0.9709499245128347 0.9660013550612779 0.9691555909743149\n",
      "spline 1.1214711130698451 1.2091768495716297 0.20681445699211962 0.27780713193515083 0.2855483763569669 19 16 0.979718271841124 0.9778492183829883 0.9745200163635979 0.9752872010209381\n",
      "20\n",
      "DP 1.1534161217257812 1.1890276494292376 0.23630444867685454 0.2990242173705738 0.4786722482340641 21 19 0.9749446491563488 0.9802913680515666 0.9780944859638567 0.9781229208560898\n",
      "Linear 1.1534317534097946 1.1890078342251567 0.2363139795259677 0.27511520910250603 0.26941178722640446 21 18 0.9749446491563488 0.9802913680515666 0.9780944859638567 0.9781229208560898\n",
      "spline 1.1115459401700511 1.287375378334764 0.2781322691654338 0.27589480676055705 0.3027151543336651 20 17 0.9799378186547998 0.9792950588952141 0.976955568895633 0.9783755086423891\n",
      "21\n",
      "DP 1.0386130321056217 1.1618715681056169 0.4035287188825732 0.257738426041591 0.5600422989117386 22 20 0.9879013707786639 0.9856526547286001 0.9856596141251757 0.9867613901304744\n",
      "Linear 1.0304120910664136 1.148935295405273 0.19967812920397363 0.27969275712382796 0.28977904095334023 22 19 0.9820844968303578 0.9856020501138951 0.9856637159556776 0.9850357840394067\n",
      "spline 1.070055403720567 1.2207434090594917 0.35817829327621387 0.29515305177703294 0.2938619722210406 21 18 0.9820387951032121 0.9785808866094531 0.9770259961591207 0.9805370819600568\n",
      "22\n",
      "DP 1.11558987885684 0.926677523229409 0.15066632006945405 0.7414684441325138 0.34084361066188923 23 21 0.9780275740783568 0.9757293961286077 0.9778643782483463 0.9761091700693176\n",
      "Linear 1.095723749161512 0.9179092538533916 0.15096926237891936 0.2778016038696779 0.27013453784243213 23 20 0.9780937418105915 0.9757942152823899 0.9778610011433826 0.9761326883135424\n",
      "spline 1.1694501500971797 0.9408391695882334 0.32982948582632693 0.29028560882911575 0.2989232668758168 22 19 0.9780709639239078 0.975746144420238 0.9778478870754292 0.9738729969223998\n",
      "23\n",
      "DP 0.9942093276005243 1.040635256341748 0.12976084690895656 0.3254146403982271 0.3075769147113109 24 22 0.9684893189728181 0.964762681758807 0.9528313598930658 0.9617113675683897\n",
      "Linear 0.995151905429701 1.043285477613288 0.055181789983860016 0.2710765131716949 0.2799782180529931 24 21 0.9687089159037134 0.9650054967820131 0.9535386191628821 0.9620915389682119\n",
      "spline 1.0483718828277824 1.008407639419479 0.5230897586861971 0.2862125898829085 0.2858325156426003 23 20 0.9685213780514474 0.9647911640308398 0.9443680277977642 0.9590288981031587\n",
      "24\n",
      "DP 0.9244163756745916 0.911730394112386 0.10395363776824594 0.277211150514036 0.4167993678556769 25 23 0.9797946083442296 0.9800842034847339 0.9818247583181663 0.9801255548905422\n",
      "Linear 0.919324276937784 0.906480942347643 0.10261307518931448 0.2725600185297964 0.2723573394004073 25 22 0.9797946083436857 0.9800842034844635 0.9818247583181663 0.980125554889657\n",
      "spline 0.9632202344316882 0.8403385868488664 0.18837626278564348 0.2820617939950317 0.27705236542576556 24 21 0.9795001803849257 0.9800603580050131 0.9818116829784999 0.9799715820535143\n",
      "25\n",
      "DP 1.1886972271005705 1.1848895713285996 0.15969616009785675 0.31034031330489714 0.34094517413050585 26 24 0.9745780680588498 0.9698911771721068 0.9694139654881876 0.9707276870498612\n",
      "Linear 1.1406285698642027 1.1500829410846518 0.13722536747016784 0.2727011709663579 0.2674652097294777 26 23 0.9747690699522997 0.97011629981831 0.9695165093664231 0.9708900168302215\n",
      "spline 1.2078256830586784 1.196774297664531 0.3551704242281853 0.2757775203727347 0.28827938917790397 25 22 0.9803133363885922 0.9767453755823645 0.977800833274726 0.9774264417037521\n",
      "26\n",
      "DP 0.8794983995662767 0.9227876130391146 0.13839257071069772 0.27776134493045995 0.37438707383294734 27 25 0.975432239945975 0.9741918069082883 0.9717943984502329 0.973430709328615\n",
      "Linear 0.8836982442788035 0.9254805586212622 0.13820943630611654 0.26753041395899535 0.26879406059636995 27 24 0.9754322414316747 0.9741918087584795 0.9717944009608331 0.9734307108293038\n",
      "spline 0.934763929310979 0.9719574301935172 0.22020604472416555 0.28681040647203676 0.27879469769901954 26 23 0.9753106249212953 0.9810092554726858 0.9802493939305955 0.978387521975139\n",
      "27\n",
      "DP 1.143494401004475 0.9846677081074416 0.3024928066988198 0.4243291321778016 0.23304724614262345 28 26 0.9837279336010133 0.9846738573950462 0.9899034472386493 0.9859000203708539\n",
      "Linear 1.0715183359537406 0.9488550937434613 0.2307037907811971 0.29503098973994557 0.2589664538423186 28 25 0.9841459069189112 0.985125326531916 0.9819090362790887 0.9826309682949663\n",
      "spline 1.1084589073100233 0.9438517013770404 0.31609553797297446 0.3091803628384287 0.2699094770337271 27 24 0.984015738164052 0.98493321353118 0.9816791494086595 0.9824424642928278\n",
      "28\n",
      "DP 1.0275099362066298 0.7739488411602385 0.21332526539546895 0.27426089935452597 0.3874109026497967 29 27 0.9813349676000669 0.9849103885568788 0.9824362957728747 0.9828178916434502\n",
      "Linear 1.0172156991570613 0.7725334838268773 0.19539792057088282 0.290708128482417 0.27014565159535564 29 26 0.9813320474620377 0.9779614532052491 0.97388963584683 0.9772776020905241\n",
      "spline 1.0443271955089355 0.8632339189636526 0.2939946871778211 0.30715129334666574 0.2802696711653807 28 25 0.9813564920052713 0.9779917226855388 0.9739097213179504 0.9793230536342675\n",
      "29\n",
      "DP 0.9508073716373413 0.9678742237966365 0.42033206514212035 0.33553317616337053 0.3153911663300817 30 28 0.9893424638380249 0.9845158025730513 0.9817033563337998 0.9847175786441792\n",
      "Linear 0.8839462176349563 0.9683113452427795 0.2656474546627732 0.2853569137840243 0.28225809633857707 30 27 0.983492734731608 0.977485669664379 0.9730599415204348 0.9788950786680957\n",
      "spline 0.9710076957088005 0.9256446798854864 0.5056492848083156 0.29152338197391897 0.29272076752923115 29 26 0.9834830020633812 0.9774740553726697 0.981590111812753 0.9808893823756165\n",
      "30\n",
      "DP 0.8512275387373002 1.1852891562394319 0.11192104486454672 0.3737892711612781 0.4913892248958801 31 29 0.9779808774578322 0.9780766143768918 0.9644920167972404 0.9731579125758646\n",
      "Linear 0.8512629020885594 1.1853686005212967 0.11191517756572243 0.2788403742236369 0.2679629742660657 31 28 0.9779808774578322 0.9780766143768918 0.9644920167972404 0.9731579125758646\n",
      "spline 0.845098352243543 1.2096989268703375 0.276136186793878 0.29769413135006406 0.2780053810697593 30 27 0.97203264507763 0.9710265164684105 0.9556768247330407 0.9657917842370062\n",
      "31\n",
      "DP 1.2199887749169975 1.3656154538528158 0.2814588481436816 0.3739305865435938 0.35127651371429436 32 29 0.9738031123133801 0.9770906008401178 0.9832616664878429 0.9790164105424397\n",
      "Linear 1.1970866569707537 1.3808589428792553 0.07403108674982027 0.26438674736817097 0.27061509250379207 32 28 0.9743810460561684 0.970202083851113 0.9747602393185895 0.9737603329938194\n",
      "spline 1.2725531535755945 1.3948426298832735 0.3902688826210252 0.2738355171493586 0.2839127306706269 31 27 0.9740043026614573 0.9695767727277685 0.9745456173981348 0.973389036949329\n",
      "32\n",
      "DP 0.7995617726838565 1.013974624068706 0.15226796040536916 0.3055680044244155 0.9841548045025024 33 30 0.9818548436834024 0.9813508000554548 0.9907452403476029 0.9857608867550447\n",
      "Linear 0.8716920568709067 1.008048319189376 0.11692776074115191 0.27376115193551975 0.2871522550526124 33 29 0.981868554668224 0.9813731906141078 0.9822395309395997 0.9814855118282364\n",
      "spline 0.8728320708783684 1.0333080687702396 0.22762360337798995 0.2884858761186723 0.28982955169984287 32 28 0.9876472614356917 0.9882613412168397 0.9907135802685627 0.9882392374301459\n",
      "33\n",
      "DP 1.322614015984045 0.9249499980843492 0.32870522090123055 0.6982641194082551 0.30140906024127423 34 31 0.9731512493949289 0.9723356380698961 0.9732783103085938 0.9733946403776019\n",
      "Linear 1.1927102854734422 0.9072476774620578 0.1420750637724899 0.2903266308634263 0.2825370363811502 34 30 0.975051195222048 0.9735806749078015 0.974460841164872 0.9752478995931626\n",
      "spline 1.2600146091352467 0.8742710630186648 0.22734853818689182 0.31056844911257286 0.2930071275872231 33 29 0.9744303713679878 0.9730217847544425 0.9744536277596622 0.9749763176606927\n",
      "34\n",
      "DP 0.8781214539803297 1.0313572118186487 0.18346837169912722 0.3538344723295166 0.6219926327521178 35 32 0.9855536661793041 0.9902249793455797 0.991866475125583 0.9876085330999939\n",
      "Linear 0.865825393224306 1.0225300217676818 0.18287922718479102 0.26755509349141515 0.27514839148156095 35 31 0.9855598027914214 0.990232148601656 0.991865938884571 0.9876148572235564\n",
      "spline 0.9186565816540656 0.9998068162492042 0.2140489477194625 0.2838650097049166 0.29294501908498793 34 30 0.9855459014535444 0.9832729733981571 0.983301492657291 0.9835012655379217\n",
      "35\n",
      "DP 1.0986181823348646 0.9731842606483753 0.15390351290517634 0.28244155487735007 0.3470003734026087 36 33 0.9824823859907146 0.9868835488472854 0.9849215270826488 0.9840561072643486\n",
      "Linear 1.0638617931508874 0.9555801318852224 0.1356052902683563 0.2767533562902358 0.2603168681420522 36 32 0.9824830416606928 0.9799398527088972 0.9763745671256686 0.9793799500215246\n",
      "spline 1.0534950385769075 1.0111938174616353 0.2359959430621287 0.281880221641901 0.26729320254233935 35 31 0.9824084377601109 0.9798632956190423 0.9762865330799555 0.9792930002517527\n",
      "36\n",
      "DP 1.141366035873883 0.9157186070571461 0.14072744059059597 0.3006763854498735 0.6441228492916155 37 34 0.9711549839879781 0.9680553083557343 0.9777440049025754 0.9735430412944193\n",
      "Linear 1.141362335114013 0.9157223919645233 0.14076736538911444 0.27664457404420584 0.2691552933362866 37 33 0.9711549839879781 0.9680553083557343 0.9777440049025754 0.9735430412944193\n",
      "spline 1.1755677047999382 0.9637928928728176 0.24671494134622987 0.2889576218857059 0.27361349001234375 36 32 0.9703295604073493 0.9670602528570654 0.9677133250605309 0.9705335239410414\n",
      "37\n",
      "DP 1.1341674209479775 1.1519858974213055 0.24114083861026742 0.3700386266260446 0.6461939056349413 38 35 0.9756322193016433 0.9796579836634626 0.9814138547773275 0.9793819693141792\n",
      "Linear 1.0778929169404305 1.1381997192581947 0.05406137830611125 0.2625854012890503 0.27628792036788913 38 34 0.9756636138609938 0.9727499063708706 0.9729130932248036 0.9743641711570521\n",
      "spline 1.1770033053211464 1.1072162486282036 0.18104618574486894 0.2905233207298374 0.28868019769458114 37 33 0.9757680939672778 0.9728858962562037 0.9730602893103574 0.9744804744167255\n",
      "38\n",
      "DP 1.1392748062581683 0.8672722708588889 0.13620559671706597 0.32955256649096415 0.3014685827063995 39 36 0.9835239341316382 0.9809940001469294 0.9777125824163253 0.9798567867350632\n",
      "Linear 1.139286965655981 0.8672770792589789 0.1361682673886927 0.2887008057171446 0.27626026952646343 39 35 0.9835239341358429 0.9809940001519674 0.9777125824225961 0.9798567867390731\n",
      "spline 1.2856480198454532 1.0130671422190751 0.22431826679500658 0.3110779646734305 0.28344594769956527 38 34 0.9835113534148612 0.9809841096473859 0.9777038034669496 0.9798471818733407\n",
      "39\n",
      "DP 0.8923056892206059 0.5758077086265971 0.13841373878716612 0.2596082033799305 0.327965905791315 40 36 0.9833770840111933 0.9807972711333772 0.9801654287942012 0.9811778640505204\n",
      "Linear 0.8782602729136885 0.5626325599627068 0.09716140801450117 0.2768707178897575 0.2679199282084742 40 35 0.9833879624895104 0.9808081317220165 0.9887128789691118 0.9839716489802562\n",
      "spline 0.836157958904293 0.6041633312276922 0.3225977726298648 0.3003929669714331 0.29239765748695584 39 34 0.9830031264150174 0.9803843441547178 0.9887083800429335 0.9851475398820492\n",
      "40\n",
      "DP 0.821396446876283 1.1442045409330046 0.14547690577994873 0.5516271213043782 0.5562170549671416 41 37 0.9810557142590969 0.9793167575498921 0.976049746554671 0.9786548817915427\n",
      "Linear 0.8180549792949833 1.1240016840615075 0.1253683596888642 0.2857225086539104 0.2805711095188203 41 36 0.9810579321142291 0.9793188671607778 0.9845986938244129 0.9829830942525309\n",
      "spline 0.8333756424180114 1.1069895287922316 0.20928840208932797 0.30152905969698374 0.29346219277135216 40 35 0.9810170344518523 0.9792745486654264 0.9760011736315581 0.9786038841628162\n",
      "41\n",
      "DP 1.2564248674775387 1.268252011232443 0.26217628956370326 0.3630717607375667 0.3668780744134749 42 38 0.9720473598897084 0.9703443784680735 0.965735371792665 0.9677289531963277\n",
      "Linear 1.2561722881498703 1.2901977009086816 0.18083159533877152 0.29848179247392365 0.2796854617846937 42 36 0.9730768372086974 0.971670395593865 0.9674101291838444 0.9694545830937713\n",
      "spline 1.3230406763478975 1.4025647352875212 0.27656947907393625 0.31597220038197166 0.29232127869914964 40 35 0.9726567511220713 0.9711302751039247 0.9667295673247336 0.9685476714776834\n",
      "42\n",
      "DP 0.8346474809969837 1.1698711035298675 0.16131158825161837 0.553773079717435 0.44407709312935706 43 39 0.968966943926006 0.9634617250293641 0.9636012063074688 0.9658178623246805\n",
      "Linear 0.8440829037956492 1.1678785892180275 0.09518314655067042 0.2812491320402708 0.2916819831530188 43 37 0.9689688969008189 0.9634651407530113 0.9636033843106493 0.9658223350113921\n",
      "spline 0.9066046437238467 1.2275290195058637 0.2262638567803011 0.2972252152840028 0.31063834398798096 41 36 0.9684185896373864 0.9628106178597642 0.9631609493426854 0.9652526914519287\n",
      "43\n",
      "DP 1.08461980206919 1.1011121994143211 0.31725912368220677 0.6133316454527132 1.0206430515945677 44 40 0.9788135060102864 0.9755659513304014 0.9732047911579012 0.9760628323987772\n",
      "Linear 1.0928492840550352 1.0861479775832337 0.1978681059248324 0.27531731271922205 0.30662688796747445 44 38 0.97885241880682 0.9756133439073135 0.9732615431806926 0.9756541579998315\n",
      "spline 1.0957851324709096 1.0822570863345475 0.2847431632683209 0.2801318531310367 0.31079679258019116 42 37 0.9787942450344276 0.9755654364863098 0.9817530604931085 0.9783161260704497\n",
      "44\n",
      "DP 0.9023240522779741 0.8203158960784993 0.34394497660276063 0.32292392765639416 0.8490234446612268 45 41 0.9741097315488676 0.9720085880505619 0.9738351511451817 0.9728510946987148\n",
      "Linear 1.0532290356776506 0.9236185953799825 0.03748400192890888 0.2703570606879061 0.2674701574855806 45 39 0.9741377262030835 0.9720535360291773 0.9738469996419434 0.9729149272563883\n",
      "spline 0.9771120418052877 0.9791283968374396 0.2384639801618168 0.2921307147662732 0.28759568133210145 43 38 0.9740874098199203 0.9719753516439544 0.9737846889939173 0.972874293117703\n",
      "45\n",
      "DP 1.023191791006356 0.7275994083569118 0.15550579485336985 0.5554563344099143 0.44081254336423564 46 42 0.9870032296533374 0.986667616720573 0.9771260532194441 0.983899038348813\n",
      "Linear 1.0218230881282442 0.7085643047300804 0.11180172008601227 0.2917919108478128 0.2732638656375927 46 39 0.986998388400846 0.986659343384257 0.9771094444652882 0.9838820857653565\n",
      "spline 1.033923545157282 0.7313278185237788 0.15947510306739165 0.30211238677575836 0.2757675469064529 44 39 0.9869956570083057 0.9866573638104378 0.9771411808006167 0.9839193253968332\n",
      "46\n",
      "DP 1.3406964526981864 1.2762461488717431 0.12135893035363324 0.27779633913377705 0.23260362746066124 46 42 0.9894852808928101 0.9904940416556673 0.9534880956764437 0.9707081262328673\n",
      "Linear 1.3713695302545554 1.2796907504702886 0.05976822682925814 0.28118764094448667 0.31148747273630717 46 40 0.9894602354444777 0.9904991238392438 0.9535416844245241 0.9707235282112964\n",
      "spline 1.36315178598714 1.3455474167242507 0.25994503464864716 0.30754757393726984 0.3410312498866667 44 39 0.9834581046077446 0.9834722193452511 0.943281949644228 0.9635307183683914\n",
      "47\n",
      "DP 1.1710749104125309 1.0135238506694235 0.10546206971418161 0.3826828796694284 0.46282163028581225 47 43 0.9775500646511533 0.9790133971805152 0.9846339032126966 0.9803194987059699\n",
      "Linear 1.1731537794121871 1.0156890923478035 0.08916257677311475 0.30715014235735955 0.30337993912502575 47 41 0.977550219079004 0.9790135266170189 0.9846340638278309 0.9803196534804881\n",
      "spline 1.2661432591983846 1.017453304233846 0.20896774993773642 0.3068527493717148 0.31293529353602234 45 40 0.9775724758756374 0.9790073626373842 0.9846505130788064 0.9803834947583763\n",
      "48\n",
      "DP 0.9941562985746168 0.904814414372144 0.04799536690363721 0.4703958324863139 0.40636836448551744 48 44 0.9708564644927615 0.9676867691055013 0.968133876014074 0.9685646885041974\n",
      "Linear 0.9956142010401037 0.9046294606872226 0.03912728211036288 0.28700443434873996 0.2822084177464327 48 42 0.9708564707645788 0.9676867767043072 0.9681338903730314 0.9685646984168519\n",
      "spline 0.976741334302531 0.9709732844742862 0.2824062789311444 0.28842816765165474 0.29806198727858174 46 41 0.9766028160437337 0.9744105919465873 0.9764111467636846 0.9752943153735898\n",
      "49\n",
      "DP 1.0828372011228704 1.1914990100401839 0.39628316573219574 0.525610332974042 0.437434957227371 49 45 0.983347854706687 0.9764516637267596 0.9799093888833641 0.9812853496786128\n",
      "Linear 1.0477546589405815 1.2183587469215271 0.20137866563564294 0.2760561400319006 0.28351473231627694 49 43 0.9833570816386153 0.976491956251607 0.9799542066569472 0.9798370382988286\n",
      "spline 1.1086030653500476 1.3265243257432675 0.3213733766855723 0.2810345184599129 0.29855880363721676 47 41 0.9833213227927484 0.9763496205898483 0.97084019029236 0.9768352729673584\n",
      "50\n",
      "DP 1.0852119865669672 1.0921146981408676 0.3216556624724138 0.4723182773642917 0.38175490523373296 50 46 0.984757585235748 0.9850235916441616 0.9822995765831191 0.9830168046272874\n",
      "Linear 1.0690957856158316 1.10637772909474 0.18315218644499037 0.2620468776770604 0.2691082867078293 50 44 0.9847777947833157 0.9850232838518683 0.9822990901689373 0.9830224642609042\n",
      "spline 1.1919903620242422 1.2090010824665274 0.24628436652541313 0.2888989458859461 0.2909302037343338 48 42 0.9847592133203025 0.9850322582057007 0.9823104505274816 0.9830229057696205\n",
      "51\n",
      "DP 0.7880470574622582 0.9259506341410907 0.04443808538439075 0.2682418655220965 0.3301990711201902 51 47 0.9782691560739893 0.9780023165958154 0.9747189755244963 0.9767683673896319\n",
      "Linear 0.7879937480793096 0.9259295013634681 0.044419674149698544 0.2527939024659896 0.2787993260620543 51 45 0.9782691560739893 0.9780023165958154 0.9747189755244963 0.9767683673896319\n",
      "spline 0.8900437915055636 0.9483909668037603 0.24182887186270954 0.295419949173631 0.2940346985580017 49 43 0.9839516880057961 0.9849152184009531 0.9832573778920514 0.9826678838771412\n",
      "52\n",
      "DP 0.9398519014128501 1.016540439960815 0.07731112128719676 0.30515363107377314 0.545903500890636 52 48 0.9757825408114996 0.9797225327824891 0.9805522567120861 0.9789096830427829\n",
      "Linear 0.9217518369768226 0.9693313987183876 0.05094423738777309 0.280710679328307 0.2969095024694964 52 46 0.9757818941129448 0.9797199024144314 0.9805480694300056 0.9789067011957467\n",
      "spline 1.020997127563349 0.9821511403241461 0.18202155557834976 0.31116888896462536 0.30872629817080205 50 44 0.975762227992699 0.9727551056382834 0.9718137794937176 0.9739340090903797\n",
      "53\n",
      "DP 0.9797957364424248 0.9649121356199185 0.14175549995735076 0.30372918683643585 0.5958383384389181 53 49 0.9803932795942241 0.9742176570257557 0.9707536215228683 0.9749746565555814\n",
      "Linear 0.9454999388816566 0.9646747907671059 0.14228481645715962 0.27856717770568074 0.2950138793372575 53 47 0.9804004215053929 0.9742156606178721 0.9707523960544195 0.974976113905655\n",
      "spline 1.0062096503019158 0.9897755229610875 0.37633920501221474 0.30073404282430927 0.3016133873484509 51 45 0.9798616817607007 0.9727458683824158 0.9707185703423085 0.9738494828689139\n",
      "54\n",
      "DP 0.9905163148796806 1.0174625903773178 0.03838466180207285 0.3643158492149094 0.5374479614859687 54 50 0.9793720350951786 0.9767051403719628 0.9717560791405571 0.9760876325452766\n",
      "Linear 0.990536664841558 1.0174653193646586 0.03834766463973791 0.2797398673609527 0.28747242576798127 54 48 0.9793720350951786 0.9767051403719628 0.9717560791405571 0.9760876325452766\n",
      "spline 1.07149124866075 1.1521248087426799 0.1481926743901607 0.294832423643825 0.30454682992402077 52 46 0.9797613718800002 0.9766187367435417 0.9801977339212982 0.9797234052808017\n",
      "55\n",
      "DP 1.1061362090617652 0.8809015279375871 0.0465280474236588 0.28371993713932386 0.6757201891134639 55 51 0.9766352381642575 0.9753807590567379 0.9766311148625351 0.9779355120725219\n",
      "Linear 1.1061470261241362 0.8809048153616768 0.04671819381828194 0.3126997345597247 0.2707852136376722 55 49 0.9766352381642575 0.9753807590567379 0.9766311148625351 0.9779355120725219\n",
      "spline 1.2513845544094588 0.8957509539547831 0.13218638194247015 0.32632865277137935 0.2883996932644533 53 47 0.982420143385736 0.982365418149728 0.9852458397586576 0.9824380094347059\n",
      "56\n",
      "DP 0.9738161241140175 0.8522828629064592 0.09633917101952277 0.2843511667650537 0.35972605126926294 56 52 0.9810640466107714 0.9796464585808935 0.9799536963611022 0.980675024561108\n",
      "Linear 0.9738884812880453 0.8523732271060106 0.09987434259350118 0.25163479742895145 0.2675550384506324 56 50 0.9810640466099843 0.9796464585800377 0.9799536963603797 0.980675024558321\n",
      "spline 1.0141307001870852 0.9163269379548639 0.24792928746679363 0.2587621431882351 0.28992478492928414 54 48 0.9809936638945769 0.9795564637052079 0.9799362798369311 0.9806064797314497\n",
      "57\n",
      "DP 1.0984247422304732 0.9022404035382149 0.08689601748287198 0.4159796949896879 0.24717624809146033 57 53 0.9769403471360163 0.9747990962062615 0.9699636651538169 0.9733406541484837\n",
      "Linear 1.0733148447801764 0.8203223917861272 0.06871687824160636 0.2539470332200141 0.24621486729352932 57 51 0.9769739667540527 0.9748082998471517 0.9785144035438208 0.9773206130895041\n",
      "spline 1.112818168343919 0.8443163221817972 0.28866115021648164 0.2605364793580134 0.26268301426270607 55 49 0.9765170261698274 0.9742812308198503 0.9693150129080246 0.9733071133493625\n",
      "58\n",
      "DP 0.753599331307685 0.9722391182867305 0.44132740573876617 0.3155670713363958 0.1882311474838461 58 54 0.991736879990617 0.9926186194359217 0.9921490077538 0.9909430075185097\n",
      "Linear 0.7263390741980588 0.9859820471333935 0.3063150498354766 0.27656424333160673 0.2796119122945222 58 52 0.9859309635460283 0.9856862228140618 0.9921510211748273 0.9878167484931533\n",
      "spline 0.7913875883021793 0.9954034680491745 0.5788949542322066 0.292797380849736 0.3068093051338746 56 50 0.991709205987784 0.9925820199096356 0.9921246202626476 0.9913792330698905\n",
      "59\n",
      "DP 1.1877769439567214 1.0079074083679656 0.10241913997292075 0.30565419341871025 0.7862246353852435 59 55 0.9730081101372274 0.9698303378853622 0.9690777509948316 0.970600021829287\n",
      "Linear 1.1877819470697972 1.007883612519561 0.09856969877857967 0.282877921395957 0.28039583703913046 59 53 0.9730081101372078 0.969830337885338 0.9690777509947945 0.9706000218292636\n",
      "spline 1.2042167589359236 1.1240940415184368 0.2601851891130188 0.29098883144704535 0.29320665462859036 57 51 0.9729466878561195 0.9697682694461526 0.9689984281486911 0.970529869837428\n",
      "60\n",
      "DP 1.1368598012518982 1.0092140867611055 0.12241527240070431 0.5730652433575484 0.4143871121073039 60 56 0.96594433160873 0.9699264329810556 0.9717451679708073 0.9688320289730099\n",
      "Linear 1.1133841510167586 1.005904305116707 0.10654429029113206 0.2830543699080517 0.2651428294359003 60 54 0.966010310888063 0.9699559666815771 0.9717608359338383 0.9688789800863952\n",
      "spline 1.296292415853575 1.0967490001147577 0.18494380244690184 0.2913096127177802 0.27116337478109115 57 52 0.9703898863932089 0.9750702107997494 0.97858154512819 0.9747015125452161\n",
      "61\n",
      "DP 0.9585494016858718 1.1747493967843081 0.028862328817663818 0.2963972466345907 0.7915364617655947 61 57 0.9753672085315048 0.973721388227132 0.9741710132477931 0.9739623281737231\n",
      "Linear 0.9583806880987163 1.1748833548248845 0.028838527981079734 0.2820602477234588 0.28839477229839244 61 55 0.9753672085315048 0.973721388227132 0.9741710132477931 0.9739623281737231\n",
      "spline 0.9961832911168645 1.0931425672218467 0.31885935914739494 0.3040924357324461 0.31023763418911415 58 53 0.9752945239236592 0.9736406902715649 0.9740864031986296 0.9738633237509882\n",
      "62\n",
      "DP 1.1121789427728368 1.032278953803864 0.1878907969175179 0.44737677499689366 0.7690174157988351 62 58 0.9730149082099744 0.9766219665044751 0.9796894239179632 0.9752278412379708\n",
      "Linear 1.1177562937634982 1.0166628586749262 0.08602306361792346 0.28038868433030983 0.27678165712579417 62 56 0.9729859918858932 0.9766790307693787 0.9797354143860875 0.9752676644127634\n",
      "spline 1.1359569195049921 1.0934749661402232 0.2589279061909599 0.2998651872535574 0.29224625307607255 59 54 0.9728508293464814 0.9766131497479164 0.9798687849013397 0.9744739980978693\n",
      "63\n",
      "DP 1.1162527838877492 0.9335484584298891 0.2930649673098448 0.37415607858553795 0.6843297411689364 63 59 0.9826251875140575 0.9826430004983606 0.9907529326112055 0.9856427060510617\n",
      "Linear 1.1137203184047357 0.9560347980213739 0.12593953688904838 0.31130503745847343 0.29811029121509214 63 57 0.9827232149706181 0.9826498299543052 0.9822066458981997 0.9819059958173848\n",
      "spline 1.1267571519551534 1.0782689822573974 0.19744654602703277 0.32475739517299546 0.3423567805193418 60 55 0.9825552687172128 0.9825239876874312 0.982126344003338 0.9817774460966363\n",
      "64\n",
      "DP 1.0413835149267316 1.1468433191925758 0.09752874490945798 0.24983616292171426 0.3533580061176703 64 60 0.9693781324191373 0.9721998594092296 0.960226250840648 0.9685154131997321\n",
      "Linear 1.0692217726815678 1.196066187625271 0.091742386887736 0.25441769387706714 0.27866990421947196 64 58 0.9693889603895439 0.9721872574181261 0.9601928266192521 0.9685018039302146\n",
      "spline 1.0771456161797037 1.2536916077524516 0.2798469171420028 0.26527963033436136 0.2904720362517841 61 56 0.969230518935349 0.9719645409223774 0.9600109316861587 0.9683314799707987\n",
      "65\n",
      "DP 1.0405446367949553 1.2821616956732946 0.13155409612073427 0.3753176772858497 0.47644756160842044 65 61 0.9865625204657142 0.9843369107648552 0.9817263430494367 0.9814568056464061\n",
      "Linear 1.0476695782897052 1.2877227893193046 0.11911470478687419 0.29713362113475017 0.291707634907059 65 59 0.9865382359407615 0.984307792355881 0.9816866370638918 0.981411824050147\n",
      "spline 1.0060108819657716 1.3012540018540886 0.2133081400132516 0.29765197732370946 0.3185188494070955 62 57 0.980735369244096 0.9773780159253764 0.9731608107698946 0.9746054831780839\n",
      "66\n",
      "DP 0.8995386467688425 1.249790420627241 0.27338522255864983 0.3029240480016574 0.4979249821265845 66 62 0.9844287846067299 0.9881704433262509 0.9886627334965757 0.9858453579790472\n",
      "Linear 0.897924595470297 1.247050895746565 0.12070496179404737 0.2623407954196286 0.2726512791744279 66 60 0.984469303527182 0.9882558530902475 0.9887915630599668 0.9859322488293811\n",
      "spline 0.9612404602660999 1.2624260860468626 0.18263253444654579 0.2590728545519491 0.2829532243324158 63 58 0.9763796163171828 0.9881444979808988 0.988740632935009 0.9843484255926455\n",
      "67\n",
      "DP 0.9393747204119145 1.0150019565252248 0.030582422270799825 0.517649049325265 0.4412117914215277 67 63 0.9770231706622068 0.9745298208805319 0.9819723222852035 0.978338292746422\n",
      "Linear 0.9393932803329874 1.0149794838293165 0.03033737677654298 0.2794696061043073 0.2619344591191825 67 61 0.977023170662207 0.9745298208805321 0.9819723222852037 0.9783382927464221\n",
      "spline 0.8767087919788461 1.0895496916367207 0.31592214307477073 0.2811389477507313 0.28834706570429036 64 59 0.9753691328645203 0.9724877018964501 0.9792321251370124 0.9760955806192715\n",
      "68\n",
      "DP 0.9827026725941104 1.0144471882158395 0.11385020316589126 0.2354327810445021 0.614697726183148 68 64 0.9738507653670756 0.9837633172220579 0.9822943691771611 0.9796378059275193\n",
      "Linear 0.9561061255622261 0.9911300635111606 0.09567181808261936 0.2742541561845528 0.2789903779689607 68 62 0.973858549876908 0.9837720993139025 0.9823029536675203 0.9796348205225268\n",
      "spline 1.0161203763281428 1.0167091408142979 0.1842946743370324 0.2902917224865236 0.2964256139184874 65 60 0.9736365232843401 0.976595816957354 0.9735743830611052 0.9745241281333792\n",
      "69\n",
      "DP 1.2392177669518267 0.9694519573937818 0.10404483659338286 0.28865553503233965 0.3375766200889731 69 65 0.9754154051375186 0.9789402382293302 0.9803421639174863 0.9775153804704118\n",
      "Linear 1.2468832723718775 0.9767998796489707 0.10749241744322652 0.27270392019933143 0.27457213068650443 69 63 0.9754153868194704 0.9789402161369365 0.980342127443016 0.9775153535423592\n",
      "spline 1.247021860369445 1.0129150643466 0.19342078973687676 0.2819653247281668 0.29015057706553643 66 61 0.9750988094360313 0.9716472540394561 0.9716851783217172 0.9740992230152666\n",
      "70\n",
      "DP 1.3687648534207475 1.197106084349795 0.09690614736352454 0.30743001675379716 0.3563988701881744 69 66 0.9798098678583286 0.9791308024168579 0.9785685820424345 0.9731445544408277\n",
      "Linear 1.3688081326819028 1.1971321060376112 0.09676803148484384 0.286656557397153 0.2790182199641258 69 64 0.9798098678583286 0.9791308024168579 0.9785685820424345 0.9731445544408277\n",
      "spline 1.39622507962929 1.2295977642309348 0.39547736451305415 0.30768805074404554 0.29268729787025144 66 62 0.9794870758652909 0.9787347955482847 0.9781896002791944 0.9724963199540206\n",
      "71\n",
      "DP 1.366938255909988 1.1842524998297048 0.15210252511295155 0.3051599271274756 0.43521530135980696 69 67 0.989979584941068 0.9890581323767912 0.9865871641469104 0.9850643306254698\n",
      "Linear 1.3670021294801684 1.184330544212421 0.152125008707171 0.2978680040480524 0.28456100865282985 69 65 0.989979584941068 0.9890581323767912 0.9865871641469104 0.9850643306254698\n",
      "spline 1.5357679919014582 1.223573814671484 0.45951993822951237 0.33826727185589867 0.30951042466678136 66 63 0.9840399146419512 0.9820055028492696 0.9779076636713477 0.9786234624568488\n",
      "72\n",
      "DP 0.8840119966512832 0.9755637356230867 0.31123009332355384 0.4193581124732987 0.4758516626575865 70 68 0.9790337014718864 0.9857829109285394 0.9850311767426555 0.978631623203833\n",
      "Linear 0.8414550599205398 0.9704403679413086 0.10017975811889644 0.25777890986071206 0.27652074743296745 70 66 0.9791798411779719 0.9789707276632263 0.9766536074483275 0.9735603928504988\n",
      "spline 0.876523540930974 0.9861297517123541 0.1722853939590268 0.2693071774609026 0.28857919104805896 67 64 0.9790290212375667 0.9789239455709777 0.9765845178099948 0.9734262212769129\n",
      "73\n",
      "DP 1.1591659210766956 1.1546522429534212 0.28338600889725046 0.27425538550781026 0.26762143872583705 71 69 0.9766328513679134 0.9748246971385257 0.9817054630011974 0.9793670154039268\n",
      "Linear 1.1610618230324392 1.0722026599421388 0.10438309722419789 0.2981052355056139 0.2911893449144992 71 67 0.9766483306352637 0.974784402827065 0.9816629527909483 0.9793584736502656\n",
      "spline 1.0953969616497785 1.1615005463205714 0.19741829085353285 0.31011990341612455 0.3000724150679465 68 65 0.9765009447665537 0.9745411869722301 0.9728072247399118 0.9766720714003835\n",
      "74\n",
      "DP 1.3341690303372018 1.326897316689419 0.11141606255701873 0.5519530719746931 0.5716598210041232 72 70 0.9824559174795606 0.9821518002243732 0.9886838366118722 0.9838522603807098\n",
      "Linear 1.324804116803184 1.3234102245695376 0.11188186191777144 0.2878563024381257 0.29712735755633274 71 67 0.9824570483704436 0.9821532208835366 0.9886858158803284 0.983853000584446\n",
      "spline 1.4451389604626796 1.4904832030599502 0.5075888025493596 0.30866768318232224 0.32486922559338344 68 65 0.9824146067301975 0.982140591586037 0.9801290559033617 0.9805491210643561\n",
      "75\n",
      "DP 0.78876608159655 1.0835277439091733 0.3405939804572126 0.35015473312007894 0.6233712659318242 73 71 0.9717700179947275 0.9702218846905983 0.9702475178256807 0.970481421621663\n",
      "Linear 0.7641984410757418 0.9737165752416443 0.16980793501997107 0.2839152169522635 0.25528025640189983 72 68 0.9717720121235459 0.9701276424268716 0.9702900290003093 0.9704718432390487\n",
      "spline 0.7824759110441877 1.0509312858701827 0.43989250144436454 0.275524326832714 0.26744136886703757 69 66 0.9718146952027694 0.9702845011183245 0.9704437261829042 0.9705116862113965\n",
      "76\n",
      "DP 1.1805885984629634 1.3689927501404606 0.19634936747258358 0.3435163296967797 0.534322681773049 74 72 0.9790540563883938 0.9840545472635788 0.9830540353823363 0.9822875857697531\n",
      "Linear 1.1805809813846522 1.3690018726822815 0.19622860650672377 0.2847530029186312 0.29441323331762437 73 68 0.9790540563883926 0.9840545472635764 0.9830540353823335 0.9822875857697518\n",
      "spline 1.4159692795170313 1.485549798190274 0.4335727534328471 0.3558342814007447 0.330758473333051 69 66 0.9787315114759929 0.9768723698803023 0.9828151185982739 0.9794680833974304\n",
      "77\n",
      "DP 1.2052961541008234 1.1088140097940744 0.3096604940389917 0.27880889601796655 0.9387051844437431 75 73 0.9737609885323049 0.978192972795283 0.9778500735473173 0.9758039688432869\n",
      "Linear 1.1867987640763678 1.0800982122334981 0.1499073474869299 0.29354382108348337 0.279681282215225 74 69 0.9738050626123596 0.9713001543729041 0.9693228928625024 0.9706342081753471\n",
      "spline 1.269878463033461 1.146812514613499 0.2807721701455495 0.3069053524088377 0.2927980656643022 70 67 0.9734657095485394 0.970919342783008 0.9773770237759236 0.9746708528647581\n",
      "78\n",
      "DP 1.0800836935964573 1.29597754967903 0.09699495569067644 0.3508228225510915 1.1968402563217502 76 74 0.9752528932223337 0.9706643665210994 0.9709734420209526 0.9732843883087108\n",
      "Linear 1.0350425772047147 1.269680396993701 0.07352648207533857 0.2903345570043495 0.28997315020949566 75 70 0.9753345505916557 0.9708460797802321 0.9710146625340927 0.9733974989449894\n",
      "spline 1.1572647554194362 1.412527477408507 0.3028675793200332 0.30176723283770207 0.3124510366733238 71 67 0.9751925476698933 0.9705658785452604 0.9709292809811442 0.9732153528039998\n",
      "79\n",
      "DP 0.9667602463711379 1.0530263141777212 0.3793645666764067 0.4803161873002003 0.70403905669118 77 75 0.9870219058558618 0.9854260093187026 0.9847674873789288 0.9840710474734338\n",
      "Linear 0.9025639681747467 1.0312981595136737 0.23478636526077337 0.28502012833879253 0.29217747349283 76 71 0.9870185945827585 0.9854215374627123 0.9847646707961599 0.9829858825864223\n",
      "spline 0.9415801789012652 1.072235055627367 0.2718189470715636 0.29670289276240547 0.30940576446868084 72 68 0.9870095471299982 0.9853967369355757 0.984764620559792 0.9829703600551513\n",
      "80\n",
      "DP 1.1250349502094588 1.1795725431541126 0.2830877564147552 0.2383820222965245 0.3804229704404895 78 76 0.9810447654903719 0.9809710834342936 0.9779551960410251 0.9798362056702322\n",
      "Linear 1.1203933461644253 1.1196117301648658 0.05334895484934538 0.2850598697774725 0.2629525929678614 77 72 0.9810822303628826 0.980904534328361 0.977981504072718 0.9798786827133223\n",
      "spline 1.1706337901260842 1.2319609085614576 0.16403297314381285 0.29262881003752106 0.26374793821687936 73 69 0.981035462120309 0.9809453984944636 0.9779161799392364 0.979819309767267\n",
      "81\n",
      "DP 1.0981949220818443 0.9127717746073171 0.2462363787259291 0.35384546042236964 0.3527777320347478 79 77 0.9820918823975029 0.9811921906896288 0.9783684739940025 0.9801755028869457\n",
      "Linear 1.120871241782196 0.8933917666269031 0.2456723651079547 0.2765558175303628 0.2597641092915214 78 73 0.982090542539789 0.9811925034292015 0.9783688615661321 0.980175698276619\n",
      "spline 1.053982968211702 0.8652288983924955 0.3830362571328775 0.29310521611233437 0.2739491724774872 74 70 0.9820718851815975 0.9811717183832849 0.9783633150991985 0.980156759243965\n",
      "82\n",
      "DP 0.946398111339654 0.7105579804784701 0.25240859319791714 0.3115061476749569 0.37601115448784217 80 78 0.9608009461090813 0.9622260173156203 0.9575774930238115 0.9588921028754339\n",
      "Linear 0.9479251912119647 0.7325132010534662 0.04373628543069892 0.29286459185036173 0.28324246855070645 79 74 0.9664363436401033 0.96906800964007 0.9660040181209009 0.964787907996233\n",
      "spline 0.982766682495358 0.7336111985530425 0.2470968507219045 0.30169421670231045 0.28853846939764793 75 71 0.9606080201200198 0.9622042415071492 0.9575634046495425 0.9588100093730867\n",
      "83\n",
      "DP 1.0632817787022175 1.180192360642908 0.06269351694045953 0.43354064455197616 0.7921316918062001 81 79 0.9838786245047121 0.9818224581285218 0.9790277614703916 0.9809622897824459\n",
      "Linear 1.0632667652304713 1.1801813806624846 0.0627102935623253 0.2713961820051319 0.2627802936362097 80 75 0.9838786245047121 0.9818224581285218 0.9790277614703916 0.9809622897824459\n",
      "spline 1.0690550228854214 1.192309277524494 0.16222378570300044 0.2888717893429978 0.27651910086679415 76 72 0.9838671203804801 0.9818139978663201 0.9790201354818158 0.9809480609248618\n",
      "84\n",
      "DP 0.9425676710762292 0.861854350256213 0.0738659249419177 0.3828744252289807 0.48774135286586084 82 80 0.9829659605549247 0.9799312861676033 0.9851272514166941 0.982479838612596\n",
      "Linear 0.9461570189451639 0.9016494219808672 0.0681720774025249 0.26744914139573545 0.28219546650013766 81 76 0.9829597842417046 0.9799261770357535 0.9851205804680363 0.9824739576157897\n",
      "spline 1.007188269487234 0.8883005239390976 0.2911314799512821 0.2879911780326554 0.29443196662819393 77 73 0.9829028390374037 0.9798526950385649 0.9764834932565336 0.9790405283520666\n",
      "85\n",
      "DP 0.9388536131172721 0.9824908091098224 0.32098688298186717 0.3452593561691738 0.3780706349762105 83 81 0.9794108338213479 0.9778501989046307 0.9784260100495223 0.9791389496038696\n",
      "Linear 0.9754572307596415 1.018251831668344 0.13035554908491512 0.2557227331522278 0.3025390342315905 82 77 0.9793862605773167 0.9846336048398723 0.9869698346143205 0.9836114891438706\n",
      "spline 1.0625711527501693 0.8889586088253559 0.2260612757158212 0.2716812353143045 0.3260930877884525 78 74 0.9789057095899035 0.9777172880398203 0.9784227177818142 0.9789744971383874\n",
      "86\n",
      "DP 0.978569189709698 1.226532215402159 0.13529082188179178 0.27753300727590213 0.6659295628295756 84 82 0.9704583423386972 0.9714123590532766 0.9706976710754934 0.9719295172027387\n",
      "Linear 0.9756875207541956 1.223838967338454 0.09738665081998053 0.28884362651916473 0.2741282280468792 83 78 0.9704943222406144 0.9714127793028815 0.9706980769262183 0.9719304944727104\n",
      "spline 1.0011940033836955 1.2178666290285232 0.2220808927629231 0.28352624519913444 0.29213096216875845 79 75 0.9705122170193202 0.9713973758620698 0.9791768221506237 0.975251619832386\n",
      "87\n",
      "DP 1.085310605933014 0.9765934445165965 0.23492973679281562 0.32870930018498035 0.7281136112906421 85 83 0.9632440096241294 0.9629894246736164 0.965925275312006 0.9646776199133915\n",
      "Linear 1.006813890831421 0.9297967943222906 0.03666137569102241 0.2944189408713717 0.2952027736722867 84 79 0.9641871306176435 0.9643120315899282 0.9664375445009485 0.9656292217104485\n",
      "spline 1.1365270562687932 0.9667566295773021 0.3226737486135529 0.313679200985491 0.3065308476276427 80 76 0.9620702568683929 0.9687429059634709 0.9733270149129682 0.9681492496343039\n",
      "88\n",
      "DP 0.992788562451866 0.8261900528375569 0.26131831150737167 0.2645881205122396 0.443960715751701 86 84 0.9820734293220568 0.9866445246056239 0.9747438064051498 0.9816743925928062\n",
      "Linear 1.0010002455115163 0.7758782117231542 0.05426535695868247 0.264745617535113 0.2667826994204404 85 80 0.9822436428947525 0.9798927681303304 0.9668381379505275 0.9767375059359742\n",
      "spline 0.9929489092900398 0.8258589982707999 0.29759070879739585 0.28832457216771473 0.2851099287893825 81 77 0.9820284288908385 0.9865913071399794 0.9748411641817851 0.9816447080642496\n",
      "89\n",
      "DP 1.2004188823854058 1.1197950098068827 0.3448621234490029 0.4271802791392597 0.445748279157445 87 85 0.9768998118531458 0.9746393140347479 0.9800491873214566 0.9780414707728757\n",
      "Linear 1.326384430345308 1.147039694428795 0.12375967389033969 0.3226238876691202 0.30318548061358036 85 81 0.9769482245451305 0.9746793030880402 0.9800643835031734 0.9780761507962985\n",
      "spline 1.4348683107685616 1.1765298081943592 0.27998214447696557 0.3475860712017726 0.30524143506922863 81 78 0.9826879483023244 0.9815417035927118 0.980012148339509 0.9796043727146821\n",
      "90\n",
      "DP 0.9109791688433652 1.1885085361029954 0.12809255776292483 0.3804529870455848 0.39228873171362577 88 86 0.9840483173540471 0.9857417458407426 0.9832134743756464 0.9838134259108788\n",
      "Linear 0.8649202059709592 1.2353096790848481 0.06965999489921343 0.27759745851981427 0.29948211398229624 86 82 0.9841009776685983 0.9857585218141108 0.9832212318050264 0.9848119163633995\n",
      "spline 0.9017429758928083 1.2356050035536648 0.316251564491714 0.29232905701667306 0.29902796807507276 82 79 0.9840885172670993 0.9857478741266829 0.9832149061479993 0.9838327312916766\n",
      "91\n",
      "DP 0.6303976754497392 1.0812155689999932 0.29631872440749807 0.32988321767919065 0.41192757375642375 88 87 0.9792100221298468 0.9780250692803104 0.977208578938994 0.9772660625696032\n",
      "Linear 0.5962735383076354 1.1063894882881822 0.11130024643951762 0.2849304698224876 0.30096392204202294 86 83 0.9734447851089677 0.9711561080941642 0.9686658208489902 0.9703460508155908\n",
      "spline 0.6434837042845254 1.1496263167504839 0.16699714218771755 0.30273598892594406 0.3116233596810816 82 80 0.9728565984523325 0.9704259390952058 0.9682286924583607 0.9698228261961662\n",
      "92\n",
      "DP 1.233961779061133 1.0335352676175626 0.2044166437389247 0.24875384701789027 0.41196975963633137 89 88 0.9782734955870369 0.9787957659634019 0.9889261664341177 0.9837776378094711\n",
      "Linear 1.2474056730308551 1.0226172473726145 0.19131730130501945 0.29141039500180305 0.280218895289877 87 84 0.9781863050042854 0.9787962074662089 0.9889275458029475 0.983751325142018\n",
      "spline 1.3204230036535918 0.9781484798498327 0.28377167761340616 0.30901965910234164 0.28325059346325077 82 81 0.9782318209076302 0.9787433446981738 0.9803769329197678 0.9814170217758451\n",
      "93\n",
      "DP 1.1074673231175116 0.9295493643408334 0.1140596456793628 0.263377408671659 0.3726832414944637 90 89 0.9792259837443864 0.9764030947728312 0.9792847023709265 0.9785779942388073\n",
      "Linear 1.1088701571029647 0.9304176498783313 0.11398573190078888 0.2754076869786934 0.27518831448655184 88 85 0.9792260187809925 0.9764031372805566 0.9792847675843329 0.9785780178338653\n",
      "spline 1.1056146914366731 0.9683883709014626 0.23526529186269227 0.29588998908189645 0.2930602854770117 83 82 0.9793061325292812 0.9765326983135505 0.9878354163169024 0.9824844724951085\n",
      "94\n",
      "DP 1.0147590780926639 1.2047998078084827 0.32136728429991157 0.5459470426556122 0.3404927805932436 91 90 0.976739962494189 0.9759783033506776 0.9713919342331561 0.9735494828126277\n",
      "Linear 1.0323286279894415 1.174707762332304 0.05901333295793892 0.28245630226244195 0.2817316604406857 89 86 0.9709348986992495 0.9690520890683403 0.9628772270321042 0.9672438813554195\n",
      "spline 1.03417249411026 1.1844758756732006 0.265853779405 0.3085340696723835 0.2962462156123025 84 83 0.9708972184882327 0.9690023454084464 0.971356846799803 0.9697100307336015\n",
      "95\n",
      "DP 0.7807610392066162 0.8934083424189884 0.08512012797204457 0.44096329384698046 0.45218803933624674 92 91 0.9694737369764095 0.9762588081726538 0.9714396122971719 0.9739953533720149\n",
      "Linear 0.7343413192476165 0.8990791427296578 0.019479633321724733 0.2686953843425254 0.2848803849303326 90 87 0.9693102606129966 0.9690778607099503 0.9626004964907067 0.9685614543819631\n",
      "spline 0.8039228860651295 0.9646917453403923 0.14601323219553544 0.2861732950334195 0.30489250593418527 85 84 0.96929621148378 0.9689476336417139 0.9624395395995586 0.9684059434406911\n",
      "96\n",
      "DP 1.1714340297583425 1.2794136714219249 0.04030340968901814 0.6565271658947777 0.45457007327935856 93 92 0.9742131285630675 0.9762337704888644 0.9745037607967588 0.9761868289506799\n",
      "Linear 1.1714842655771398 1.2794917531721646 0.04030078524409689 0.276255743209592 0.3119712257200313 91 88 0.9742131285630675 0.9762337704888644 0.9745037607967588 0.9761868289506799\n",
      "spline 1.2216219504741466 1.358549786800726 0.1392050161261765 0.2772143351861411 0.31686793730456037 86 84 0.9741748614011132 0.9762332301736594 0.9745341976690065 0.9761902120842321\n",
      "97\n",
      "DP 1.2337985814427093 1.2734810882678897 0.26550904786881513 0.3891492602508916 0.4270102191962544 94 93 0.9761394002902876 0.9747537060865528 0.9765175562820552 0.9758718107298741\n",
      "Linear 1.2098718598835925 1.255907351857733 0.13196141066051734 0.28444977853617004 0.2993852170605006 92 89 0.9771137305066567 0.975312827543686 0.9767088630687124 0.976427436765108\n",
      "spline 1.2723326046872911 1.3172211859901544 0.4421490960757789 0.30693786742149876 0.3225740367418686 87 85 0.9770543093404899 0.9752270609913348 0.9765862983115841 0.9763355270608685\n",
      "98\n",
      "DP 1.0154972633521167 1.3636320319442963 0.16793779658900101 0.3919016854649425 0.39813551288566745 95 94 0.9714240774916012 0.9677087214288758 0.9713733069836348 0.9694091663005242\n",
      "Linear 1.0020670153300217 1.2519757584560807 0.131201318709809 0.2978254500401691 0.3039134338029912 93 90 0.9714030466522194 0.9676924532011943 0.9713479101243394 0.9693940106657013\n",
      "spline 1.0547034627052876 1.3316670394991603 0.3559646628588587 0.30266409418666407 0.32123461252233426 88 85 0.9771777341578435 0.974612590752473 0.9713249188043198 0.9732682857044811\n",
      "99\n",
      "DP 1.0775112083574379 1.3000477892666051 0.1612107950190663 0.33354164237237 0.3047504587012743 96 95 0.9789064796861011 0.9758949686567941 0.962692547328889 0.9701781675333323\n",
      "Linear 1.072235770274808 1.2585122802283744 0.15075346779956134 0.2943157470886308 0.32838968974282506 94 91 0.978819381595855 0.975786180157547 0.9624294614828816 0.9700573035355979\n",
      "spline 1.072862958205538 1.284014628459001 0.22543984683855592 0.3238201863024429 0.34670684884997577 89 86 0.9779626842985066 0.9747600392158285 0.9599212440387594 0.9682287517810786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\znj\\AppData\\Local\\Temp\\ipykernel_17388\\1473708906.py:184: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DP 1.0169135499994868 1.2460435362253288 0.09547852842965758 0.305609186943528 0.37697633376211104 1 1 0.9522652455927721 0.9631388522579951 0.9522894917006346 0.9471699783953547\n",
      "Linear 1.0169165016028152 1.246042447121479 0.09551202285964819 0.2586087404663728 0.287162427222481 1 1 0.9522652455927721 0.9631388522579951 0.9522894917006346 0.9471699783953547\n",
      "spline 1.0823199226777236 1.3211843410288646 0.15295908733881144 0.2833176159234714 0.3015858057784846 1 0 0.9507986707310941 0.9623759814049466 0.9509328195650645 0.9455816748851003\n",
      "1\n",
      "DP 0.941802867394373 0.9049281503682733 0.09110537487674562 0.32868618116690335 0.4106736527384698 2 2 0.9721708005161265 0.971954260744738 0.9691295516236171 0.9702706359412197\n",
      "Linear 0.9363271143320815 0.9246503565934308 0.03193447638031376 0.28975079387380664 0.2561448607799347 2 2 0.9722641741376793 0.9721065802404543 0.9693724121076878 0.9704495605570108\n",
      "spline 1.0382194048264115 0.9947272139379927 0.15920730867587396 0.3026859049737636 0.2826153779016794 2 1 0.9719976564393491 0.9718819139354968 0.9781091716442667 0.9728777700164424\n",
      "2\n",
      "DP 1.0147237605785464 0.7650954245740816 0.16770517382021013 0.29154125373180545 0.5872252212962348 3 3 0.9763683187008039 0.9805012331314156 0.9796774275212713 0.9791486108585952\n",
      "Linear 1.0166680462964386 0.7745831923037395 0.05509009380479184 0.27390378631577855 0.2621450790646762 3 3 0.9763703457456638 0.9805025327711607 0.9796784333061253 0.9791499883958356\n",
      "spline 1.035751532300401 0.7379146128537178 0.34981833729398565 0.27994987713025343 0.27683999920578767 3 2 0.9760508474245352 0.9801869249111358 0.9795909338060247 0.9788348722827174\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m N_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[0;32m     71\u001b[0m     \u001b[39mfor\u001b[39;00m kk \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m         loss, beta1,beta2,REg_,g ,model \u001b[39m=\u001b[39m estimation(N_index\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m,train, valid, test,G_hat_train,G_hat_valid,G_hat_train0,G_hat_valid0,g_)\n\u001b[0;32m     73\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(loss):\n\u001b[0;32m     74\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 364\u001b[0m, in \u001b[0;36mestimation\u001b[1;34m(Net_index, train, valid, test, G_hat_train, G_hat_valid, G_hat_train0, G_hat_valid0, g)\u001b[0m\n\u001b[0;32m    362\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    363\u001b[0m g_train \u001b[39m=\u001b[39m model(X_train)\n\u001b[1;32m--> 364\u001b[0m loss\u001b[39m=\u001b[39mcriterion(T_train,epsilon_train,Z_train,model\u001b[39m.\u001b[39;49mbeta ,g_train,delta_train,G_hat_train)\n\u001b[0;32m    365\u001b[0m loss1 \u001b[39m=\u001b[39m loss\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(loss2):\n",
      "File \u001b[1;32md:\\Users\\znj\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 281\u001b[0m, in \u001b[0;36mMyLoss.forward\u001b[1;34m(self, T, epsilon, Z, beta, g, delta, G_hat)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, T,epsilon,Z ,beta ,g,delta,G_hat):\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlikelihood(T,epsilon,Z,beta ,g,delta,G_hat)\n",
      "Cell \u001b[1;32mIn[5], line 118\u001b[0m, in \u001b[0;36mlikelihood\u001b[1;34m(t, epsilon, Z, beta, g, delta, G_hat)\u001b[0m\n\u001b[0;32m    116\u001b[0m G[torch\u001b[39m.\u001b[39misnan(G)] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    117\u001b[0m G[torch\u001b[39m.\u001b[39misinf(G)] \u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m--> 118\u001b[0m R1 \u001b[39m=\u001b[39m t_re\u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39;49mones((\u001b[39m1\u001b[39;49m,n))\u001b[39m.\u001b[39;49mto(device) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m (t_re\u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m,n))\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mT\n\u001b[0;32m    119\u001b[0m R2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mminimum(t_re\u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m,n))\u001b[39m.\u001b[39mto(device) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (t_re\u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m,n))\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mT,torch\u001b[39m.\u001b[39mones((n,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device)\u001b[39m@\u001b[39m(epsilon_re\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mT)\n\u001b[0;32m    120\u001b[0m R2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mminimum(R2,torch\u001b[39m.\u001b[39mones((n,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device)\u001b[39m@\u001b[39m(delta)\u001b[39m.\u001b[39mT)\u001b[39m*\u001b[39mG\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "samplesize_set = [1000,2000]\n",
    "g_index_set  =  [1,2,3,4]\n",
    "g__set = [g1,g2,g3,g4]\n",
    "z_index_set =[1,2]\n",
    "b_set = [0.15,0.5]\n",
    "#b=0.15对应60%删失率，b=0.5对应40%删失率\n",
    "p_set = [0.3,0.7]\n",
    "folder_path = r'D:\\simulations'\n",
    "for samplesize in samplesize_set:\n",
    "    for g_index in g_index_set:\n",
    "        g_ = g__set[g_index-1]\n",
    "        for z_index in z_index_set:\n",
    "            for b in b_set:\n",
    "                for p in p_set:\n",
    "                    excelname = ' '+str(samplesize)+' '+str(g_index)+' '+str(z_index)+' '+str(b)+' '+str(p)+'.xlsx'\n",
    "                    outcomes_DP = []\n",
    "                    outcomes_spline = []\n",
    "                    outcomes_Linear = []\n",
    "                    cover_num_DP1 = 0\n",
    "                    cover_num_DP2 = 0\n",
    "                    cover_num_Linear1 = 0\n",
    "                    cover_num_Linear2 = 0\n",
    "                    cover_num_spline1 = 0\n",
    "                    cover_num_spline2 = 0\n",
    "                    for i_  in range(100): \n",
    "                        print(i_)\n",
    "                        loss_kk = []\n",
    "                        beta1_kk = []\n",
    "                        beta2_kk = []\n",
    "                        REg_kk = []\n",
    "                        g_kk =[]\n",
    "                        model_kk = []\n",
    "                        data = torch.hstack(dataproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=i_, p=p,a=0,b=b))\n",
    "                        data_np = data\n",
    "                        train, valid, test = cv(data,1)\n",
    "                        train_np = train.detach().numpy()\n",
    "                        valid_np = valid.detach().numpy()\n",
    "                        test_np = test.detach().numpy()\n",
    "                        train0, valid0, test0 = cv(train,1)\n",
    "                        T_train0,epsilon_train0,Z_train,X_train0,delta_train0=dedata(train0)\n",
    "                        T_valid0,epsilon_valid0,Z_valid0,X_valid0,delta_valid0=dedata(valid0)\n",
    "                        T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "                        T_test, epsilon_test, Z_test, X_test, delta_test = dedata(test)\n",
    "                        T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "                        T, epsilon, Z, X, delta = dedata(data)\n",
    "                        G_hat_train = G_hat(T_train,delta_train)\n",
    "                        G_hat_train0 = G_hat(T_train0,delta_train0)\n",
    "                        G_hat_valid = G_hat(T_valid,delta_valid)\n",
    "                        G_hat_valid0 = G_hat(T_valid0,delta_valid0)\n",
    "                        G_hat_test = G_hat(T_test,delta_test)\n",
    "                        G_hat_data = G_hat(T,delta)\n",
    "                        data = data.to(device)\n",
    "                        train, valid, test = cv(data,1)\n",
    "                        T, epsilon, Z, X, delta = dedata(data)\n",
    "                        test_in_data_index = []\n",
    "                        for i in range(len(test[:,0])):\n",
    "                            test_in_data_index.append(torch.where(torch.isin(data[:,0],test[i,0]))[0][0])\n",
    "                        torch.Tensor(test_in_data_index).int()\n",
    "                        '''\n",
    "                        找出test在整个data里对应的index\n",
    "                        '''\n",
    "                        U_test =Uproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=0, p=p,a=0,b=b)[test_in_data_index].to(device)\n",
    "                        U_test[torch.isnan(U_test)]=1e4\n",
    "                        U = Uproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=0, p=p,a=0,b=b).to(device)\n",
    "                        U[torch.isnan(U)]=1e4\n",
    "                        U_unordered = U_test[(U_test>0)&(U_test<U_test.max())]\n",
    "                        U_ordered = U_test[(U_test>0)&(U_test<U_test.max())].sort().values\n",
    "                        len_T = len(U_ordered)\n",
    "                        M0 = (Z_test[:,0]+Z_test[:,1]+g_(X_test)).to(device)\n",
    "                        for N_index in range(4):\n",
    "                            for kk in range(5):\n",
    "                                loss, beta1,beta2,REg_,g ,model = estimation(N_index+1,train, valid, test,G_hat_train,G_hat_valid,G_hat_train0,G_hat_valid0,g_)\n",
    "                                if torch.isnan(loss):\n",
    "                                    break\n",
    "                                loss_kk.append(loss)\n",
    "                                beta1_kk.append(beta1)\n",
    "                                beta2_kk.append(beta2)\n",
    "                                REg_kk.append(REg_)\n",
    "                                g_kk.append(g)\n",
    "                                model_kk.append(model)\n",
    "                        index_kk = torch.where(torch.min(torch.tensor(loss_kk))==torch.tensor(loss_kk))[0]\n",
    "                        std1,std2=variance_estimation(train, valid, test)\n",
    "                        if CI_cover(samplesize,beta1_kk[index_kk].item(),std1,1.0) == 1:\n",
    "                            cover_num_DP1 = cover_num_DP1+1\n",
    "                        if CI_cover(samplesize,beta2_kk[index_kk].item(),std2,1.0) == 1:\n",
    "                            cover_num_DP2 = cover_num_DP2+1\n",
    "                        T_test, epsilon_test, Z_test, X_test, delta_test = dedata(test)\n",
    "                        M = beta1_kk[index_kk]*Z_test[:,0]+beta2_kk[index_kk]*Z_test[:,1]+g_kk[index_kk].reshape(int(samplesize*0.2))-g_kk[index_kk].reshape(int(samplesize*0.2)).mean()\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_DP = [loss_kk[index_kk].item(),beta1_kk[index_kk].item(),beta2_kk[index_kk].item(),REg_kk[index_kk].item(),(std1/np.sqrt(samplesize)).item(),(std2/np.sqrt(samplesize)).item(),cover_num_DP1,cover_num_DP2,q1,q2,q3,Ctau]\n",
    "                        outcomes_DP.append(outcome_DP)\n",
    "                        print('DP',beta1_kk[index_kk].item(),beta2_kk[index_kk].item(),REg_kk[index_kk].item(),(1.96*std1/np.sqrt(samplesize)).item(),(1.96*std2/np.sqrt(samplesize)).item(),cover_num_DP1,cover_num_DP2,q1,q2,q3,Ctau)\n",
    "                        T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train_np)\n",
    "                        T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test_np)\n",
    "                        T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid_np)\n",
    "                        df = pd.DataFrame(np.hstack([T_train,epsilon_train*delta_train,Z_train,X_train]))\n",
    "                        # 创建一个新的列标列表\n",
    "                        new_columns = ['ftime', 'fstatus', 'Z1', 'Z2', 'X1', 'X2', 'X3', 'X4', 'X5']\n",
    "\n",
    "                        # 使用rename方法将列标更改为新列表中的值\n",
    "                        df = df.rename(columns=dict(zip(df.columns, new_columns)))\n",
    "                        a = cmprsk.crr(failure_time = df['ftime'],failure_status = df['fstatus'],static_covariates =df[['Z1','Z2','X1','X2','X3','X4','X5']], failcode=1, cencode=0)\n",
    "                        g_hat = torch.tensor(X_test@a.summary.coefficients.values[2:])\n",
    "                        g_0 = g_(torch.tensor(X_test))\n",
    "                        std1 = a.summary[\"std\"][0]\n",
    "                        std2 = a.summary[\"std\"][1]\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[0],std1*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_Linear1 = cover_num_Linear1+1\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[1],std2*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_Linear2 = cover_num_Linear2+1\n",
    "                        M = torch.tensor(Z_test@a.summary.coefficients.values[0:2]+X_test@a.summary.coefficients.values[2:]-(X_test@a.summary.coefficients.values[2:]).mean()).to(device)\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_Linear = [a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),std1,std2,cover_num_Linear1,cover_num_Linear2,q1,q2,q3,Ctau]\n",
    "                        outcomes_Linear.append(outcome_Linear)\n",
    "                        print('Linear',a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),1.96*std1,1.96*std2,cover_num_Linear1,cover_num_Linear2,q1,q2,q3,Ctau)\n",
    "                        q = 5\n",
    "                        m = 3\n",
    "                        spline_df = np.hstack([cubic_spline(q,X_train[:,0],m),cubic_spline(q,X_train[:,1],m),cubic_spline(q,X_train[:,2],m),cubic_spline(q,X_train[:,3],m),cubic_spline(q,X_train[:,4],m)])\n",
    "                        spline_df = np.hstack([Z_train,spline_df])\n",
    "                        spline_df = pd.DataFrame(spline_df)\n",
    "                        df = pd.DataFrame(np.hstack([T_train,epsilon_train*delta_train,Z_train,X_train]))\n",
    "                        # 创建一个新的列标列表\n",
    "                        new_columns = ['ftime', 'fstatus', 'Z1', 'Z2', 'X1', 'X2', 'X3', 'X4', 'X5']\n",
    "                        # 使用rename方法将列标更改为新列表中的值\n",
    "                        df = df.rename(columns=dict(zip(df.columns, new_columns)))\n",
    "                        a = cmprsk.crr(failure_time = df['ftime'],failure_status = df['fstatus'],static_covariates =spline_df, failcode=1, cencode=0)\n",
    "                        X_test_spline = np.hstack([cubic_spline(q,X_test[:,0],m),cubic_spline(q,X_test[:,1],m),cubic_spline(q,X_test[:,2],m),cubic_spline(q,X_test[:,3],m),cubic_spline(q,X_test[:,4],m)])\n",
    "                        g_hat = torch.tensor(X_test_spline@a.summary.coefficients.values[2:])\n",
    "                        g_0 = g_(torch.tensor(X_test))\n",
    "                        std1 = a.summary[\"std\"][0]\n",
    "                        std2 = a.summary[\"std\"][1]\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[0],std1*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_spline1 = cover_num_spline1+1\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[1],std2*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_spline2 = cover_num_spline2+1\n",
    "                        M = torch.tensor(Z_test@a.summary.coefficients.values[0:2]+X_test_spline@a.summary.coefficients.values[2:]-(X_test_spline@a.summary.coefficients.values[2:]).mean()).to(device)\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_spline= [a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),std1,std2,cover_num_spline1,cover_num_spline2,q1,q2,q3,Ctau]\n",
    "                        outcomes_spline.append(outcome_spline)\n",
    "                        print('spline',a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),1.96*std1,1.96*std2,cover_num_spline1,cover_num_spline2,q1,q2,q3,Ctau)\n",
    "                    df_outcomes_DP = outcome_to_df(outcomes_DP,1)\n",
    "                    df_outcomes_Linear = outcome_to_df(outcomes_Linear,0)\n",
    "                    df_outcomes_spline = outcome_to_df(outcomes_spline,0)\n",
    "                    df_outcomes_DP['cover_rate1']=df_outcomes_DP['cover_rate1'][99]\n",
    "                    df_outcomes_Linear['cover_rate1']=df_outcomes_Linear['cover_rate1'][99]\n",
    "                    df_outcomes_spline['cover_rate1']=df_outcomes_spline['cover_rate1'][99]\n",
    "                    df_outcomes_DP['cover_rate2']=df_outcomes_DP['cover_rate2'][99]\n",
    "                    df_outcomes_Linear['cover_rate2']=df_outcomes_Linear['cover_rate2'][99]\n",
    "                    df_outcomes_spline['cover_rate2']=df_outcomes_spline['cover_rate2'][99]\n",
    "                    df_mean = pd.DataFrame([df_outcomes_DP.mean(),df_outcomes_Linear.mean(),df_outcomes_spline.mean()])\n",
    "                    df_std = pd.DataFrame([df_outcomes_DP.std(),df_outcomes_Linear.std(),df_outcomes_spline.std()])\n",
    "                    filepath = os.path.join(folder_path,excelname)\n",
    "                    writer = pd.ExcelWriter(filepath)\n",
    "                    # 将每个 DataFrame 写入不同的工作表（sheet）中\n",
    "                    df_outcomes_DP.to_excel(writer, sheet_name='DP')\n",
    "                    df_outcomes_Linear.to_excel(writer, sheet_name='Linear')\n",
    "                    df_outcomes_spline.to_excel(writer, sheet_name='spline')\n",
    "                    df_mean.to_excel(writer, sheet_name='mean')\n",
    "                    df_std.to_excel(writer, sheet_name='std')\n",
    "                    writer.save()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f3ed56a6e50d597e18db0e21426aae15710eba3e727edd94da93a2f95f1a3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
