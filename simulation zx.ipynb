{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import scipy.stats as st\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter  \n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "import cmprsk.cmprsk as cmprsk\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "device=\"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "def g1(x):\n",
    "    g = x[:,0] + 2*x[:,1] + 3*x[:,2] + 4*x[:,3] + 5*x[:,4] -15.5\n",
    "    return g\n",
    "def g2(x):\n",
    "    g = (x[:,0]**2 + 2*x[:,1]**2 + x[:,2]**3 + torch.sqrt(x[:,3]+1) + torch.log(x[:,4]+1) -8.6)*2+1.13\n",
    "    return g\n",
    "def g3(x):\n",
    "    y1 = x[:,0]**2*x[:,1]**3 -2* torch.log(x[:,2]+1) + torch.sqrt(x[:,3]*x[:,4]+1) - torch.exp(x[:,4]/2) -8.2\n",
    "    y2 = (x[:,0]+2*x[:,1]-x[:,2]-0.5*x[:,3]+x[:,4])\n",
    "    y3 = (x[:,0]-3*x[:,1]-2*x[:,2]+1.5*x[:,3]+x[:,4])\n",
    "    g = (y1-y2+1.5*y3)*(y1+2*y2-0.5*y3)/20-3.05/2\n",
    "    return g\n",
    "def g4(x):\n",
    "    g = (x[:,0]**2*x[:,1]**3 + torch.log(x[:,2]+1) + torch.sqrt(x[:,3]*x[:,4]+1) + torch.exp(x[:,4]/2))**2/20 -6.0\n",
    "    return g\n",
    "#定义非参数部分函数\n",
    "\n",
    "def inv_func1(beta11,beta12 , g, u, z, x,p):\n",
    "    '''\n",
    "    beta: 线性协变量系数\n",
    "    g:非线性协变量函数\n",
    "    u:均匀分布随机变量\n",
    "    z:线性部分协变量\n",
    "    x:非线性部分协变量\n",
    "    p:主要事件生成概率\n",
    "    '''\n",
    "    hazard = torch.exp(beta11*z[:,0]+beta12*z[:,1]  + g(x))\n",
    "    y = -torch.log(1-(1-(1-u)**(1/(hazard)))/p)\n",
    "    return y\n",
    "#定义主要事件逆概率函数\n",
    "\n",
    "def inv_func2(beta21,beta22,z,u):\n",
    "    '''\n",
    "    beta: 线性协变量系数\n",
    "    u:均匀分布随机变量\n",
    "    z:线性部分协变量\n",
    "    '''\n",
    "    hazard = torch.exp(beta21*z[:,0]+beta22*z[:,1])\n",
    "    rate = torch.exp(hazard)\n",
    "    y = -torch.log(1-u)/rate\n",
    "    return y\n",
    "#定义竞争风险事件的逆概率函数\n",
    "def inv_funcc(a,b,u):\n",
    "    c=u*(b-a)+a\n",
    "    return c\n",
    "#定义删失事件的逆概率函数\n",
    "def dataproduce(n, beta11, beta12, beta21, beta22, g_index, z_index, seed, p, a, b):\n",
    "    '''\n",
    "    n:样本量\n",
    "    beta:线性协变量系数\n",
    "    g_index:非线性协变量函数的选择指标\n",
    "    z_index:线性协变量分布的选择指标\n",
    "    seed:随机数种子\n",
    "    mu:删失随机变量参数\n",
    "    p:生成主要事件概率\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)#设置随机数种子\n",
    "    sigma = 0.5*torch.ones((7,7)) + 0.5*torch.eye(7)\n",
    "    mvnorm = st.multivariate_normal(mean=[0,0,0,0,0,0,0], cov=sigma)#定义Gaussian copula\n",
    "    XZ = torch.from_numpy(2*st.norm.cdf(mvnorm.rvs(n)))\n",
    "    Z = (XZ[:,0:2])*1.2-0.7\n",
    "    X = XZ[:,2:]\n",
    "    if g_index==1:\n",
    "        g = g1\n",
    "    if g_index==2:\n",
    "        g = g2\n",
    "    if g_index==3:\n",
    "        g = g3\n",
    "    if g_index==4:\n",
    "        g = g4\n",
    "    #选择函数g\n",
    "    u =  torch.rand(n)\n",
    "    t = inv_func1(beta11, beta12, g, u, Z, X,p)#生成事件发生时间\n",
    "    index2=~(t>0)\n",
    "    n2=int(sum(index2))\n",
    "    #确定竞争风险事件的指标集和样本量\n",
    "    u=torch.rand(n2)\n",
    "    t2=inv_func2(beta21,beta22,Z[index2],u)\n",
    "    #生成竞争风险事件的数据\n",
    "    t[index2]=t2\n",
    "    epsilon = torch.zeros(n)\n",
    "    epsilon[index2] = 1\n",
    "    epsilon = epsilon+1\n",
    "    #得出事件类型指标\n",
    "    u = torch.rand(n)\n",
    "    C = inv_funcc(a,b,u)\n",
    "    delta = (C >= t)+0.0\n",
    "    t = torch.minimum(t,C)\n",
    "    return t.reshape(n,1),epsilon.reshape(n,1),Z.reshape(n,2),X,delta.reshape(n,1)\n",
    "#定义数据生成函数\n",
    "\n",
    "def likelihood(t, epsilon, Z, beta, g, delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    l = torch.sum(((2-epsilon_re[delta ==1])*((Z@beta)[delta ==1]+(g)[delta ==1]-torch.log((R@torch.exp(Z@beta+g))[delta ==1]))))\n",
    "    return l\n",
    "#定义似然函数\n",
    "def dedata(data):\n",
    "    T = data[:,0:1]\n",
    "    epsilon = data[:,1:2]\n",
    "    Z = data[:,2:4]\n",
    "    X = data[:,4:9]\n",
    "    delta = data[:,9:]\n",
    "    return T,epsilon,Z,X,delta\n",
    "#将整个数据集分成list\n",
    "def cv(data,seed):\n",
    "    train,test = train_test_split(data, train_size=0.80, test_size=0.20,random_state=seed)\n",
    "    train,valid = train_test_split(train, train_size=0.80, test_size=0.20,random_state=seed)\n",
    "    return train,valid,test\n",
    "#划分训练集，验证集，测试集\n",
    "def G_hat(T,delta):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(T, event_observed=1-delta)\n",
    "    km = kmf.survival_function_[\"KM_estimate\"]\n",
    "    return torch.tensor(km.loc[T[:,0]].values.reshape(len(T[:,0]),1)).to(device)\n",
    "#定义删失变量的估计值\n",
    "\n",
    "def likelihood_linear(t, epsilon, Z, X, beta, gamma,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    gamma = gamma.reshape((len(gamma),1))\n",
    "    l = torch.sum(((2-epsilon_re[delta ==1])*((Z@beta)[delta ==1]+(X@gamma)[delta ==1]-torch.log((R@torch.exp(Z@beta+X@gamma))[delta ==1]))))\n",
    "    return l\n",
    "#定义fine and gray model对应的似然函数\n",
    "\n",
    "class Net_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_linear,self).__init__()\n",
    "        self.beta=nn.Parameter(torch.zeros(2).to(device),requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros(5).to(device),requires_grad=True)\n",
    "#定义fine and gray model对应的优化网络\n",
    "        \n",
    "class Myloss_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, t, epsilon, Z, X, beta, gamma,delta,G_hat):\n",
    "        return -likelihood_linear(t, epsilon, Z, X, beta,gamma,delta,G_hat)\n",
    "#定义fine and gray model对应的损失函数\n",
    "\n",
    "       \n",
    "def beta0(data,G_hat_train,G_hat_valid):\n",
    "    train, valid, test = cv(data,1)\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    model  = Net_linear().to(device)\n",
    "    criterion = Myloss_linear()\n",
    "    optimizer=optim.SGD(model.parameters(),lr=0.01)\n",
    "    model.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(5e2)):\n",
    "        optimizer.zero_grad()\n",
    "        loss=criterion(T_train,epsilon_train,Z_train,X_train, model.beta,model.gamma,delta_train,G_hat_train)\n",
    "        loss2 = criterion(T_valid,epsilon_valid,Z_valid,X_valid, model.beta,model.gamma,delta_valid,G_hat_valid)\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > 1e-5:\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model.beta\n",
    "#定义参数部分初始化函数\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net1,self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net2,self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,40),nn.ReLU(),\n",
    "            nn.Linear(40,40),nn.ReLU(),\n",
    "            nn.Linear(40,20),nn.ReLU(),\n",
    "            nn.Linear(20,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net3,self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Sequential(nn.Linear(5,10), nn.ReLU()),\n",
    "            nn.Sequential(nn.Linear(10,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,40), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(40,20), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(20,10), nn.ReLU(), nn.Dropout(0.3)),\n",
    "            nn.Sequential(nn.Linear(10,1))\n",
    "        )\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self,beta_0):\n",
    "        super(Net4,self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(5, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "        self.beta=nn.Parameter(beta_0 ,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "#定义DNN结构\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, T,epsilon,Z ,beta ,g,delta,G_hat):\n",
    "        return -likelihood(T,epsilon,Z,beta ,g,delta,G_hat)\n",
    "#定义损失函数\n",
    "\n",
    "def chooseNet(index,beta_0):\n",
    "    if index ==1:\n",
    "        model=Net1(beta_0 = beta_0)\n",
    "        lr = 5e-2\n",
    "    if index ==2:\n",
    "        model=Net2(beta_0 = beta_0)\n",
    "        lr = 1e-3\n",
    "    if index ==3:\n",
    "        model=Net3(beta_0 = beta_0)\n",
    "        lr = 1e-3\n",
    "    if index ==4:\n",
    "        model=Net4(beta_0 = beta_0)\n",
    "        lr = 5e-4\n",
    "    return model.to(device),lr\n",
    "#定义网络选择函数\n",
    "def REg(g_0, g_hat):\n",
    "    g_hat=g_hat.reshape([len(g_0),])\n",
    "    y = torch.sqrt(torch.mean(((g_hat-torch.mean(g_hat))-g_0)**2)/(torch.mean((g_0)**2)))\n",
    "    return y\n",
    "class h_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(h_net, self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(1,5),nn.ReLU(),\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,10),nn.ReLU(),\n",
    "            nn.Linear(10,5),nn.ReLU(),\n",
    "            nn.Linear(5,1),)\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x \n",
    "\n",
    "class g_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(g_net, self).__init__()\n",
    "        self.module=nn.Sequential(\n",
    "            nn.Linear(5,10),nn.ReLU(),\n",
    "            nn.Linear(10,40),nn.ReLU(),\n",
    "            nn.Linear(40,40),nn.ReLU(),\n",
    "            nn.Linear(40,20),nn.ReLU(),\n",
    "            nn.Linear(20,10),nn.ReLU(),\n",
    "            nn.Linear(10,1))\n",
    "    def forward(self,x):\n",
    "        x=self.module(x)\n",
    "        return x\n",
    "def VE_loss_func(epsilon,delta,Z,h,g):\n",
    "    l = torch.mean((2-epsilon)*delta*(Z-h-g)**2)\n",
    "    return l\n",
    "class VE_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, epsilon,delta,Z,h,g):\n",
    "        return VE_loss_func(epsilon,delta,Z,h,g)\n",
    "    \n",
    "def estimation(Net_index, train, valid, test,G_hat_train,G_hat_valid,G_hat_train0,G_hat_valid0,g):\n",
    "    window = []\n",
    "    positive = 0\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    beta_0 = beta0(train,G_hat_train0,G_hat_valid0)\n",
    "    '''\n",
    "    model = Net(beta_0=beta_0)\n",
    "    criterion = MyLoss()\n",
    "    lr = 1e-3\n",
    "    optimizer = torch.optim.Adam([{'params':model.module.parameters(),'lr':lr},\n",
    "                                {'params':model.beta,'lr':50*lr}])\n",
    "    '''\n",
    "    model,lr = chooseNet(Net_index,beta_0)\n",
    "    criterion = MyLoss()\n",
    "    optimizer = torch.optim.Adam([{'params':model.module.parameters(),'lr':lr},\n",
    "                                {'params':model.beta,'lr':50*lr}])\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    loss3 = (torch.tensor(1e5))\n",
    "    loss4 = (torch.tensor(1e6))\n",
    "    for j in range(int(1000)):\n",
    "        optimizer.zero_grad()\n",
    "        g_train = model(X_train)\n",
    "        loss=criterion(T_train,epsilon_train,Z_train,model.beta ,g_train,delta_train,G_hat_train)\n",
    "        loss1 = loss\n",
    "        if torch.isnan(loss2):\n",
    "            break\n",
    "        g_valid = model(X_valid)\n",
    "        loss2 = criterion(T_valid,epsilon_valid,Z_valid,model.beta ,g_valid,delta_valid,G_hat_valid)\n",
    "        g_test = model(X_test)\n",
    "        loss3 = criterion(T_valid,epsilon_valid,Z_valid,model.beta ,g_valid,delta_valid,G_hat_valid)\n",
    "        loss4 = torch.min(loss3,loss4)\n",
    "        window.append(loss2)\n",
    "        d = 10\n",
    "        if j >100:\n",
    "            criterion_set = torch.mean(torch.tensor(window)[j-d:j]-torch.tensor(window)[j-d-1:j-1])\n",
    "            if criterion_set>0:\n",
    "                positive = positive+1\n",
    "                if loss3 - loss4 > 0.015*torch.abs(loss4):\n",
    "                    break\n",
    "                if positive > 50:\n",
    "                    break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss2, model.beta[0], model.beta[1], REg(g(X_test),g_test),g_test,model\n",
    "\n",
    "\n",
    "def variance_estimation(train, valid, test):\n",
    "    criterion_ve = VE_loss()\n",
    "    T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "    T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test)\n",
    "    T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "    h_net_1=h_net().to(device)\n",
    "    h_net_2=h_net().to(device)\n",
    "    g_net_1=g_net().to(device)\n",
    "    g_net_2=g_net().to(device)\n",
    "    optimizer_1 = torch.optim.Adam(list(h_net_1.parameters())+list(g_net_1.parameters()), lr=0.001)\n",
    "    h_net_1.train()\n",
    "    g_net_1.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(1e4)):\n",
    "        optimizer_1.zero_grad()\n",
    "        loss = criterion_ve(epsilon_train, delta_train, Z_train[:,[0]], h_net_1(T_train), g_net_1(X_train))\n",
    "        loss2 = criterion_ve(epsilon_valid, delta_valid, Z_valid[:,[0]], h_net_1(T_valid), g_net_1(X_valid))\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > (0.1)*torch.abs(loss1):\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "    optimizer_2 = torch.optim.Adam(list(h_net_2.parameters())+list(g_net_2.parameters()), lr=0.001)\n",
    "    h_net_2.train()\n",
    "    g_net_2.train()\n",
    "    loss1 = (torch.tensor(1e5))\n",
    "    loss2 = (torch.tensor(1e6))\n",
    "    for j in range(int(1e4)):\n",
    "        optimizer_2.zero_grad()\n",
    "        loss = criterion_ve(epsilon_train, delta_train, Z_train[:,[1]], h_net_2(T_train), g_net_2(X_train))\n",
    "        loss2 = criterion_ve(epsilon_valid, delta_valid, Z_valid[:,[1]], h_net_2(T_valid), g_net_2(X_valid))\n",
    "        loss1 = torch.min(loss1,loss2)\n",
    "        if loss2-loss1 > (0.1)*torch.abs(loss1):\n",
    "            break\n",
    "        if loss1 >0:\n",
    "            0\n",
    "        else:\n",
    "            break\n",
    "        loss1 = loss2\n",
    "        loss.backward()\n",
    "        optimizer_2.step()\n",
    "    a = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[0]]-h_net_1(T_train)-g_net_1(X_train))**2)\n",
    "    b = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[0]]-h_net_1(T_train)-g_net_1(X_train))*(Z_train[:,[1]]-h_net_2(T_train)-g_net_2(X_train)))\n",
    "    c = b \n",
    "    d = torch.mean((2-epsilon_train)*delta_train*(Z_train[:,[1]]-h_net_2(T_train)-g_net_2(X_train))**2)\n",
    "    std1 = torch.sqrt(d/(a*d-b*c))\n",
    "    std2 = torch.sqrt(a/(a*d-b*c))\n",
    "    return std1,std2\n",
    "def CI_cover(n,estimation, std, parameter):\n",
    "    n = torch.tensor(n)\n",
    "    cover = 0\n",
    "    if (estimation-1.96*std/torch.sqrt(n)<= parameter)&(estimation+1.96*std/torch.sqrt(n)>= parameter):\n",
    "        cover = cover+1\n",
    "    return cover\n",
    "def riskmatrix(epsilon,T,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = T.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    return R\n",
    "def riskmatrix_add(epsilon,T,delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = T.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) < (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1)).to(device)@(delta).T)*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    return R\n",
    "def TP(R,M):\n",
    "    n = len(M)\n",
    "    expM_vector = torch.exp(M).reshape(1,n)\n",
    "    Y_expM_matrix = R*expM_vector#列时间变化，行样本变化\n",
    "    M_order_matrix = (M.reshape(n,1)>M.reshape(1,n)).double()\n",
    "    Numerator = Y_expM_matrix@M_order_matrix#T,M\n",
    "    Denominator = Y_expM_matrix@torch.ones((n,n)).to(device)\n",
    "    TP  = Numerator/Denominator\n",
    "    return TP\n",
    "def FP(R_add,M):\n",
    "    n = len(M)\n",
    "    M_order_matrix = (M.reshape(n,1)>M.reshape(1,n)).double()\n",
    "    Numerator = R_add@M_order_matrix#T,M\n",
    "    Denominator = R_add@torch.ones((n,n)).to(device)\n",
    "    Denominator[Denominator==0]=1\n",
    "    FP = Numerator/Denominator\n",
    "    return FP\n",
    "def auc_t(M,TPR,FPR):\n",
    "    n = len(M)\n",
    "    auc_array = torch.zeros(n)\n",
    "    for i in range(n):\n",
    "        sorted_index = torch.argsort(FPR[i])\n",
    "        fpr_list_sorted =  torch.tensor(FPR[i])[sorted_index]\n",
    "        tpr_list_sorted = torch.tensor(TPR[i])[sorted_index]\n",
    "        auc_array[i] = integrate.trapz(y=tpr_list_sorted, x=fpr_list_sorted)\n",
    "    return auc_array\n",
    "def Stof(S,t):\n",
    "    index = torch.argsort(t[:,0])\n",
    "    t_ordered = t[index]\n",
    "    S_ordered = S[index]\n",
    "    f = (torch.concatenate((torch.ones((1,len(S_ordered))).to(device), S_ordered[:-1])) -S_ordered )/(t_ordered- torch.concatenate((torch.tensor([[0]]).to(device), t_ordered[:-1])))\n",
    "    return f\n",
    "def Gamma_hat(t, epsilon, Z,beta, g, delta,G_hat):\n",
    "    n = len(epsilon)\n",
    "    t_re = t.reshape((n,1))\n",
    "    epsilon_re = epsilon.reshape((n,1))\n",
    "    G = G_hat/G_hat.T\n",
    "    G[torch.isnan(G)] = 0\n",
    "    G[torch.isinf(G)] =-0\n",
    "    R1 = t_re@torch.ones((1,n)).to(device) <= (t_re@torch.ones((1,n)).to(device)).T\n",
    "    R2 = torch.minimum(t_re@torch.ones((1,n)).to(device) >= (t_re@torch.ones((1,n)).to(device)).T,torch.ones((n,1)).to(device)@(epsilon_re-1).T)\n",
    "    R2 = torch.minimum(R2,torch.ones((n,1).to(device)@(delta).T))*G\n",
    "    R = torch.maximum(R1,R2)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S=(R@torch.exp(Z@beta+g))/n\n",
    "    Gamma = ((t>=(t.T)).double()@((2-epsilon)*delta/S))/n\n",
    "    return Gamma\n",
    "def f_hat(t, epsilon, Z,beta, g, delta):\n",
    "    Gamma = Gamma_hat(t, epsilon, Z,beta, g, delta)\n",
    "    n = len(epsilon)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    f = Stof(S,t)\n",
    "    return torch.mean(f,axis = 1)\n",
    "def S_hat(t, epsilon, Z,beta, g, delta):\n",
    "    Gamma = Gamma_hat(t, epsilon, Z,beta, g, delta)\n",
    "    n = len(epsilon)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((n,1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    index = torch.argsort(t[:,0])\n",
    "    S_ordered = S[index]\n",
    "    return torch.mean(S_ordered,axis = 1)\n",
    "def Gamma_real(t,p):\n",
    "    Gamma = -torch.log(1-p*(1-torch.exp(-t)))\n",
    "    return Gamma\n",
    "def f_real(t,  Z,beta, g, p):\n",
    "    n = len(t)\n",
    "    Gamma = Gamma_real(t, p)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((len(g),1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    f = Stof(S,t)\n",
    "    return torch.mean(f,axis = 1)\n",
    "def S_real(t,  Z,beta, g, p):\n",
    "    n = len(t)\n",
    "    Gamma = Gamma_real(t, p)\n",
    "    beta = beta.reshape((len(beta),1))\n",
    "    g = g.reshape((len(g),1))\n",
    "    S = torch.exp(-(Gamma)@torch.exp(Z@beta+g).T)\n",
    "    index = torch.argsort(t[:,0])\n",
    "    S_ordered = S[index]\n",
    "    return torch.mean(S_ordered,axis = 1)\n",
    "\n",
    "def C_tau(auc_array,U,U_ordered, X, Z, g_, p):  \n",
    "    beta = torch.Tensor([1.0,1.0]).to(device)\n",
    "    g = g_(X)\n",
    "    f = f_real(U.reshape((len(U),1)),  Z,beta, g, p)\n",
    "    S = S_real(U.reshape((len(U),1)),  Z,beta, g, p)\n",
    "    t_index = torch.sort(U).indices\n",
    "    t_ordered = U[t_index]\n",
    "    sub_index = torch.where(torch.isin(t_ordered, U_ordered))\n",
    "    w = (2*f*S)[sub_index]\n",
    "    W = torch.trapz(y=w, x=t_ordered[sub_index])\n",
    "    w_tau = w/W\n",
    "    C = torch.trapz(y=auc_array*w_tau, x=t_ordered[sub_index])\n",
    "    return C\n",
    "def cubic_spline(q,u,m):#q+4:样条基个数，u输入向量\n",
    "    T = np.around(np.linspace(0,0.8,q),2)\n",
    "    n = len(u)\n",
    "    B = np.zeros((n,q))\n",
    "    for i in range(n):\n",
    "        B[i,0] = 1\n",
    "        B[i,1] = u[i]\n",
    "        B[i,2] = u[i]**2\n",
    "        B[i,3] = u[i]**3\n",
    "        for j in range(q):\n",
    "            if u[i] > T[j]:\n",
    "                B[i,j] = (u[i] - T[j])**3\n",
    "    return B\n",
    "def outcome_to_df(outcomes,i):\n",
    "    outcomes_colname = ['beta1', 'beta2', 'REg', 'std1', 'std2', 'cover_rate1', 'cover_rate2', 'AUC(25%)','AUC(50%)','AUC(75%)','C^tau']\n",
    "    df_outcomes_DP = (pd.DataFrame(np.array(outcomes)[:,i:])).rename(columns=dict(zip((pd.DataFrame(np.array(outcomes)[:,i:])), outcomes_colname)))\n",
    "    return df_outcomes_DP\n",
    "def TPR_FPR(U_test,M,M0):\n",
    "    U_ordered = U_test[(U_test>0)&(U_test<U_test.max())].sort().values\n",
    "    Y_t = (~(U_test.reshape((len(U_test),1)) < U_ordered.reshape((1,len(U_ordered))))).int().to(device)\n",
    "    '''\n",
    "    Y_i(t_j)对应的矩阵(i,j)\n",
    "    '''\n",
    "    Yt_eM = Y_t*(torch.exp(M0).reshape((len(M0),1)))\n",
    "    W_t = Yt_eM.sum(axis =0).reshape((1,len(U_ordered)))\n",
    "    TPR = ((M.reshape(len(M),1)>M.sort().values.reshape(1,len(M))).double().T)@(Yt_eM/W_t)\n",
    "    '''\n",
    "    mi,tj对应(i,j)\n",
    "    '''\n",
    "    Y_t_add = (~(U_test.reshape((len(U_test),1)) <= U_ordered.reshape((1,len(U_ordered))))).int().to(device)\n",
    "    W_t_add = Y_t_add.sum(axis =0).reshape((1,len(U_ordered)))\n",
    "    FPR = ((M.reshape(len(M),1)>M.sort().values.reshape(1,len(M))).double().T)@(Y_t_add/W_t_add)\n",
    "    return torch.flip(TPR,[0]),torch.flip(FPR,[0])\n",
    "def Uproduce(n, beta11, beta12, beta21, beta22, g_index, z_index, seed, p, a, b):\n",
    "    '''\n",
    "    n:样本量\n",
    "    beta:线性协变量系数\n",
    "    g_index:非线性协变量函数的选择指标\n",
    "    z_index:线性协变量分布的选择指标\n",
    "    seed:随机数种子\n",
    "    mu:删失随机变量参数\n",
    "    p:生成主要事件概率\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)#设置随机数种子\n",
    "    sigma = 0.5*torch.ones((5,5)) + 0.5*torch.eye(5)\n",
    "    mvnorm = st.multivariate_normal(mean=[0,0,0,0,0], cov=sigma)#定义Gaussian copula\n",
    "    X = torch.from_numpy(2*st.norm.cdf(mvnorm.rvs(n)))\n",
    "    if z_index==1:\n",
    "        Z = torch.randint(2,size = [2*n])\n",
    "    else:\n",
    "        Z = torch.randn(2*n)/torch.sqrt(torch.tensor([2.0]))+0.5#生成线性协变量\n",
    "    Z = Z.reshape(n,2)\n",
    "    if g_index==1:\n",
    "        g = g1\n",
    "    if g_index==2:\n",
    "        g = g2\n",
    "    if g_index==3:\n",
    "        g = g3\n",
    "    if g_index==4:\n",
    "        g = g4\n",
    "    #选择函数g\n",
    "    u =  torch.rand(n)\n",
    "    t = inv_func1(beta11, beta12, g, u, Z, X,p)#生成事件发生时间\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DP 1.0798607994960692 1.1015243031856952 0.27060420263006174 0.15357679360003426 0.2332795201959285 1 1 0.9962486028333192 0.9957266784246115 0.9958803555808393 0.995307628149825\n",
      "Linear 0.7375989194171809 0.7942853713098381 0.29305157952251437 0.17917643397101987 0.19814217328794456 0 0 0.9932375241054012 0.9955890852908751 0.9955395659109714 0.9944314875327872\n",
      "spline 1.0517150938275106 1.0946583935158902 0.1147465762571432 0.18057871150782953 0.17408014662899737 1 1 0.9933714941978979 0.9923396193982696 0.9917708180632168 0.9927264468004207\n",
      "1\n",
      "DP 1.2308845753498578 1.2117653565969495 0.2948910954851638 0.2958921636550677 0.16431399406563443 2 1 0.9955337567079853 0.9914170799761851 0.9910124570049096 0.9935350076148356\n",
      "Linear 0.6716043901966477 0.8315832604152699 0.2728828463121075 0.178655173541651 0.18621270236582302 0 1 0.9955292893629026 0.9945221749888016 0.994256301225315 0.9953159303407417\n",
      "spline 1.1777987157387007 1.1696030805159723 0.11442816780624494 0.18235272764930532 0.19246184625074053 2 2 0.9955333893121181 0.9914174125106195 0.9910131174490616 0.9935351758333618\n",
      "2\n",
      "DP 0.8474012993731669 0.8394618859319354 0.20157142106653794 0.1769616020855844 0.17580135108326608 3 2 0.9939639748297906 0.9898069941514529 0.9872310187872446 0.9912289771168902\n",
      "Linear 0.724899427049139 0.5294788981912901 0.28125092889463593 0.18786940829739468 0.19017216525961084 0 1 0.9967878708794116 0.9929086009935405 0.9909545047344177 0.9942608471157249\n",
      "spline 0.9087725975931649 0.9075810971886498 0.053536467610182964 0.19179949895199222 0.18186882791718484 3 3 0.9968682491372213 0.9932831476015629 0.9914646954991531 0.9939631150141892\n",
      "3\n",
      "DP 1.0104690985615747 1.0120450045659035 0.307833018891491 0.1436900813719128 0.16167479224566939 4 3 0.9924101494147075 0.9921916209100387 0.9926792903475528 0.9925136563637603\n",
      "Linear 0.7537974568669856 0.5696645300936442 0.2921304898982695 0.17762903739611496 0.1717365006792723 0 1 0.9923122941773947 0.9921800862700493 0.9926781076166772 0.9924747225879862\n",
      "spline 1.0771225874025894 1.1775028934446727 0.15281428914809486 0.17905067452524068 0.18971126450028072 4 4 0.992488766310459 0.9922994072469717 0.9926795965216042 0.9931330811394625\n",
      "4\n",
      "DP 0.9209423449108223 1.0229348099796558 0.2729822977346167 0.21409834768429048 0.1806429925368066 5 4 0.9934641037158997 0.9957455095936643 0.9951187283339855 0.9944756693456137\n",
      "Linear 0.6244169404073819 0.6377255023332681 0.25893019437802567 0.17300811197076602 0.17107096424952806 0 1 0.9934558432933049 0.9923483521399393 0.9909980514553208 0.9923919807551524\n",
      "spline 0.9089318791979284 0.9917292868033044 0.061450250105203816 0.19229605465950228 0.1827530791211157 5 5 0.9934628299904003 0.9923549792517794 0.9910036698976972 0.9923985463575788\n",
      "5\n",
      "DP 0.8306036050182513 1.1073610522405537 0.33161685971053834 0.15605182358329217 0.1704676243649975 5 5 0.9969617202307546 0.9967625013249695 0.9960851693213326 0.9966924680060426\n",
      "Linear 0.69440485238315 0.8543301528637506 0.27058790787818815 0.16368200765817742 0.17120864406149536 0 2 0.9940787312130184 0.9967625283568993 0.9960870807672548 0.9949945044871015\n",
      "spline 0.8959548241522262 1.1199247359342146 0.059393051481746846 0.16513339176058886 0.17952859074886654 6 6 0.9940816444343418 0.9933739912252024 0.9919752783050307 0.9933438583351852\n",
      "6\n",
      "DP 0.8160660683825477 0.9082708694420037 0.2160480176007369 0.18433314103719656 0.20031770096741336 6 6 0.9897562350264758 0.9879653150840969 0.98848632136872 0.9889703771270777\n",
      "Linear 0.5310307488377595 0.7232060553282446 0.28322186789038023 0.1676280717639745 0.19295520428531662 0 2 0.9925054057163786 0.9912009046819955 0.9925081287675359 0.9920634817469246\n",
      "spline 0.8529342259511312 0.9321032508337116 0.09291320442409852 0.18300401370707364 0.1774096573783236 7 7 0.9896140539646863 0.9877980069602021 0.9882100507908191 0.9888249623044203\n",
      "7\n",
      "DP 1.0191603170141235 1.1097811973189715 0.4210710786697526 0.19736972037908607 0.2245363519417471 7 7 0.9919456802170725 0.9905263132018246 0.9886320381328835 0.9908236157314857\n",
      "Linear 0.5884132410242903 0.6170888827089624 0.2671168008312285 0.17414142574268499 0.17717292092922862 0 2 0.9919083399919794 0.990482447137423 0.9885829743727343 0.9907786544376799\n",
      "spline 1.021886413258439 1.0494703389011233 0.11597704706285338 0.1800936104700712 0.18431481631509025 8 8 0.9921849341230521 0.9908077368896577 0.9889752599260904 0.9911216755136688\n",
      "8\n",
      "DP 0.9282082327795195 1.0704818581689182 0.2540813938417331 0.20500520275386114 0.188194516145313 8 8 0.9911195879369299 0.9916188949406 0.9912912806232677 0.9910507840025872\n",
      "Linear 0.6961054233155056 0.6134129786535812 0.2918245915272446 0.17606446337731915 0.17526368535368944 0 2 0.9910788293302564 0.9915907223034554 0.9912516166023031 0.9913645313731425\n",
      "spline 0.9187829813150282 0.9766478027410663 0.07433909685562033 0.17179253950289566 0.168028016765695 9 9 0.9910886565862669 0.9916023017428429 0.9912678960142468 0.9910237859993289\n",
      "9\n",
      "DP 1.2305208744638327 0.8400704267561312 0.1875103537508704 0.1825692275065235 0.1804248479493964 8 9 0.9936518218313705 0.9935777229043417 0.996324291182649 0.9940073526936093\n",
      "Linear 0.6489381433440489 0.46680434814913235 0.2989253668704724 0.18444309069120654 0.1797177571509682 0 2 0.9936707594329169 0.9935802827486033 0.9922125841970064 0.9931296079289087\n",
      "spline 1.2119361325749776 0.7986551943244939 0.10423453766079585 0.20869947270840702 0.1714498076813587 9 9 0.9936232897386477 0.9935383885893572 0.9921613078461472 0.9930792649461708\n",
      "10\n",
      "DP 0.9576114559241858 0.9127931705440466 0.17381172795047467 0.1828015902975852 0.17266056306684058 9 10 0.9931521912013137 0.9920372994029457 0.9861706676056179 0.9907621683592734\n",
      "Linear 0.8714148060171824 0.5847762245052321 0.2939499174057904 0.18363951625043903 0.17162467634909953 1 2 0.9920891529944705 0.9941729986536665 0.9895934618246093 0.9919245214216025\n",
      "spline 0.9971550668374283 0.8655761953877811 0.07154920019840737 0.18208978162454464 0.16381448886903832 10 10 0.9931824310818695 0.9920736603516488 0.9865468479981738 0.9909040162986622\n",
      "11\n",
      "DP 1.0164861346643088 0.9383356269124116 0.2863544184348685 0.22888135252616668 0.15280257449321993 10 11 0.9912524710619706 0.9915323049467635 0.9900015539841369 0.9911103909767152\n",
      "Linear 0.6440396066395763 0.6244850868047757 0.339023497606353 0.1862072241022611 0.17997276873532875 1 2 0.9914772516523951 0.9915060771432858 0.9940859968302004 0.9920340868099567\n",
      "spline 1.0205164987675794 0.9422527079518518 0.12673527464142825 0.19012216314835648 0.20335679508282084 11 11 0.9943859726405438 0.9949392767066944 0.9941361894623908 0.9936389315241385\n",
      "12\n",
      "DP 1.042433199036258 1.1148408755684276 0.2966634148240778 0.30572121098721455 0.178840611236643 11 12 0.9922484848002598 0.9914429746108571 0.9912300522387012 0.9920575712243995\n",
      "Linear 0.7789816078314348 0.8162326420032616 0.2960024009426647 0.18727410295222335 0.18204906683254748 1 2 0.9920054267585776 0.9911662195848374 0.9912295752087111 0.99190905997989\n",
      "spline 1.0334179248183675 1.0984410432298366 0.10262649558893103 0.18133748791481 0.18633697393577672 12 12 0.9922684091406535 0.9914433147073982 0.9912305872775047 0.9920680996655563\n",
      "13\n",
      "DP 0.8682263905945135 1.0369929222106269 0.22772823374769585 0.22553485415268576 0.2002627602534601 12 13 0.9927799483630676 0.994354271005522 0.9918851113978262 0.9923495482712933\n",
      "Linear 0.6416939933075808 0.6832683549618441 0.2768775681338742 0.16340125866054386 0.1659817713836462 1 2 0.9925793239429409 0.9941167796293353 0.991874192300966 0.9921391745931339\n",
      "spline 0.8617430583837569 0.9970118366040991 0.058067173578084906 0.1654782305739424 0.18810760110785313 13 13 0.9927791493859651 0.9943533081841411 0.9959959727120014 0.9935802035704943\n",
      "14\n",
      "DP 1.0353463558698817 0.8818276591344284 0.14220260965720768 0.1947533789297578 0.1772267343385754 13 14 0.9938763013759588 0.9930096653863336 0.9961927769205204 0.994464761402437\n",
      "Linear 0.8342371254882287 0.7022687299232376 0.281304049860528 0.18738721078816462 0.18920946992696236 2 2 0.9937657886697646 0.99293956243751 0.9920287531428437 0.9930581091085389\n",
      "spline 1.084251007452169 0.9289405045350632 0.056682708075468734 0.18648840307832212 0.18548214718557052 14 14 0.9967692425053089 0.9964035376020652 0.9961970294350561 0.9963473835987438\n",
      "15\n",
      "DP 0.9799526803661222 1.1349491955054103 0.14046321719061336 0.1809840562034669 0.16075481132501732 14 15 0.9900927062296333 0.9896014373546045 0.9894644176570626 0.9893277844016027\n",
      "Linear 0.5902178158904392 0.640211194457722 0.3072109291566629 0.1672786319059243 0.19174464521254822 2 2 0.989693762967977 0.9894989889077173 0.9892924091706814 0.9897752323214716\n",
      "spline 0.9210355922304672 1.0905288658006227 0.04680134999885226 0.16623813585895658 0.200565199379233 15 15 0.9902805352631932 0.9896309776856331 0.9894890202666282 0.9899159762442438\n",
      "16\n",
      "DP 1.026154950232272 1.0331375171148134 0.2924613751754557 0.21734555049271265 0.2612578443479844 15 16 0.9969233900120491 0.9971551719612907 0.9973826438671105 0.9971350414352861\n",
      "Linear 0.6742657306131534 0.8007914712949324 0.2700457028852403 0.16671087097405732 0.19133704680480057 2 2 0.9969221944863438 0.9971537917153266 0.9973809641209402 0.9970090383354699\n",
      "spline 0.965944584029862 1.0457519308989291 0.06014246594446481 0.17895495999911878 0.2057873982982731 16 16 0.9969227467042963 0.9971542901119972 0.9973813619162569 0.9969573407359975\n",
      "17\n",
      "DP 0.9980834081441295 1.1150142657503221 0.39090216199587063 0.20274790136360354 0.23887913285432524 16 17 0.9925926049937518 0.9914557548319589 0.987897903060075 0.9912530827594591\n",
      "Linear 0.7033723185156381 0.8313837264194655 0.262006183057229 0.17801431435815693 0.19093416585555936 2 3 0.9925598211309254 0.9914155719077434 0.987771025948941 0.9911740571040725\n",
      "spline 1.019326361423262 1.0503364060983265 0.06681352223798966 0.19346073890198554 0.2052871522942309 17 17 0.9925852739625811 0.9914441174559572 0.98785818767954 0.9912386458895895\n",
      "18\n",
      "DP 1.093288015920153 1.0498987692078041 0.372162919337672 0.2349845193780987 0.18678899120925338 17 18 0.9897536244672342 0.992516969993853 0.9941107797080719 0.9915988225071546\n",
      "Linear 0.5767704663317642 0.6586924120852842 0.28929308670595516 0.16745153027016474 0.16145191858472452 2 3 0.9897472185418452 0.9892777589698272 0.989988794807254 0.9895717312204134\n",
      "spline 0.9913248789031998 1.0165854425944234 0.06508073627554041 0.1793701805365845 0.17705582812121629 18 18 0.9899899164923556 0.9894292205711543 0.9900783872702286 0.9897293352177776\n",
      "19\n",
      "DP 1.1310723073358806 1.1747930330233476 0.2349322824399421 0.1896656742685729 0.16705540932797497 18 18 0.9943311465429623 0.994563093715575 0.993641954417128 0.9942349851658141\n",
      "Linear 0.9702847047698473 0.7455686763993725 0.25661405038123564 0.18947393400208146 0.16595499196692232 3 3 0.9941998131793901 0.9945576909888642 0.993619928578493 0.9941727518973618\n",
      "spline 1.1328534092505975 1.101888041470512 0.06788821361545817 0.1768872500808487 0.17575681441191768 19 19 0.9943426650821052 0.994579158480819 0.9936445118229051 0.9942491085255006\n",
      "20\n",
      "DP 1.1144224463229642 0.923187197328198 0.23555807775802384 0.23258716496862514 0.18285465172072762 19 19 0.994624573213499 0.9942941363033 0.9935423271396763 0.9943134606321168\n",
      "Linear 0.7416086383080382 0.6603059152107807 0.26424813734640956 0.18085950199335155 0.15337730774169422 3 3 0.9946109878907798 0.9942772471392457 0.9935205926244159 0.9942949056705122\n",
      "spline 1.074081575087011 0.9033651346589762 0.07510750579765413 0.19473042818155262 0.15892064372541417 20 20 0.9946245533992095 0.9942947809630107 0.993542329886442 0.9943134648652403\n",
      "21\n",
      "DP 0.9882395646989119 1.154715500612177 0.2531404032802044 0.20160377959685546 0.19104191762869663 20 20 0.9941746492942649 0.9931478703502654 0.9917922219581812 0.9933948901559195\n",
      "Linear 0.6956714248927689 0.7247053544107059 0.2648197373213665 0.17838595058087084 0.16728359044270852 3 3 0.9941663938959804 0.9931381627037492 0.9917913499507038 0.9933860428577018\n",
      "spline 0.9710284388155551 1.1089345439043037 0.07583567391703552 0.1789567145428491 0.1755807085010122 21 21 0.9941775566009308 0.9931512895299709 0.9917963407863988 0.9933985517001523\n",
      "22\n",
      "DP 0.9792838869313271 1.1095686438649779 0.29857135762835435 0.1969949583813218 0.218392990500189 21 21 0.9946085285932518 0.9937294078969113 0.9973636076346706 0.9949896958913713\n",
      "Linear 0.6872846097466694 0.8404466542126937 0.28352722976324735 0.179560135557349 0.1864892530660341 3 4 0.9942281623804159 0.9971149534071408 0.9973593978131168 0.9954844150697688\n",
      "spline 1.010636980741899 1.0636075790678041 0.0827577104288545 0.1745706136343932 0.20048731710959702 22 22 0.9946079697581658 0.9971184822119438 0.9973626226445128 0.9954459770349666\n",
      "23\n",
      "DP 1.1123152039664852 0.9895152275597778 0.2142432430139293 0.20049148099616948 0.24866571305868584 22 22 0.9937775651908 0.9931415408032833 0.9915811850452716 0.9928312737589168\n",
      "Linear 0.643868403030193 0.6740561822970619 0.2863500294009208 0.1812027822641695 0.16795938295988988 3 4 0.9966477067174447 0.9965173223231499 0.9956635900084292 0.9958768520761342\n",
      "spline 1.113071174353234 0.9970401648240831 0.06559581951927287 0.20168723964226123 0.18604251302639171 23 23 0.9938045955381152 0.9931572050040284 0.9916460896823698 0.9928723209895904\n",
      "24\n",
      "DP 0.7823794985332474 0.9158567379634632 0.2720493595743252 0.2119513997910998 0.2486916427075984 22 23 0.9951381940368689 0.9943224864195033 0.993563497607379 0.9942578262039472\n",
      "Linear 0.5992708542217385 0.7727361360217155 0.2860492248884344 0.19598295543496855 0.18511030940365977 3 4 0.9951250638185347 0.9943095293652777 0.9935466427409957 0.9942552591376584\n",
      "spline 0.86254576742982 1.0281476555098352 0.07561437188023123 0.1995483976608482 0.189151226338927 24 24 0.9980305050807528 0.9977240851236797 0.9976942933365169 0.9970025159237168\n",
      "25\n",
      "DP 1.0605327388925077 1.005476384494788 0.252156825080131 0.2077272184449644 0.19561078285201425 23 24 0.9937044954528542 0.9932516442435737 0.9912517838774646 0.9929285911126617\n",
      "Linear 0.6041711653483022 0.6982908311970754 0.2657991279378557 0.17068250299646304 0.1613133131551275 3 4 0.993567784457045 0.9930802172305859 0.9909982998256893 0.9927375341122752\n",
      "spline 1.114712828078442 1.00411729219807 0.09389954969023896 0.1880060841884782 0.19823442133913305 25 25 0.9937139626268068 0.9932613541340636 0.9912741991482334 0.992941593691609\n",
      "26\n",
      "DP 1.0632847839267885 1.1478078869484316 0.3144073423489485 0.18035295053825723 0.1976671643557061 24 25 0.992466414630171 0.9913885781744696 0.9953564344216509 0.9931089444472291\n",
      "Linear 0.6726320187885452 0.8175850745494798 0.2806308658454066 0.19047040169542762 0.17122123753478088 3 4 0.9950879276010977 0.9944939352635778 0.995281707631644 0.9949820857068952\n",
      "spline 1.0177227797062052 1.1357252495456414 0.09596585460421647 0.18840722997509687 0.17426104490019592 26 26 0.9953472481339984 0.9947771807513217 0.9953565353962717 0.995097979244641\n",
      "27\n",
      "DP 1.0745252446613063 1.0213319843056783 0.22550316456260522 0.1646080036443747 0.22158004768415024 25 26 0.994995566286309 0.9942039201750039 0.9970898037694162 0.9953335924358161\n",
      "Linear 0.7992634045124981 0.6352577036321865 0.3065772531631719 0.18913161290851607 0.19586617574478693 3 4 0.9949982811293537 0.9942020271320724 0.9970887566578177 0.9950982012873704\n",
      "spline 1.061130619062586 1.1056883476274475 0.06687855794960909 0.176902353581892 0.18852446932803713 27 27 0.9950046958364946 0.9942066076426732 0.9929776892665457 0.9947657671313976\n",
      "28\n",
      "DP 0.7487818302512468 0.9728100833657294 0.19797417242221008 0.1759019903653076 0.2063112904367908 25 27 0.9967225662764216 0.9972110499303771 0.9968601248829572 0.9967289358764879\n",
      "Linear 0.5564046276410416 0.6371559088616789 0.3067201263543427 0.17195025356240426 0.1740387672211885 3 4 0.9938399432833839 0.9972099675388294 0.99685882786766 0.9952976787872774\n",
      "spline 0.8315855029405985 1.0005701832447207 0.16055603791310452 0.175585223196857 0.20364277341969017 28 28 0.9938407628477036 0.9938212801760359 0.992744936799012 0.993514294313174\n",
      "29\n",
      "DP 0.8768794971047836 0.9163788847425495 0.26113560763894744 0.18060363307584443 0.19196966203904994 26 28 0.9928975983981406 0.992440996758176 0.9966190710618581 0.9942664972485031\n",
      "Linear 0.6430356596890625 0.808828656400042 0.27382835195772154 0.17391854704560353 0.1733989618511668 3 4 0.9926821123800764 0.9921677880044262 0.9965966561245513 0.9936131424500445\n",
      "spline 0.9823451369489294 0.9759306782040934 0.06710301241700621 0.20876830494911558 0.1894826104047695 29 29 0.9929026639632755 0.9958370243994755 0.9966289059344742 0.9948610981340598\n",
      "30\n",
      "DP 1.0107095344016923 0.9954205370684379 0.1337062194834091 0.17125561555317603 0.17345306047635684 27 29 0.993837438781844 0.9931233914985721 0.9928444725858638 0.9934532928074666\n",
      "Linear 0.6614571535188396 0.7320762424205188 0.27470407535212543 0.18230070921182379 0.1609168346912656 3 4 0.9937225427237142 0.9930546428985408 0.9929047069254873 0.9934028774774115\n",
      "spline 1.0292443539169565 1.0165299713679936 0.1478269814732376 0.18370682548973716 0.17268997446523 30 30 0.9938958940684316 0.9965827014402676 0.9970346221568174 0.9955820379704629\n",
      "31\n",
      "DP 0.9473462216420017 1.0960483441454991 0.15970789720407202 0.18549754503747917 0.18658383136570966 28 30 0.9936554434958137 0.9925378008917527 0.9950842007836844 0.9938572567993311\n",
      "Linear 0.6919696016436883 0.7484575311687189 0.2757150744565333 0.18075271030569845 0.1662422208501657 3 4 0.9935750427283863 0.9958331980822908 0.9949708386106886 0.99406188968168\n",
      "spline 1.109109006742211 1.0541671233742997 0.11592473690613779 0.18835865700980323 0.17402542855406297 31 31 0.9936781727226853 0.9959543605253292 0.995115150852286 0.9942258043777401\n",
      "32\n",
      "DP 0.9446967122253568 0.8776780826700604 0.2546901690428414 0.23337060486506186 0.20631022851535133 29 31 0.9913302040466406 0.9918041059833884 0.9918818223155434 0.990893302009697\n",
      "Linear 0.7151655475445927 0.43058957873272674 0.2948922274935387 0.17077448768586864 0.178074597175657 3 4 0.9909537657547096 0.9945919198279255 0.9959079528494785 0.9927113792835603\n",
      "spline 0.9574684275751003 0.911382875236835 0.10468503632067078 0.16382738885906983 0.1784717355844214 32 32 0.9914668768316426 0.9918009023374562 0.9918746753212303 0.9909452554089019\n",
      "33\n",
      "DP 0.9574599971615313 1.1333123776470386 0.2679307952556928 0.19236752662085196 0.25396379450911877 30 32 0.991167417473729 0.989710843780957 0.991879312367178 0.9907572439856271\n",
      "Linear 0.796727780486177 0.8246898023461121 0.27364296636149205 0.18407536688005907 0.19676839569463744 3 5 0.991154308985332 0.9897000855696675 0.9959657551045269 0.9923745562815497\n",
      "spline 0.8818117351436083 1.0561688033674357 0.06567012959518449 0.18383617456515566 0.18497635819501615 33 33 0.9912558822531715 0.9898149464757938 0.9959988142627086 0.9924456834051645\n",
      "34\n",
      "DP 1.0191216593263175 0.9993086656029769 0.21247893622702063 0.1642309256972 0.20893768297711499 31 33 0.9958342543425505 0.9966022134620327 0.9963372248427274 0.9956422652293453\n",
      "Linear 0.7282252908936 0.6060713497065607 0.28978065810207604 0.1856756770620626 0.1603271238797139 3 5 0.992913932216179 0.9965426421454271 0.9962642998887292 0.9946997149102129\n",
      "spline 1.0000816594652149 0.9638984446253968 0.1099668552693914 0.1900157581317222 0.17511984497212693 34 34 0.992954675517669 0.9932160491229228 0.9922265480258476 0.9926551362434195\n",
      "35\n",
      "DP 0.8617088816300197 1.0155850835880789 0.10592068593040266 0.2126014691229711 0.21700593400766904 32 34 0.9920836405706157 0.9916801482494446 0.9908013802509087 0.9917854683725954\n",
      "Linear 0.4929088403637233 0.6780308223082737 0.2879278250977369 0.1881525614606691 0.18370658917566904 3 5 0.9917743849962617 0.9913908390296255 0.9904436122116014 0.9915305242098158\n",
      "spline 0.8377244327147988 1.0528798511759454 0.0839341162659497 0.19325477597448504 0.1763553667767088 35 35 0.9921752589786887 0.9917978671534213 0.9909477824469909 0.9918228071911673\n",
      "36\n",
      "DP 1.101207115155122 1.2252831694660835 0.19806835348353527 0.20341052437817667 0.25929729919709216 33 35 0.9954266882938084 0.9946371728111233 0.9936990691995279 0.9949065368330751\n",
      "Linear 0.7578625422139235 0.8081875945143668 0.29407152006222 0.18322330334514272 0.17930238391790623 3 5 0.9954152185086477 0.9946279243271535 0.9936870212918799 0.9945019420687444\n",
      "spline 1.1144264469077174 1.2327359623372363 0.06478177887213478 0.18701309965275392 0.18226908656926927 36 35 0.9954277960000388 0.994638467177165 0.9936991370072544 0.9945137398086942\n",
      "37\n",
      "DP 0.8665122463150916 1.0262900193079418 0.25529556130309966 0.17491935800915998 0.24456197705458255 34 36 0.9970904987518217 0.9967759802135641 0.996224510879113 0.996765788554238\n",
      "Linear 0.7134485074515173 0.6377888080514299 0.26358407443680215 0.17882617993316835 0.17805321783906752 3 5 0.9941879910611939 0.993380722689897 0.9921086151731991 0.9934166111864544\n",
      "spline 0.8993849033994558 0.9606783197218559 0.07828720688238845 0.18104472647542535 0.17390930050252337 37 36 0.9942089494529247 0.9967760017768299 0.9962245050006924 0.9953543213092761\n",
      "38\n",
      "DP 1.2718040869874248 1.1650930138185507 0.29041332211920984 0.20873131860191207 0.18618261587339438 34 37 0.9933303491013014 0.9932877057191607 0.9901172231176385 0.992321165197072\n",
      "Linear 0.800691179288612 0.691846676876724 0.27531595471254916 0.1937608974005137 0.19065181954164998 3 5 0.9932823624174368 0.9965935352501095 0.993808272991492 0.9942448290312265\n",
      "spline 1.2557900856888704 1.1358517639384755 0.07223863529404861 0.19386293710040856 0.18914729150278128 37 37 0.993367410323521 0.9933525950094177 0.9945601932498122 0.993861062001944\n",
      "39\n",
      "DP 0.9870940755231745 1.2088143271191014 0.20015172366948966 0.1820525338660443 0.20443996411199605 35 37 0.9913022281321956 0.9907285201965094 0.989602176639254 0.990716702917193\n",
      "Linear 0.646875617621186 0.8216963175087989 0.28189197599642396 0.16124078061307637 0.1634167137300536 3 5 0.9907670065176475 0.989922703944007 0.9891338739578843 0.9902063795882732\n",
      "spline 1.0648705677475712 1.2042425722505143 0.10499122915605474 0.18695470198811445 0.1863664674387428 38 37 0.9914032236684189 0.9908810817484004 0.9897784533813285 0.99082421257466\n",
      "40\n",
      "DP 0.9163045744086572 1.0878251443562152 0.1597225112075878 0.18663154823678005 0.22877164261899258 36 38 0.9935323562010173 0.9923997712690453 0.9909079979871107 0.9926493249352878\n",
      "Linear 0.6174114087473109 0.6595999204917578 0.2911005603147133 0.17789853328870298 0.19051228095248965 3 5 0.9935622323496087 0.9924350894393035 0.9950753972501625 0.9935328839907325\n",
      "spline 0.9405998235284289 0.9810827758611351 0.0692302726421708 0.18792163280540644 0.1892279399399447 39 38 0.993568277237785 0.9924420552878241 0.9909640471905026 0.9927244047783136\n",
      "41\n",
      "DP 0.9704950771240727 1.0634654763161275 0.21698691545182036 0.20195251858165322 0.17890342055638447 37 39 0.9936650132339739 0.9930273151888502 0.991563085865189 0.9929101762547171\n",
      "Linear 0.5937947511802296 0.6926716010082754 0.27321674225263326 0.17258768816786918 0.1590213394665217 3 5 0.9936374560201688 0.9964165853282685 0.9956779738256425 0.9949712073885959\n",
      "spline 0.9571822187091531 1.0568189646016717 0.056751340589687616 0.1887080062945326 0.17526822113782609 40 39 0.9936845812341233 0.9930509570714655 0.9915906672343069 0.9929339552565529\n",
      "42\n",
      "DP 1.0718849561538013 1.194157316668467 0.23787905415523672 0.16477660864314658 0.24261781730234855 38 40 0.9935525887045016 0.992857992451474 0.9918185328328527 0.992848557877543\n",
      "Linear 0.7502734322788353 0.7664039171842455 0.2816954070133176 0.16483664267315565 0.15754296026642434 3 5 0.9935573429225246 0.9928559120465444 0.9959295124697274 0.9940539787859543\n",
      "spline 1.023549727556811 1.1057170405410637 0.07612193973603068 0.1827571460618872 0.18953515730258577 41 40 0.9935832140017647 0.9928659707619478 0.9918268569164957 0.9928666663491998\n",
      "43\n",
      "DP 0.9558595056805289 1.1605342822317615 0.20978292120957823 0.1644587227310763 0.22738017850812062 39 41 0.9947480443764883 0.992613006769905 0.9923928893176784 0.993224129452504\n",
      "Linear 0.6311723076762542 0.7556178630758414 0.29594958105015995 0.16759537581188197 0.1803627424588364 3 5 0.9946700966134192 0.9921335507481486 0.9923118330363181 0.9929936875148848\n",
      "spline 0.9487492705653987 1.1018804986739283 0.0761561892598346 0.16824570291100352 0.17796834677515455 42 41 0.9947437488148887 0.992585553699707 0.992354781758305 0.9931820060452495\n",
      "44\n",
      "DP 1.138440775579213 0.9412014207291479 0.25699920073125065 0.19750822720558206 0.3030328588430058 40 42 0.9897937787389329 0.9883546515493759 0.9862239933435932 0.9886629105201707\n",
      "Linear 0.7938029171842038 0.7627321269103471 0.28984079887933595 0.198962958817565 0.20622814810127177 3 5 0.9898194474723283 0.9885607691463885 0.9864767046786789 0.9890584566168479\n",
      "spline 1.021898146995959 0.9873810423647409 0.08190002203084283 0.19878869868754856 0.2035001337637638 43 42 0.9928385598527927 0.9919464060609421 0.9905785332188732 0.9919321563059995\n",
      "45\n",
      "DP 0.6330357194128353 0.5316008935955567 0.3158544627124367 0.21171681153767605 0.18806892835280448 40 42 0.7568161068111388 0.7541595348369623 0.9318096792742443 0.7938432767465817\n",
      "Linear 0.1379937976930421 0.061964079542890675 0.7297507661711988 0.13659830783876198 0.15471726753447665 3 5 0.9947193876339887 0.9939150372513307 0.989817773136022 0.9932617563883477\n",
      "spline 0.31641281255892295 0.19503741015644485 0.6759827929973747 0.13132174250785947 0.129123488516974 43 42 0.9102691168942094 0.9136756987882546 0.9384761970029651 0.915623516771968\n",
      "46\n",
      "DP 0.8560927172938463 0.956089237436727 0.2716776474548272 0.1972820903838729 0.18778978953974435 41 43 0.9923785994777525 0.9943207875691542 0.9934722023091672 0.9927661794124114\n",
      "Linear 0.5782179815554893 0.6572939671516594 0.28771776194970006 0.18920178740895152 0.167809136018009 3 5 0.9921182915762968 0.9943247467976174 0.9934779273579664 0.9918597853026099\n",
      "spline 0.8006968624091073 0.9230954813056458 0.12496279050396016 0.18071412494316094 0.17477804417079285 43 43 0.9923845221116953 0.9943336185441093 0.9934887546895839 0.9921032698382821\n",
      "47\n",
      "DP 1.0402719375002223 0.8813939278379461 0.2593598056112884 0.18016310990709505 0.17958464798630397 42 44 0.991030418417526 0.990024639209898 0.9911088254021234 0.9908177011333303\n",
      "Linear 0.808673783858707 0.5597552726987153 0.2661340446732479 0.18036158706852648 0.2018373928703998 3 5 0.9940104999259416 0.9935051273616543 0.9952172206607641 0.994030274723501\n",
      "spline 1.0796435054551163 1.0263861843816073 0.06207210800403377 0.18932019621113066 0.1918501002463663 44 44 0.9911936685554998 0.9902212902577558 0.9911147197145898 0.9909394804641221\n",
      "48\n",
      "DP 1.028079535480586 0.7926724217970198 0.21167637647411205 0.19247546689893014 0.20576135642454974 43 44 0.9950001007144414 0.9924417946971011 0.9954127175716297 0.9941888787052765\n",
      "Linear 0.7366895739171091 0.5048033535056008 0.2770284473179293 0.16787771178784439 0.17277668496472384 3 5 0.9949865798759823 0.9924226772496696 0.9953899490120242 0.9940699340366916\n",
      "spline 1.009187058552299 0.8953856506893534 0.15652801608793718 0.1672040970242009 0.18019400049777723 45 45 0.9950148056701154 0.9925903146173936 0.9914885852568768 0.9932383162707631\n",
      "49\n",
      "DP 0.9988452767212801 1.0724231498054528 0.3314995621629566 0.1961206560362677 0.18961940008802639 44 45 0.9925949686544064 0.9962223005297656 0.9954719126550595 0.9947046010768902\n",
      "Linear 0.6742513649628243 0.6957528402915439 0.2791415286143798 0.17714876071554134 0.1753627993792217 3 5 0.992582144899866 0.9927687055979668 0.9912789965266708 0.9923996912688513\n",
      "spline 0.9705806634139496 1.1021537144402327 0.04773795557623311 0.17312696386842402 0.1877098947622234 46 46 0.9926173881375135 0.9928327804214991 0.9913568478871237 0.9924604860187871\n",
      "50\n",
      "DP 0.9221223748049558 1.0040879882654545 0.2125163663244285 0.1958721537626532 0.20719528703809004 45 46 0.9940972000999039 0.9932361040035922 0.9959048843685866 0.9944059420409446\n",
      "Linear 0.792447440898604 0.8369513292072808 0.27206906643475803 0.17822050014085464 0.19371136915658307 3 6 0.9940925124905086 0.9932306232344692 0.9917831875605443 0.9937758349922183\n",
      "spline 1.1220143365785804 1.0037850021824275 0.09092578940928589 0.18654017978353463 0.17924615008977596 47 47 0.9941030490088331 0.9932371416059458 0.9917909172580353 0.9934467749490135\n",
      "51\n",
      "DP 1.106542065537867 1.193049784744323 0.2690047984390106 0.2000424587394153 0.19203616654177164 46 46 0.9911895986251735 0.993069507255376 0.993555566551675 0.9918962472970726\n",
      "Linear 0.6143223694776193 0.7524670243975591 0.25939556844168765 0.1896771013999457 0.17843834014002266 3 6 0.9911479367290879 0.9896345308874251 0.9893773579685433 0.9900817666773695\n",
      "spline 1.0337385756840365 1.1419591356599579 0.15062101389026863 0.201097754077345 0.19748991103626073 48 48 0.9911953566672054 0.9896845656435258 0.98944742823894 0.9901568102649243\n",
      "52\n",
      "DP 1.1420593316534509 1.3984488825441674 0.32750456176303133 0.15093149470719663 0.17707779468516804 47 46 0.9898240678002586 0.9881872870007895 0.9898099709758306 0.9894918830217851\n",
      "Linear 0.641863050793194 0.7879900911004227 0.28459361891511653 0.17004419414137953 0.18800669316218316 3 6 0.9896843055780686 0.9880423568744956 0.9855188639489914 0.9878299978573601\n",
      "spline 1.1104475444104975 1.335995544407368 0.1375801647535006 0.18325342876703335 0.21111996640099648 49 48 0.9898235987613947 0.9881868258399998 0.9856946039578744 0.987999066663774\n",
      "53\n",
      "DP 0.8797117000534219 0.939608254230741 0.24199642754558695 0.20940102629675206 0.1921906770920732 48 47 0.9945655072432971 0.9949882827029266 0.9943266318015012 0.9942153753377904\n",
      "Linear 0.618255920850904 0.7936897782352864 0.2697037427272479 0.19404630353685626 0.1726234742705465 3 6 0.9917724046629717 0.9951269104493516 0.994691966728914 0.9933778385601015\n",
      "spline 0.8833552834922945 1.0265102222813967 0.05951510219245022 0.20855498255857205 0.18739993903449106 50 49 0.9917927165305079 0.9917621582171257 0.9906052505841966 0.9913406895810437\n",
      "54\n",
      "DP 1.0235110880134948 0.9592558837562529 0.3186273515806094 0.17631832704907058 0.28962007524792316 49 48 0.9904660870912863 0.9890288054332486 0.9884290384527907 0.9899192414353841\n",
      "Linear 0.5581044835976104 0.7528706155910972 0.3165566444677674 0.16490095506391295 0.1818242937192583 3 6 0.9933551415047317 0.9924331677819007 0.9925287616925287 0.9922820742583779\n",
      "spline 1.1000020978690959 0.9874170630755184 0.14790695437748044 0.18809604350406173 0.17308684449828896 51 50 0.9904857767787714 0.9890526490454634 0.9884283054367085 0.9897575996076035\n",
      "55\n",
      "DP 1.0041211560075969 1.1173453042319217 0.2786572366018356 0.17411786111455813 0.1635593809531907 50 49 0.993098876644483 0.9960057227472506 0.9956318630794191 0.9944697812805423\n",
      "Linear 0.5543336437166775 0.8504483669867484 0.28150124870299376 0.16116691032698208 0.18487342638576676 3 7 0.9929992206764541 0.9924978664805435 0.9914685836159605 0.9929308922560638\n",
      "spline 0.9828733243124749 1.160252807738062 0.07629955379520527 0.1729037140199172 0.1936122877533742 52 51 0.9931706674071864 0.9927103951626225 0.9916192329012672 0.9928399051098891\n",
      "56\n",
      "DP 1.2965741877807189 0.8721608253449291 0.318646231663608 0.25512508749503787 0.21424630720156704 50 50 0.993321518864081 0.9955433112134089 0.9945910669999576 0.9938991841664981\n",
      "Linear 0.7491128182503904 0.4839392902658431 0.30295837331872405 0.17615033982728817 0.19129131454552048 3 7 0.9932947833805494 0.992122166805024 0.9904379278573532 0.9922686302918061\n",
      "spline 1.0745106374399314 0.8419612911315149 0.09565051238074851 0.18947585306525558 0.19636071715938544 53 52 0.9962135125917644 0.9955549875433582 0.9946051538701783 0.9948430873056278\n",
      "57\n",
      "DP 0.9508703301364618 1.1616491062945304 0.1833639458987908 0.17678849577727362 0.16113572263687662 51 50 0.9961234312056548 0.9958744685157652 0.9952733627855488 0.9951601009631263\n",
      "Linear 0.6853291094363062 0.6495801925627368 0.28622807678567536 0.166530311166807 0.1779907189543145 3 7 0.9931799621506081 0.9958292609884464 0.9952570565518022 0.9945655800767077\n",
      "spline 0.9777101162510909 1.0935988318504755 0.05961203815950831 0.16025520891320735 0.1795610608515578 54 53 0.9932559230992439 0.9925018193482973 0.9911720066193817 0.9924668504601547\n",
      "58\n",
      "DP 1.0578163531939648 1.054983145630209 0.2297652899120052 0.19748110655144663 0.21494851864228756 52 51 0.9912966891896045 0.9924988743432708 0.9911106827481329 0.9915206193223182\n",
      "Linear 0.7381841459510409 0.7287885345184706 0.2747363424937399 0.20487848569396455 0.1988459009218482 3 7 0.9912887051841389 0.9924820081125088 0.9911065870430344 0.9916154892072743\n",
      "spline 1.085851463536982 1.032241912736785 0.12289492865156784 0.2061292393431831 0.1974864861907361 55 54 0.9941854204019094 0.9958891651419122 0.9952265267649433 0.9946765766942218\n",
      "59\n",
      "DP 1.1716990132438863 0.9069673595260483 0.2769020485317196 0.19932396082107914 0.28136700045485696 53 52 0.9945697376766924 0.9943303340096935 0.993117279860698 0.9940545488276662\n",
      "Linear 0.7824508218776564 0.6699847782062248 0.285542682928276 0.16923842501935227 0.18063534532254083 3 7 0.9945631592880437 0.9943298822088573 0.993116829734307 0.9940405088844144\n",
      "spline 1.1318001514756455 0.8973244662878946 0.10336305991769155 0.18968274616205927 0.17571876751935644 56 55 0.9945697349498109 0.994330329449524 0.9931172743751526 0.9945151729777804\n",
      "60\n",
      "DP 1.056089262482966 1.0735865667875586 0.20161933891580305 0.2024492049974804 0.19693408900581225 54 53 0.9953900320164464 0.9945775010759192 0.9934330277022136 0.9946137188035442\n",
      "Linear 0.7694255348748389 0.625622573564082 0.27294011555775133 0.18337084896270095 0.1715638428797263 3 7 0.9953850815319877 0.9945716778142275 0.9934287441981376 0.9952405280177675\n",
      "spline 1.0597270867836721 1.0647099962505078 0.16552883098612642 0.18483302022894524 0.1682276297900847 57 56 0.9982721503101089 0.9979676495485632 0.9975496438486233 0.9973156018344987\n",
      "61\n",
      "DP 1.0009043346153141 1.0200596987480819 0.1101803808769005 0.20087327504044736 0.19563964137594642 55 54 0.9947897570546709 0.9941192671059482 0.9937063722909576 0.9942174305544298\n",
      "Linear 0.6919132759710581 0.8840558893119828 0.30847359639592614 0.16181104657891818 0.17230139274975992 3 8 0.9947863665054435 0.9941188757532561 0.9937039816543676 0.9942146299390016\n",
      "spline 0.9730567155558166 1.0659015492564998 0.10826231730281415 0.17491092408369638 0.17659244687945766 58 57 0.9947902395746092 0.9941198994881324 0.9937063695219976 0.9942176881644382\n",
      "62\n",
      "DP 1.0718795634280462 1.0312589302456274 0.2333899259848645 0.2110951043258938 0.21677639238028712 56 55 0.9948405662025985 0.9942485941809048 0.993487380483459 0.9943384936492454\n",
      "Linear 0.7204799611476534 0.5735545298373531 0.29011286442706313 0.1900573070519106 0.20382276626124696 3 8 0.9948153020002157 0.9942356637063077 0.9934715808394118 0.99431883842159\n",
      "spline 1.0670352341997054 1.006222971690991 0.0797578811043337 0.19604391268869445 0.2029165221923476 59 58 0.9948405347644503 0.9942485459440917 0.993487316944919 0.9946036413208426\n",
      "63\n",
      "DP 0.8517119995053964 1.0703359018309844 0.20831994998364728 0.18824510207038528 0.2027994644284239 57 56 0.9932828422663573 0.9927557944499648 0.9912279974857295 0.9937550756109318\n",
      "Linear 0.5657902070345847 0.8848887312801179 0.2895954941223404 0.16936347470097154 0.16335089530521885 3 9 0.9932829088298472 0.9927557330786313 0.9912282928034425 0.9929646637690579\n",
      "spline 0.9279316813077872 1.1878470380582218 0.09959743422610706 0.18025840753321298 0.18029823635746978 60 58 0.9932852369083274 0.9961474186632505 0.9953436908481906 0.995112481391407\n",
      "64\n",
      "DP 0.9695440263033014 1.0250361685867153 0.17073781863261053 0.19483502545491316 0.19927628242280399 58 57 0.992083808377092 0.9928167550130723 0.9952145021676118 0.9927546484801661\n",
      "Linear 0.7505422781421976 0.6496490921409628 0.2628384895086299 0.18876133204088857 0.1728497015262549 3 9 0.9888287173759819 0.9896949746204602 0.9912171906002087 0.989549013434679\n",
      "spline 0.9780342681573321 1.0512529494477356 0.10760999021771352 0.18839832181645907 0.18297140191501687 61 59 0.9893577347604066 0.9896877057656539 0.995351378621907 0.9909836833640411\n",
      "65\n",
      "DP 0.9598444994438032 1.0679427842167333 0.2460912626619124 0.2075194540615312 0.1943169655156631 59 58 0.9920290627654362 0.9911806089858058 0.9894111111467454 0.9908872039407127\n",
      "Linear 0.5924494170835061 0.6602133568779528 0.27261789009363174 0.16566069907273143 0.1756522999000832 3 9 0.9919534687867014 0.9909832438941121 0.9891903438707388 0.9907365392641482\n",
      "spline 0.9692915306953717 1.1091859424167174 0.0794530991361967 0.18211373249733906 0.18616037378944147 62 60 0.9949186043985998 0.9945902598984537 0.9935504809263279 0.9936851215804365\n",
      "66\n",
      "DP 1.1016681198390301 0.9248634230547306 0.21585173675852354 0.19754860049630146 0.1532318174096151 60 59 0.9938174707559362 0.9939747783725724 0.9933112726885408 0.9937822814475207\n",
      "Linear 0.8270284670791717 0.6646556153930516 0.26195692974155 0.19016874085787233 0.19977782737023903 4 9 0.9937962264621667 0.993961499998999 0.997408416219048 0.9950703311797556\n",
      "spline 1.193570695507938 1.0456627473247133 0.06319704264915667 0.183446727163285 0.20956425620903 62 61 0.9938200093015532 0.9939767025143436 0.9974286801865322 0.9947893827081268\n",
      "67\n",
      "DP 0.8601118776857665 0.8195128791426878 0.24210893761504723 0.23331208759425115 0.17278797091084072 61 59 0.9945687266581064 0.9921869130392262 0.9907091213303264 0.9930434978594636\n",
      "Linear 0.718547947352319 0.5338439765396695 0.28187896721355465 0.17871391572365058 0.17186330211450532 4 9 0.9945955180413081 0.9923414819598705 0.9908925843433938 0.9931356615893383\n",
      "spline 0.9437954291075348 0.9213282747591759 0.05357553229371585 0.18743023801817535 0.18572252896526853 63 62 0.9945986315589144 0.9923521895486556 0.9908986700854752 0.993393054494009\n",
      "68\n",
      "DP 0.827396107945997 1.0969524917360005 0.28510005566072677 0.19636018745416975 0.22901726804431882 62 60 0.9926399931295946 0.9921952891052703 0.9844543474960841 0.989888233022379\n",
      "Linear 0.6336474135471031 0.7398822370516415 0.26666834912689136 0.17244259944826518 0.17853327137998887 4 9 0.992067252736277 0.9917701973444732 0.9826931779798305 0.9894694965706412\n",
      "spline 0.964869319811978 1.0267769354012741 0.0578479755652012 0.18477197013649804 0.19950617191000883 64 63 0.9927346241223048 0.9921848917264844 0.9843951479077194 0.9898750012074786\n",
      "69\n",
      "DP 0.9686967395430304 0.9184485376778998 0.28421445446770904 0.22578685133410206 0.20435974056766554 63 61 0.9911584181023511 0.9908177042852463 0.9911285896887976 0.9913807879821788\n",
      "Linear 0.7715225705997141 0.6912455872873033 0.27476938632161974 0.16868858293458794 0.1943168900212525 4 9 0.9904443138133798 0.9898762153152549 0.9900445146320606 0.9906805767646562\n",
      "spline 0.9420334563981628 1.059454220154814 0.07156989392627795 0.18749292025140024 0.1936071860839039 65 64 0.9912047239495889 0.9908738981030675 0.9911344037902907 0.9914130640790517\n",
      "70\n",
      "DP 0.9752174690246257 1.2489181487513579 0.27684076375514843 0.282052945688757 0.1698821548718877 64 61 0.9932361698778249 0.992054631321384 0.992667448190709 0.9929525201454072\n",
      "Linear 0.6058172817419339 0.691981807160561 0.265208431179453 0.17074674124692354 0.18249012209443213 4 9 0.9932350000107493 0.9920532510668469 0.9967760219575321 0.9939083701878129\n",
      "spline 0.9957457798005898 1.1862346620837194 0.09081858146167064 0.1893081867365611 0.18315274847075244 66 64 0.9932558918610358 0.9920779254443945 0.9926677008754607 0.9930098611687519\n",
      "71\n",
      "DP 1.2262719682915109 1.18984003415737 0.29569638408317095 0.23510280705191838 0.27373551941436836 65 62 0.9940226007542348 0.992969635168153 0.9951831399285169 0.9934104098260108\n",
      "Linear 0.734397559166819 0.8217230119382427 0.2743110356359777 0.18733175085554785 0.1763668226200231 4 9 0.9912296033601581 0.9896843457769399 0.9908284060805526 0.9906587250165051\n",
      "spline 1.1412823294066266 1.1035657374393306 0.18401223486067614 0.20277982080133086 0.2056837348753616 67 65 0.994256801619656 0.9932451208809816 0.9951957324014042 0.9941168618441679\n",
      "72\n",
      "DP 1.1712468910096043 0.9661358560185475 0.21689763917090774 0.24025130504368222 0.17642130803425685 66 63 0.9949250310382612 0.9942039352414388 0.9930658817215074 0.9929345287721874\n",
      "Linear 0.7270209311933507 0.7039649117913401 0.27992800465385137 0.18179895567048326 0.1822553797696314 4 9 0.9949092878672157 0.9941883312425486 0.993064970876353 0.9929192779472763\n",
      "spline 1.1524707755415515 0.9969474487387094 0.16556537668369387 0.19688686964641525 0.19108874511031568 68 66 0.9949591210122299 0.9942392990062001 0.9931017879631372 0.9930523613929555\n",
      "73\n",
      "DP 1.036692466743673 1.0053913973746367 0.2861520279089971 0.21113108511381662 0.23455696651528704 67 64 0.9918441372741356 0.9904171874736747 0.9862148435369644 0.9899157727734592\n",
      "Linear 0.7029096452680267 0.7735442730972936 0.2650786163127255 0.1763748393902826 0.18088715863169716 4 9 0.9916989970606721 0.9902590177184567 0.9856421276134272 0.9896584390485977\n",
      "spline 0.9717965194013743 0.9728137562567465 0.05326859811308046 0.18673249501517697 0.1801535114630097 69 67 0.9919093115961857 0.990496924582803 0.986501593517814 0.9900377196658321\n",
      "74\n",
      "DP 0.980000420384603 1.0969069109634977 0.2814095206550761 0.19008337283907864 0.16361130802970433 68 65 0.9953167336287255 0.9945087933587302 0.9934481571167395 0.9945247974502732\n",
      "Linear 0.6531367433180078 0.7494178906824204 0.27103089548598625 0.17752313952459864 0.1762814283375389 4 9 0.9953152280934946 0.9945071919324336 0.9934465060527731 0.994523073225963\n",
      "spline 0.9868938468497954 1.09037116672821 0.0498859835869548 0.17204054627784454 0.18265732572640614 70 68 0.9953174316219323 0.9945091798635801 0.9934481726321814 0.9945252817386921\n",
      "75\n",
      "DP 1.1136603749949339 1.0832019668466686 0.19624214423473002 0.21656613370391284 0.20854601906134224 69 66 0.9923266479607931 0.9909748994939904 0.9894636540574024 0.9917396684708565\n",
      "Linear 0.7045216843441572 0.7236013813738432 0.27337173629658157 0.18535036762674004 0.1875100404174995 4 9 0.9925710010385473 0.9912623287306924 0.9899127435856129 0.991968927829564\n",
      "spline 1.1281692063382178 1.183238491105435 0.08440418946814122 0.20129623841655167 0.21122538085382347 71 69 0.9923265422608897 0.9909747710589414 0.9894636515165904 0.9917395747920064\n",
      "76\n",
      "DP 0.9385349609405435 1.0944843303541896 0.17684423480184921 0.18086246927597913 0.16986170731508488 70 67 0.9923610518926937 0.9911221538119871 0.9905140206484737 0.9919450016377893\n",
      "Linear 0.605035362982495 0.6550737838867436 0.2782191904655027 0.1602805348424666 0.17810799549150308 4 9 0.992701579156635 0.9949191306363079 0.9954608704733625 0.9945876925247656\n",
      "spline 0.9173111039360173 1.0578571223065432 0.05066506032612891 0.188186265762084 0.18371205195156992 72 70 0.9927069846028116 0.991530830865653 0.9913472621801356 0.9931153529612808\n",
      "77\n",
      "DP 0.9810160940943996 0.8917236245923512 0.2803829849786528 0.2291897293460723 0.21952812795501248 71 68 0.9942290891327483 0.9932390584842011 0.9934772447311299 0.9933752956598357\n",
      "Linear 0.6485487191385081 0.5288648243441498 0.27547842691001095 0.18361054814728475 0.18651559854767832 4 9 0.9942024010329081 0.9966149872790575 0.9975702407498527 0.9951840453257045\n",
      "spline 0.9454217861214231 0.9383595524891987 0.06577123117314124 0.16871774124991623 0.20050095329220458 73 71 0.9971112022000501 0.9966290652069667 0.9975917094637643 0.9959898640082147\n",
      "78\n",
      "DP 0.7927082640333666 0.8382224476877794 0.17962283911700921 0.19510373499350908 0.15347805335254144 71 68 0.9963191086460386 0.9956730877029272 0.995959766676134 0.995994415317203\n",
      "Linear 0.6926958541528003 0.6072837339740023 0.26664709248207746 0.17122504504084257 0.17950494167615055 4 9 0.9934378675662481 0.9922841339462951 0.9918441017643305 0.9927788291108933\n",
      "spline 0.8943070079118671 1.0031965609033868 0.11222307710896931 0.17488361977730488 0.18359395053298116 74 72 0.9934399307375985 0.9922863107732253 0.9918454672137209 0.993167121763509\n",
      "79\n",
      "DP 0.9285955218911232 1.013490384999046 0.3565410162236182 0.1893276992882767 0.1896562123483354 72 69 0.9928199326247011 0.9916602722640718 0.9915117933480013 0.9919044692878477\n",
      "Linear 0.7102498039169968 0.7176036993338202 0.2869428804424349 0.18154651495045313 0.23348566847572721 4 9 0.9928804280802405 0.99173184837114 0.9915170386967507 0.9920763937838698\n",
      "spline 1.0495391839060253 1.054475982129626 0.06483872283534144 0.18326572134829194 0.2061065267061089 75 73 0.9957558906418227 0.9951152015835086 0.9956395559748568 0.9953987147631673\n",
      "80\n",
      "DP 0.8430771213105654 1.1619192510229293 0.25864938699060486 0.19729280689528492 0.18592900551698271 73 70 0.9932987348159509 0.9921626260808536 0.9923658528993784 0.9928541174107169\n",
      "Linear 0.5739331635949099 0.8192037744106654 0.2654241672596115 0.17339401595130666 0.2091758391187059 4 10 0.9920940699217756 0.9941327851057324 0.9948159200884217 0.9934191098451143\n",
      "spline 0.9033990683182884 1.066288473193138 0.04434965100134557 0.1868489908115772 0.20960011922274704 76 74 0.9932991414329473 0.9921630302879253 0.9923665006166049 0.9928486837304373\n",
      "81\n",
      "DP 0.9042018132048278 1.0528277302908768 0.3036738743777195 0.15832727639453334 0.2002591688336425 74 71 0.9937425713040573 0.9927546298749783 0.9912049392389726 0.9927003894498693\n",
      "Linear 0.7059463651264347 0.6772313322044737 0.28560990477839493 0.18604655834486977 0.1715912809171622 4 10 0.9937503957971233 0.9961306516934464 0.9953035411887208 0.9944890502332399\n",
      "spline 0.9030247673749747 1.0941725372546922 0.12220240197465189 0.18204635291400348 0.17770452291010075 77 75 0.9937426174722652 0.9927546730998097 0.9912049428164743 0.992700422350761\n",
      "82\n",
      "DP 0.907527983477588 0.8887303375855148 0.26870504086815744 0.19621844107226272 0.19225712388532293 75 72 0.9933760977448983 0.9931850522708382 0.9927127155520887 0.9930279353385312\n",
      "Linear 0.7093662726383546 0.6129994011817126 0.28383840730251364 0.19075052158572886 0.1912109100893408 4 10 0.9932931035239301 0.9930755529851567 0.9927124359426872 0.9931010849674438\n",
      "spline 0.9498920599984374 0.9636346796016356 0.05639716233051222 0.20331834553756659 0.1786724737007094 78 76 0.9933761636719723 0.9931850739707085 0.9927127168608816 0.9930279568761399\n",
      "83\n",
      "DP 0.8789433690270271 0.7493617756435634 0.15671716863576898 0.16313884321882563 0.16559850320241187 76 72 0.9935196941742168 0.9900535874098633 0.9848788594625272 0.989794730596695\n",
      "Linear 0.7064490807863032 0.49727017210286123 0.2989433066971758 0.1727969718160648 0.15742682832183158 4 10 0.9933599968714415 0.9894523687512298 0.9831106071881494 0.9890120429614961\n",
      "spline 0.9614788949257745 0.732310536947638 0.0736599386733103 0.16619061275110356 0.1666639080003455 79 76 0.996393290699954 0.9934358692671905 0.9889753271902056 0.9923447395774823\n",
      "84\n",
      "DP 1.0214962908042613 0.81220106457953 0.19140110206330818 0.16305313055395562 0.16659170099120824 77 72 0.9909491880429914 0.9916269440724019 0.9898758160883493 0.9917868764444652\n",
      "Linear 0.8074581172621318 0.5298730604036205 0.28886412941521034 0.17233501474990193 0.1791987944187196 4 10 0.990996380807097 0.9916207249507993 0.9898676134875616 0.9909881314749954\n",
      "spline 1.103779570767172 0.8328206100613783 0.06963750640446914 0.19528996121436876 0.1811134238540997 80 77 0.9909073727734876 0.9915487391789173 0.9897791551667817 0.9908964430096614\n",
      "85\n",
      "DP 0.9180038445920652 1.0900452936506648 0.21545270578630024 0.1755215285581787 0.15527745904188955 78 73 0.9917877287242682 0.9940216650549349 0.9936598654802069 0.9929787035458401\n",
      "Linear 0.6253831379847685 0.8219823387702451 0.2809946448864901 0.1935251788462019 0.1923631232904059 4 11 0.9916581193880631 0.9905426374618178 0.9936539201269803 0.9916138520891922\n",
      "spline 0.8572025946522287 1.063088204447026 0.07846762859823195 0.19319308927823145 0.20522201153449818 81 78 0.991788007964501 0.9940219820506655 0.9936602532037817 0.992616671022687\n",
      "86\n",
      "DP 0.9626656967959036 1.0116692604613156 0.2889992241546788 0.20175500169323488 0.21179216760104405 79 74 0.9917358037289132 0.9904652968268174 0.9951194274209937 0.9927883179783343\n",
      "Linear 0.6814905321820511 0.6079155725506821 0.25983269094076306 0.18664545334034593 0.1793278350241541 4 11 0.9945943345120853 0.9938193104431955 0.9951186194309884 0.9938361243173022\n",
      "spline 1.0546844937542064 1.0883134070805542 0.08221210391302011 0.20496632571594142 0.1946751468196019 82 79 0.9915833346595937 0.9936651362638811 0.9951193343990007 0.9934890137516228\n",
      "87\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (208) must match the size of tensor b (207) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m q2 \u001b[38;5;241m=\u001b[39m auc_array[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnum\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     96\u001b[0m q3 \u001b[38;5;241m=\u001b[39m auc_array[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mnum\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 97\u001b[0m Ctau\u001b[38;5;241m=\u001b[39m\u001b[43mC_tau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauc_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU_ordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     98\u001b[0m outcome_DP \u001b[38;5;241m=\u001b[39m [loss_kk[index_kk]\u001b[38;5;241m.\u001b[39mitem(),beta1_kk[index_kk]\u001b[38;5;241m.\u001b[39mitem(),beta2_kk[index_kk]\u001b[38;5;241m.\u001b[39mitem(),REg_kk[index_kk]\u001b[38;5;241m.\u001b[39mitem(),(std1\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(samplesize))\u001b[38;5;241m.\u001b[39mitem(),(std2\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(samplesize))\u001b[38;5;241m.\u001b[39mitem(),cover_num_DP1,cover_num_DP2,q1,q2,q3,Ctau]\n\u001b[0;32m     99\u001b[0m outcomes_DP\u001b[38;5;241m.\u001b[39mappend(outcome_DP)\n",
      "Cell \u001b[1;32mIn[1], line 569\u001b[0m, in \u001b[0;36mC_tau\u001b[1;34m(auc_array, U, U_ordered, X, Z, g_, p)\u001b[0m\n\u001b[0;32m    567\u001b[0m W \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrapz(y\u001b[38;5;241m=\u001b[39mw, x\u001b[38;5;241m=\u001b[39mt_ordered[sub_index])\n\u001b[0;32m    568\u001b[0m w_tau \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m/\u001b[39mW\n\u001b[1;32m--> 569\u001b[0m C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrapz(y\u001b[38;5;241m=\u001b[39m\u001b[43mauc_array\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw_tau\u001b[49m, x\u001b[38;5;241m=\u001b[39mt_ordered[sub_index])\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m C\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (208) must match the size of tensor b (207) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "samplesize_set = [2000]\n",
    "g_index_set  =  [2]\n",
    "g__set = [g2]\n",
    "z_index_set =[2]\n",
    "b_set = [0.15]\n",
    "#b=0.15对应60%删失率，b=0.5对应40%删失率\n",
    "p_set = [0.7]\n",
    "folder_path = r'C:\\Users\\znj\\OneDrive - sjtu.edu.cn\\课题进展及规划\\DNN for partial linear Fine and Gray model\\simulations\\Z与X相关'\n",
    "for samplesize in samplesize_set:\n",
    "    for g_index in g_index_set:\n",
    "        g_ = g__set[0]\n",
    "        for z_index in z_index_set:\n",
    "            for b in b_set:\n",
    "                for p in p_set:\n",
    "                    excelname = ' '+str(samplesize)+' '+str(g_index)+' '+str(z_index)+' '+str(b)+' '+str(p)+'2.xlsx'\n",
    "                    outcomes_DP = []\n",
    "                    outcomes_spline = []\n",
    "                    outcomes_Linear = []\n",
    "                    cover_num_DP1 = 0\n",
    "                    cover_num_DP2 = 0\n",
    "                    cover_num_Linear1 = 0\n",
    "                    cover_num_Linear2 = 0\n",
    "                    cover_num_spline1 = 0\n",
    "                    cover_num_spline2 = 0\n",
    "                    for i_  in range(100): \n",
    "                        print(i_)\n",
    "                        loss_kk = []\n",
    "                        beta1_kk = []\n",
    "                        beta2_kk = []\n",
    "                        REg_kk = []\n",
    "                        g_kk =[]\n",
    "                        model_kk = []\n",
    "                        data = torch.hstack(dataproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=i_+10400, p=p,a=0,b=b))\n",
    "                        data_np = data\n",
    "                        train, valid, test = cv(data,1)\n",
    "                        train_np = train.detach().numpy()\n",
    "                        valid_np = valid.detach().numpy()\n",
    "                        test_np = test.detach().numpy()\n",
    "                        train0, valid0, test0 = cv(train,1)\n",
    "                        T_train0,epsilon_train0,Z_train,X_train0,delta_train0=dedata(train0)\n",
    "                        T_valid0,epsilon_valid0,Z_valid0,X_valid0,delta_valid0=dedata(valid0)\n",
    "                        T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train)\n",
    "                        T_test, epsilon_test, Z_test, X_test, delta_test = dedata(test)\n",
    "                        T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid)\n",
    "                        T, epsilon, Z, X, delta = dedata(data)\n",
    "                        G_hat_train = G_hat(T_train,delta_train)\n",
    "                        G_hat_train0 = G_hat(T_train0,delta_train0)\n",
    "                        G_hat_valid = G_hat(T_valid,delta_valid)\n",
    "                        G_hat_valid0 = G_hat(T_valid0,delta_valid0)\n",
    "                        G_hat_test = G_hat(T_test,delta_test)\n",
    "                        G_hat_data = G_hat(T,delta)\n",
    "                        data = data.to(device)\n",
    "                        train, valid, test = cv(data,1)\n",
    "                        T, epsilon, Z, X, delta = dedata(data)\n",
    "                        test_in_data_index = []\n",
    "                        for i in range(len(test[:,0])):\n",
    "                            test_in_data_index.append(torch.where(torch.isin(data[:,0],test[i,0]))[0][0])\n",
    "                        torch.Tensor(test_in_data_index).int()\n",
    "                        '''\n",
    "                        找出test在整个data里对应的index\n",
    "                        '''\n",
    "                        U_test =Uproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=0, p=p,a=0,b=b)[test_in_data_index].to(device)\n",
    "                        U_test[torch.isnan(U_test)]=1e4\n",
    "                        U = Uproduce(n=samplesize, beta11=1.0, beta12=1.0, beta21=-0.5, beta22=0.5, g_index=g_index, z_index=z_index, seed=0, p=p,a=0,b=b).to(device)\n",
    "                        U[torch.isnan(U)]=1e4\n",
    "                        U_unordered = U_test[(U_test>0)&(U_test<U_test.max())]\n",
    "                        U_ordered = U_test[(U_test>0)&(U_test<U_test.max())].sort().values\n",
    "                        len_T = len(U_ordered)\n",
    "                        M0 = (Z_test[:,0]+Z_test[:,1]+g_(X_test)).to(device)\n",
    "                        for N_index in range(4):\n",
    "                            for kk in range(5):\n",
    "                                loss, beta1,beta2,REg_,g ,model = estimation(N_index+1,train, valid, test,G_hat_train,G_hat_valid,G_hat_train0,G_hat_valid0,g_)\n",
    "                                if torch.isnan(loss):\n",
    "                                    break\n",
    "                                loss_kk.append(loss)\n",
    "                                beta1_kk.append(beta1)\n",
    "                                beta2_kk.append(beta2)\n",
    "                                REg_kk.append(REg_)\n",
    "                                g_kk.append(g)\n",
    "                                model_kk.append(model)\n",
    "                        index_kk = torch.where(torch.min(torch.tensor(loss_kk))==torch.tensor(loss_kk))[0]\n",
    "                        std1,std2=variance_estimation(train, valid, test)\n",
    "                        if CI_cover(samplesize,beta1_kk[index_kk].item(),std1,1.0) == 1:\n",
    "                            cover_num_DP1 = cover_num_DP1+1\n",
    "                        if CI_cover(samplesize,beta2_kk[index_kk].item(),std2,1.0) == 1:\n",
    "                            cover_num_DP2 = cover_num_DP2+1\n",
    "                        T_test, epsilon_test, Z_test, X_test, delta_test = dedata(test)\n",
    "                        M = beta1_kk[index_kk]*Z_test[:,0]+beta2_kk[index_kk]*Z_test[:,1]+g_kk[index_kk].reshape(int(samplesize*0.2))-g_kk[index_kk].reshape(int(samplesize*0.2)).mean()\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_DP = [loss_kk[index_kk].item(),beta1_kk[index_kk].item(),beta2_kk[index_kk].item(),REg_kk[index_kk].item(),(std1/np.sqrt(samplesize)).item(),(std2/np.sqrt(samplesize)).item(),cover_num_DP1,cover_num_DP2,q1,q2,q3,Ctau]\n",
    "                        outcomes_DP.append(outcome_DP)\n",
    "                        print('DP',beta1_kk[index_kk].item(),beta2_kk[index_kk].item(),REg_kk[index_kk].item(),(1.96*std1/np.sqrt(samplesize)).item(),(1.96*std2/np.sqrt(samplesize)).item(),cover_num_DP1,cover_num_DP2,q1,q2,q3,Ctau)\n",
    "                        T_train,epsilon_train,Z_train,X_train,delta_train=dedata(train_np)\n",
    "                        T_test,epsilon_test,Z_test,X_test,delta_test=dedata(test_np)\n",
    "                        T_valid,epsilon_valid,Z_valid,X_valid,delta_valid=dedata(valid_np)\n",
    "                        df = pd.DataFrame(np.hstack([T_train,epsilon_train*delta_train,Z_train,X_train]))\n",
    "                        # 创建一个新的列标列表\n",
    "                        new_columns = ['ftime', 'fstatus', 'Z1', 'Z2', 'X1', 'X2', 'X3', 'X4', 'X5']\n",
    "\n",
    "                        # 使用rename方法将列标更改为新列表中的值\n",
    "                        df = df.rename(columns=dict(zip(df.columns, new_columns)))\n",
    "                        a = cmprsk.crr(failure_time = df['ftime'],failure_status = df['fstatus'],static_covariates =df[['Z1','Z2','X1','X2','X3','X4','X5']], failcode=1, cencode=0)\n",
    "                        g_hat = torch.tensor(X_test@a.summary.coefficients.values[2:])\n",
    "                        g_0 = g_(torch.tensor(X_test))\n",
    "                        std1 = a.summary[\"std\"][0]\n",
    "                        std2 = a.summary[\"std\"][1]\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[0],std1*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_Linear1 = cover_num_Linear1+1\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[1],std2*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_Linear2 = cover_num_Linear2+1\n",
    "                        M = torch.tensor(Z_test@a.summary.coefficients.values[0:2]+X_test@a.summary.coefficients.values[2:]-(X_test@a.summary.coefficients.values[2:]).mean()).to(device)\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_Linear = [a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),std1,std2,cover_num_Linear1,cover_num_Linear2,q1,q2,q3,Ctau]\n",
    "                        outcomes_Linear.append(outcome_Linear)\n",
    "                        print('Linear',a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),1.96*std1,1.96*std2,cover_num_Linear1,cover_num_Linear2,q1,q2,q3,Ctau)\n",
    "                        q = 5\n",
    "                        m = 3\n",
    "                        spline_df = np.hstack([cubic_spline(q,X_train[:,0],m),cubic_spline(q,X_train[:,1],m),cubic_spline(q,X_train[:,2],m),cubic_spline(q,X_train[:,3],m),cubic_spline(q,X_train[:,4],m)])\n",
    "                        spline_df = np.hstack([Z_train,spline_df])\n",
    "                        spline_df = pd.DataFrame(spline_df)\n",
    "                        df = pd.DataFrame(np.hstack([T_train,epsilon_train*delta_train,Z_train,X_train]))\n",
    "                        # 创建一个新的列标列表\n",
    "                        new_columns = ['ftime', 'fstatus', 'Z1', 'Z2', 'X1', 'X2', 'X3', 'X4', 'X5']\n",
    "                        # 使用rename方法将列标更改为新列表中的值\n",
    "                        df = df.rename(columns=dict(zip(df.columns, new_columns)))\n",
    "                        a = cmprsk.crr(failure_time = df['ftime'],failure_status = df['fstatus'],static_covariates =spline_df, failcode=1, cencode=0)\n",
    "                        X_test_spline = np.hstack([cubic_spline(q,X_test[:,0],m),cubic_spline(q,X_test[:,1],m),cubic_spline(q,X_test[:,2],m),cubic_spline(q,X_test[:,3],m),cubic_spline(q,X_test[:,4],m)])\n",
    "                        g_hat = torch.tensor(X_test_spline@a.summary.coefficients.values[2:])\n",
    "                        g_0 = g_(torch.tensor(X_test))\n",
    "                        std1 = a.summary[\"std\"][0]\n",
    "                        std2 = a.summary[\"std\"][1]\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[0],std1*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_spline1 = cover_num_spline1+1\n",
    "                        if CI_cover(samplesize,a.summary.coefficients.values[1],std2*np.sqrt(samplesize),1.0) == 1:\n",
    "                            cover_num_spline2 = cover_num_spline2+1\n",
    "                        M = torch.tensor(Z_test@a.summary.coefficients.values[0:2]+X_test_spline@a.summary.coefficients.values[2:]-(X_test_spline@a.summary.coefficients.values[2:]).mean()).to(device)\n",
    "                        TPR,FPR = TPR_FPR(U_test,M,M0)\n",
    "                        auc_array = torch.zeros(len_T).to(device)\n",
    "                        for i in range(len_T):\n",
    "                            auc_array[i]=torch.trapz(y=TPR[:,i], x=FPR[:,i])\n",
    "                        num  = len(auc_array)\n",
    "                        q1 = auc_array[int(num/4)].item()\n",
    "                        q2 = auc_array[int(2*num/4)].item()\n",
    "                        q3 = auc_array[int(3*num/4)].item()\n",
    "                        Ctau=C_tau(auc_array,U,U_ordered, X, Z, g_, p).item()\n",
    "                        outcome_spline= [a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),std1,std2,cover_num_spline1,cover_num_spline2,q1,q2,q3,Ctau]\n",
    "                        outcomes_spline.append(outcome_spline)\n",
    "                        print('spline',a.summary.coefficients.values[0],a.summary.coefficients.values[1],REg(g_0,g_hat).item(),1.96*std1,1.96*std2,cover_num_spline1,cover_num_spline2,q1,q2,q3,Ctau)\n",
    "                    df_outcomes_DP = outcome_to_df(outcomes_DP,1)\n",
    "                    df_outcomes_Linear = outcome_to_df(outcomes_Linear,0)\n",
    "                    df_outcomes_spline = outcome_to_df(outcomes_spline,0)\n",
    "                    df_outcomes_DP['cover_rate1']=df_outcomes_DP['cover_rate1'][99]\n",
    "                    df_outcomes_Linear['cover_rate1']=df_outcomes_Linear['cover_rate1'][99]\n",
    "                    df_outcomes_spline['cover_rate1']=df_outcomes_spline['cover_rate1'][99]\n",
    "                    df_outcomes_DP['cover_rate2']=df_outcomes_DP['cover_rate2'][99]\n",
    "                    df_outcomes_Linear['cover_rate2']=df_outcomes_Linear['cover_rate2'][99]\n",
    "                    df_outcomes_spline['cover_rate2']=df_outcomes_spline['cover_rate2'][99]\n",
    "                    df_mean = pd.DataFrame([df_outcomes_DP.mean(),df_outcomes_Linear.mean(),df_outcomes_spline.mean()])\n",
    "                    df_std = pd.DataFrame([df_outcomes_DP.std(),df_outcomes_Linear.std(),df_outcomes_spline.std()])\n",
    "                    filepath = os.path.join(folder_path,excelname)\n",
    "                    writer = pd.ExcelWriter(filepath)\n",
    "                    # 将每个 DataFrame 写入不同的工作表（sheet）中\n",
    "                    df_outcomes_DP.to_excel(writer, sheet_name='DP')\n",
    "                    df_outcomes_Linear.to_excel(writer, sheet_name='Linear')\n",
    "                    df_outcomes_spline.to_excel(writer, sheet_name='spline')\n",
    "                    df_mean.to_excel(writer, sheet_name='mean')\n",
    "                    df_std.to_excel(writer, sheet_name='std')\n",
    "                    writer.close()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f3ed56a6e50d597e18db0e21426aae15710eba3e727edd94da93a2f95f1a3a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
